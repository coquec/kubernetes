= Kubernetes
:tags: Publish
:author: Jose Couto
:email: jcouto
:date: junio 2022
:revdate: 20220602
:source-highlighter: rouge
:toc:
:toc-title: √çndice
:toclevels: 3
:numbered:
:appendix-caption: Ap√©ndice
:figure-caption: Figura
:table-caption: Tabla

== Kubernetes

Kubernetes (abreviado, K8s), es un orquestador de contenedores.  Se encarga de
ejecutarlos cuando se cumplan las condiciones adecuadas y de vigilar que tengan
los recursos que necesiten.  Corre en un cluster de _nodos_, en el que uno o
varios de ellos (los _masters_), ejecutan el __control plane_, que controla el
cluster y tiene los siguientes componentes:

* `etcd`, que guarda el estado completo del cluster: los par√°metros de
   configuraci√≥n, las especificaciones y el estado de los trabajos.

* `kube-controller-manager`, encargado de ejecutar los distintos bucles de
   control o _controllers_ necesarios para alcanzar el estado deseado del
   cluster, como los siguientes:

** _Node controller_, que vigila los nodos y responde ante sus cambios.

** _Job controller_, que vigila si es necesario lanzar trabajos y crea
   <<pod,pods>> para hacerlo.

** _Endpoints controller_, que asocia los servicios con los <<pod,pods>>.

** _Service accounts_ y _Token controlers_, que crean las cuentas y los tokens
   de acceso para las API de nuevos espacios de nombres.

* `kube-scheduler`, que decide en qu√© nodo ejecutar un nuevo <<pod>>
   ajust√°ndose lo m√°s posible a sus necesidades.

* `kube-apiserver`, que proporciona una API para trabajar con K8s, y cuyo
   principal cliente es la orden `kubectl`.

Los clusters que corren en un proveedor en la nube tambi√©n tienen un componente
llamado `cloud-controller-manager`, que se encarga de ejecutar los
controladores espec√≠ficos para gestionar los recursos del proveedor.

Todos los nodos son capaces de ejecutar trabajos, aunque suele evitarse hacer
esto en los nodos que corren el control plane.  Para ejecutar trabajos, los
nodos tienen:

* Un proceso llamado `kubelet` que se encarga de comunicarse con el control
  plane y ejecutar lo que les pidan.

* El proceso `kube-proxy`, que se encarga de gestionar las reglas de red de los
  nodos para permitir la comunicaci√≥n con los <<pod,pods>> tanto dentro del
  cluster como fuera.  Implementan el concepto de _servicio_.  Utiliza las
  reglas de filtrado del sistema operativo si est√°n disponibles, o hace de
  proxy a nivel de aplicaci√≥n si no.

* Un _runtime_ para ejecutar los contenedores, como Docker, containerd, CRI-O,
  rktlet...

== Instalaci√≥n de un cluster de pruebas

Podemos instalar un cluster de K8s en un equipo con Linux y Docker o Podman
(para contenedores _rootless_), utilizando herramientas como
https://kind.sigs.k8s.io/[kind] o https://minikube.sigs.k8s.io[minikube].  Kind
funciona mejor con Podman, pero solo crea clusters con un nodo.  Para mis
pruebas, utilizar√© minikube con Docker, con un nodo master y dos adicionales,
todo ello corriendo en una distribuci√≥n Debian Sid..

=== Instalaci√≥n de minikube sobre docker

Instalamos Docker desde los repositorios de Debian:

[source,console]
----
$ sudo apt install docker.io
...

$ docker version
Client:
 Version:           20.10.14+dfsg1
 API version:       1.41
 Go version:        go1.18.1
 Git commit:        a224086
 Built:             Sun May  1 19:59:40 2022
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      true

Server:
 Engine:
  Version:          20.10.14+dfsg1
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.18.1
  Git commit:       87a90dc
  Built:            Sun May  1 19:59:40 2022
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.6~ds1
  GitCommit:        1.6.6~ds1-1
 runc:
  Version:          1.1.1+ds1
  GitCommit:        1.1.1+ds1-1+b1
 docker-init:
  Version:          0.19.0
  GitCommit:
----

Descargamos e instalamos el paquete de Debian de minikube, que solo tiene el
ejecutable.

[source,console]
----
$ cd /tmp
$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 23.2M  100 23.2M    0     0  11.8M      0  0:00:01  0:00:01 --:--:-- 11.8M

$ dpkg -I minikube_latest_amd64.deb
 new Debian package, version 2.0.
 size 24363252 bytes: control archive=407 bytes.
     406 bytes,    12 lines      control
 Package: minikube
 Version: 1.25.2-0
 Section: base
 Priority: optional
 Architecture: amd64
 Recommends: virtualbox
 Maintainer: Thomas Str√∂mberg <t+minikube@stromberg.org>
 Description: Minikube
  minikube is a tool that makes it easy to run Kubernetes locally.
  minikube runs a single-node Kubernetes cluster inside a VM on your
  laptop for users looking to try out Kubernetes or develop with it
  day-to-day.

$ sudo dpkg -i minikube_latest_amd64.deb
Selecting previously unselected package minikube.
(Reading database ... 297313 files and directories currently installed.)
Preparing to unpack minikube_latest_amd64.deb ...
Unpacking minikube (1.25.2-0) ...
Setting up minikube (1.25.2-0) ...

$ dpkg -L minikube
/.
/usr
/usr/bin
/usr/bin/minikube
----

Lanzamos `minikube` para que levante tres nodos sobre Docker:

[source,console]
----
$ minikube start --kubernetes-version=latest --driver=docker --nodes=3
üòÑ  minikube v1.25.2 on Debian bookworm/sid
‚ú®  Using the docker driver based on user configuration
üëç  Starting control plane node minikube in cluster minikube
üöú  Pulling base image ...
üíæ  Downloading Kubernetes v1.23.4-rc.0 preload ...
    > preloaded-images-k8s-v17-v1...: 505.63 MiB / 505.63 MiB  100.00% 10.77 Mi
üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
üê≥  Preparing Kubernetes v1.23.4-rc.0 on Docker 20.10.12 ...
    ‚ñ™ kubelet.housekeeping-interval=5m
    ‚ñ™ kubelet.cni-conf-dir=/etc/cni/net.mk
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîó  Configuring CNI (Container Networking Interface) ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass

üëç  Starting worker node minikube-m02 in cluster minikube
üöú  Pulling base image ...
üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=192.168.49.2
üê≥  Preparing Kubernetes v1.23.4-rc.0 on Docker 20.10.12 ...
    ‚ñ™ env NO_PROXY=192.168.49.2
üîé  Verifying Kubernetes components...

üëç  Starting worker node minikube-m03 in cluster minikube
üöú  Pulling base image ...
üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=192.168.49.2,192.168.49.3
üê≥  Preparing Kubernetes v1.23.4-rc.0 on Docker 20.10.12 ...
    ‚ñ™ env NO_PROXY=192.168.49.2
    ‚ñ™ env NO_PROXY=192.168.49.2,192.168.49.3
üîé  Verifying Kubernetes components...
üí°  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
----

`minikube` crea una configuraci√≥n para `kubectl` en `~/kube/config` para
permitirle conectarse al cluster reci√©n creado.

[[metrics_server,`metrics-server`]]
=== Instalaci√≥n de metrics server

Algunas funciones de K8s, como la obtenci√≥n de m√©tricas de los pods con
<<kubectl_top>> o el autoescalado horizontal, necesitan que est√© instalado el
paquete https://github.com/kubernetes-sigs/metrics-server[Kubernetes Metrics
Server], que se puede desplegar sobre minikube siguiendo un remiendo
documentado
https://github.com/kubernetes-sigs/metrics-server/issues/196#issuecomment-451061841[aqu√≠],
que hace falta porque minikube usa certificados digitales autofirmados para los
`kubelet` de los nodos:

[source,console]
----
$ curl -sL https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml | sed -e '/cert-dir/p' -e '0,/cert-dir/s/cert-dir.*/kubelet-insecure-tls/'| kubectl apply -f -
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
----

=== Instalaci√≥n de kubectl

Aunque recomienda definir el alias `kubectl='minikube kubectl --'` para
utilizar su propio cliente de `kubectl`, para garantizar que usamos la misma
versi√≥n del cliente y del servidor, pero con √©l
https://github.com/kubernetes/minikube/issues/12938[no funciona el
autocompletado].  En Debian, podemos instalar `kubectl` con un snap, aunque la
versi√≥n de Debian es distinta que la que instala minikube:

[source,console]
----
$ sudo snap install kubectl --classic
2022-06-10T18:41:03+02:00 INFO Waiting for automatic snapd restart...
kubectl 1.24.0 from Canonical‚úì installed

$ kubectl version --output=yaml
clientVersion:
  buildDate: "2022-05-04T02:28:17Z"
  compiler: gc
  gitCommit: 4ce5a8954017644c5420bae81d72b09b735c21f0
  gitTreeState: clean
  gitVersion: v1.24.0
  goVersion: go1.18.1
  major: "1"
  minor: "24"
  platform: linux/amd64
kustomizeVersion: v4.5.4
serverVersion:
  buildDate: "2022-01-25T21:44:57Z"
  compiler: gc
  gitCommit: 72506a8439cb4465d176af044e4404439135c915
  gitTreeState: clean
  gitVersion: v1.23.4-rc.0
  goVersion: go1.17.6
  major: "1"
  minor: 23+
  platform: linux/amd64
----

== API de Kubernetes

`kube-apiserver` implementa un servicio API REST que utilizan los usuarios,
partes del cluster y los componentes externos para interactuar con K8s.  La API
permite consultar y manipular el estado de los _API objects_ de K8s, como
<<pod,pods>>, namespaces, ConfigMaps, eventos...  Todas las entradas tienen el
formato `<punto_de_entrada_a_API>/<group>/<version>/<resource>`

Se puede ver qu√© APIs soporta un cluster con <<kubectl_api_versions>>, y qu√©
recursos podemos manipular con <<kubectl_api_resources>>.

La API de K8s requiere que los objetos se pasen en formato JSON. `kubectl` se
encarga de convertir los objetos especificados como YAML a JSON.

Para poder manipular un objeto en K8s, necesitamos:

* *apiVersion*, la versi√≥n de la API que utiliza el objeto.

* *kind*, la clase del objeto.

* *metadata.name*, el nombre √∫nico del objeto en su namespace.

* *metadata.namespace*, el namespace donde est√° definido el objeto (por
   defecto, el actual o _current_).

* *metadata.uid*, el identificador √∫nico generado para el objeto.

En YAML, esto tendr√≠a el siguiente aspecto:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
    name: mypod
    namespace: default
    uid: '145c2436-e0bb-11ec-b44c-e7f1d45f0a43'
----

Los objetos de K8s pueden examinarse con <<kubectl_get>>.

Las versiones de API `apiVersion` tienen tres niveles de soporte:

* Alpha, para todos los nombres que contienen `alpha`, como `v1alpha2`.  No hay
  ning√∫n tipo de garant√≠a sobre estas API: pueden cambiar o desaparecer en
  cualquier momento.

* Beta, para todos los nombres que contienen `beta`, como `v2beta1`.  Son API
  probadas, aunque puede que se introduzcan peque√±os cambios en versiones
  posteriores beta o estables, que obliguen a recrear los objetos afectados.
  Hay garant√≠as de que no desaparecer√°n.  No se recomienda que se usen estas
  API en producci√≥n, salvo que tengamos varios clusters que se puedan
  actualizar de forma independiente.

* Estable, que se refieren a todos los nombres que no contienen `alpha` ni
  `beta`.

=== Control de acceso a la API

WARNING: https://kubernetes.io/docs/concepts/security/controlling-access/[TODO].

Por defecto, la API de K8s est√° accesible en dos direcciones, una insegura y
otra segura.  La direcci√≥n insegura est√° pensada para hacer diagn√≥stico, y se
encuentra en la direcci√≥n `localhost:8080` de los nodos que tienen el control
plane. Utiliza HTTP en claro y no requiere autenticaci√≥n ni autorizaci√≥n,
aunque s√≠ que aplican los m√≥dulos de control de entrada (_admission control_).
La direcci√≥n segura es la que usamos habitualmente con `kubectl`.

=== Creaci√≥n de plantillas YAML

Cada recurso de K8s se puede definir en YAML o en JSON.  Aunque `kubectl` no
tiene forma directa de crear las plantillas con todas las opciones de un
recurso, se puede sacar suficiente informaci√≥n con `kubectl explain <recurso>`,
y generar una base bastante parecida a YAML, y demasiado extensa, con la opci√≥n
`--recursive`:

[source,console]
----
$ kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status
...

$ kubectl explain pod.spec
KIND:     Pod
VERSION:  v1

RESOURCE: spec <Object>

DESCRIPTION:
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

     PodSpec is a description of a pod.

FIELDS:
   activeDeadlineSeconds        <integer>
     Optional duration in seconds the pod may be active on the node relative to
     StartTime before the system will actively try to mark it failed and kill
     associated containers. Value must be a positive integer.

   affinity     <Object>
     If specified, the pod's scheduling constraints

   automountServiceAccountToken <boolean>
     AutomountServiceAccountToken indicates whether a service account token
     should be automatically mounted.

   containers   <[]Object> -required-
     List of containers belonging to the pod. Containers cannot currently be
     added or removed. There must be at least one container in a Pod. Cannot be
     updated.
...

$ kubectl explain pod --recursive
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
   kind <string>
   metadata     <Object>
      annotations       <map[string]string>
      clusterName       <string>
      creationTimestamp <string>
      deletionGracePeriodSeconds        <integer>
      deletionTimestamp <string>
      finalizers        <[]string>
      generateName      <string>
      generation        <integer>
      labels    <map[string]string>
      managedFields     <[]Object>
         apiVersion     <string>
         fieldsType     <string>
         fieldsV1       <map[string]>
         manager        <string>
         operation      <string>
         subresource    <string>
         time   <string>
      name      <string>
      namespace <string>
      ownerReferences   <[]Object>
         apiVersion     <string>
         blockOwnerDeletion     <boolean>
         controller     <boolean>
         kind   <string>
         name   <string>
         uid    <string>
      resourceVersion   <string>
      selfLink  <string>
      uid       <string>
   spec <Object>
      activeDeadlineSeconds     <integer>
      affinity  <Object>
         nodeAffinity   <Object>
            preferredDuringSchedulingIgnoredDuringExecution     <[]Object>
               preference       <Object>
                  matchExpressions      <[]Object>
                     key        <string>
                     operator   <string>
                     values     <[]string>
                  matchFields   <[]Object>
                     key        <string>
                     operator   <string>
                     values     <[]string>
               weight   <integer>
...
----

Para los recursos que se pueden crear con `kubectl create`, tambi√©n se puede
hacer una prueba de la creaci√≥n de un objeto con la opci√≥n `--dry-run=client`:

[source,console]
----
$ kubectl create deployment trospido --image=nginx --dry-run=client -o=yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: trospido
  name: trospido
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trospido
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: trospido
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
----

== Etiquetas

Todos los objetos de K8s pueden tener etiquetas asociadas (<<label,_labels_>>),
que se utilizan para agruparlos de forma l√≥gica, pudi√©ndose utilizar en los
seleccionadores (<<selector,_selectors_>>).  Podemos crear o modificar Las
etiquetas de los objetos en cualquier momento.

Las etiquetas y los seleccionadores pueden usarse para cosas como decidir en
qu√© nodos del cluster deben ejecutarse determinados servicios o el tipo de
almacenamiento a utilizar.

Las etiquetas se asignan como parte de los metadatos de un objeto:

[source,yaml]
----
metadata:
  labels:
    key1: value1
    key2: value2
----

Las claves tienen la forma `[prefijo/]nombre`, con un prefijo opcional que
tiene la forma de un dominio DNS, y un nombre obligatorio que empieza y termina
por un car√°cter alfanum√©rico y que puede incluir entre medias eso mismo m√°s
`-`, `_` y `.`.  Se entiende que las claves sin prefijo son privadas para los
usuarios.  Todas las etiquetas que utilizan los componentes propios de K8s
tienen prefijo.  Los prefijos `kubernetes.io` y `k8s.io` est√°n reservados para
ellos.

K8s
https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/[recomienda]
utilizar algunas etiquetas para agrupar objetos, todas con el prefijo
`app.kubernetes.io`.

NOTE: Es importante que las organizaciones definan un conjunto est√°ndar de
etiquetas para facilitar la gesti√≥n de los objetos de sus clusters, y que se
utilicen en las plantillas de los distintos objetos.

[[seleccionadores,_seleccionadores_]]
=== Seleccionadores

Son filtros que permiten elegir objetos de K8s bas√°ndose en valores de sus
etiquetas.  Los hay de dos tipos, los basados en la igualdad y los que permiten
buscar en conjuntos de valores.

.Seleccionador basado en la igualdad
[source,yaml]
----
selector:
  matchLabels:
    key1: value
----

Los seleccionadores basados en la igualdad admiten tres operadores, `=` e `==`,
que son equivalentes y requieren que las etiquetas sean iguales a un valor, y
`!=`, para requerir que sean distintas a un valor *o que el objeto no tenga esa
etiqueta*.  Pueden tener uno o varios requisitos separados por comas, que
act√∫an como un AND l√≥gico (deben cumplirse todos los requisitos):

[source,console]
----
$ get pods --selector environment=pro,tier!=frontend
----

WARNING: Parece que no hay forma de conseguir el efecto de `!=` en YAML con los
seleccionadores basados en igualdad.  Se puede conseguir algo similar con los
seleccionadores basados en conjuntos y el operador `NotIn`, pero no todos los
objetos de K8s soportan este tipo de seleccionadores.

WARNING: No hay operador OR para ninguno de los dos tipos de seleccionadores.

.Seleccionador basado en conjuntos [source,yaml]
----
selector:
  matchExpressions:
  - key: key1
    operator: In
    values:
    - value1
    - value2
----

Este tipo de seleccionadores admite los operadores `In`, `NotIn`, `Exists`,
`DoesNotExist`, `Gt` y `Lt`.

== Pods

Un _pod_ (en el sentido de "manada"), es la unidad m√≠nima de proceso de
Kubernetes.  Consiste en un grupo de contenedores que comparten ciertos
recursos, como los vol√∫menes (aunque cada uno tenga su propio _mount
namespace_), el _namespace_ de red y el de IPC (comunicaci√≥n entre procesos
Posix y System V).  El contenido de un pod se lanza en un √∫nico nodo, y se
gestiona como un todo.  Se puede pensar en ellos como en hosts virtuales para
ejecutar procesos fuertemente acoplados.

Al compartir el _namespace_ de red, todos los procesos de un pod pueden
comunicarse mediante la direcci√≥n IP del localhost (127.0.0.1).  Como comparten
los n√∫meros de puertos, es necesario que los contenedores de un pod utilicen
puertos distintos para prestar sus servicios.

Para comprobar qu√© _namespaces_ comparten dos procesos que forman parte del
mismo pod en el cluster de minikube creado antes, lanzamos el siguiente pod:

.pod-2containers.yaml
[source,yaml]
----
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-2containers
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  - name: loop
    image: nginx
    command:  ['sh', '-c', 'while true; do date; sleep 10s; done']
----

[source,console]
----
$ kubectl apply -f pod-2containers.yaml
pod/pod-2containers created

$ kubectl get pods
NAME               READY   STATUS    RESTARTS   AGE
pod-2containers   2/2     Running   0          5s

$  ps -ef | grep -iE 'nginx|sleep'
root     1014754 1014734  0 18:15 ?        00:00:00 nginx: master process nginx -g daemon off;
systemd+ 1014792 1014754  0 18:15 ?        00:00:00 nginx: worker process
systemd+ 1014793 1014754  0 18:15 ?        00:00:00 nginx: worker process
systemd+ 1014794 1014754  0 18:15 ?        00:00:00 nginx: worker process
systemd+ 1014795 1014754  0 18:15 ?        00:00:00 nginx: worker process
root     1014841 1014820  0 18:15 ?        00:00:00 sh -c while true; do date; sleep 10s; done
root     1049523 1014841  0 18:52 ?        00:00:00 sleep 10s

$ pstree -pslT 1014754
systemd(1)‚îÄ‚îÄ‚îÄcontainerd-shim(12954)‚îÄ‚îÄ‚îÄsystemd(12978)‚îÄ‚îÄ‚îÄcontainerd-shim(1014734)‚îÄ‚îÄ‚îÄnginx(1014754)‚îÄ‚î¨‚îÄnginx(1014792)
                                                                                                 ‚îú‚îÄnginx(1014793)
                                                                                                 ‚îú‚îÄnginx(1014794)
                                                                                                 ‚îî‚îÄnginx(1014795)

$ pstree -plT 12978
systemd(12978)‚îÄ‚î¨‚îÄcontainerd(13170)
               ‚îú‚îÄcontainerd-shim(15374)‚îÄ‚îÄ‚îÄpause(15394)
               ‚îú‚îÄcontainerd-shim(15416)‚îÄ‚îÄ‚îÄpause(15437)
               ‚îú‚îÄcontainerd-shim(15465)‚îÄ‚îÄ‚îÄkube-proxy(15514)
               ‚îú‚îÄcontainerd-shim(15494)‚îÄ‚îÄ‚îÄkindnetd(15522)
               ‚îú‚îÄcontainerd-shim(1014591)‚îÄ‚îÄ‚îÄpause(1014611)
               ‚îú‚îÄcontainerd-shim(1014734)‚îÄ‚îÄ‚îÄnginx(1014754)‚îÄ‚î¨‚îÄnginx(1014792)
               ‚îÇ                                           ‚îú‚îÄnginx(1014793)
               ‚îÇ                                           ‚îú‚îÄnginx(1014794)
               ‚îÇ                                           ‚îî‚îÄnginx(1014795)
               ‚îú‚îÄcontainerd-shim(1014820)‚îÄ‚îÄ‚îÄsh(1014841)‚îÄ‚îÄ‚îÄsleep(1088462)
               ‚îú‚îÄdbus-daemon(13166)
               ‚îú‚îÄdockerd(13196)
               ‚îú‚îÄkubelet(14943)
               ‚îú‚îÄsshd(13183)
               ‚îî‚îÄsystemd-journal(13145)

# diff -y (readlink /proc/1014754/ns/* | psub) (readlink /proc/1014841/ns/* | psub)
cgroup:[4026534600]                                           | cgroup:[4026534604]
ipc:[4026534462]                                                ipc:[4026534462]
mnt:[4026534597]                                              | mnt:[4026534601]
net:[4026534464]                                                net:[4026534464]
pid:[4026534599]                                              | pid:[4026534603]
pid:[4026534599]                                              | pid:[4026534603]
time:[4026531834]                                               time:[4026531834]
time:[4026531834]                                               time:[4026531834]
user:[4026531837]                                               user:[4026531837]
uts:[4026534598]                                              | uts:[4026534602]

# systemd-cgls -l
...
  ‚îÇ     ‚îî‚îÄkubepods-besteffort-podd9d2bfea_9b77_43db_9741_e5f9ad6a70ec.slice (#99978)
  ‚îÇ       ‚Üí trusted.invocation_id: 26cce36417ae4549bf775fb45a9c2bf8
  ‚îÇ       ‚îú‚îÄdocker-6b5120debb47b88bef33a471edf1ce451f679587033b44f4fe83ac4e2be5e190.scope ‚Ä¶ (#100173)
  ‚îÇ       ‚îÇ ‚Üí trusted.delegate: 1
  ‚îÇ       ‚îÇ ‚Üí trusted.invocation_id: 76b501c6ec94475f81fa407e21cfe218
  ‚îÇ       ‚îÇ ‚îú‚îÄ1014841 sh -c while true; do date; sleep 10s; done
  ‚îÇ       ‚îÇ ‚îî‚îÄ1068429 sleep 10s
  ‚îÇ       ‚îú‚îÄdocker-f34da3d995e1fc06f1d71e22b960e7b4d16fb1cefe71c40c1184229c1f62b0b2.scope ‚Ä¶ (#100043)
  ‚îÇ       ‚îÇ ‚Üí trusted.delegate: 1
  ‚îÇ       ‚îÇ ‚Üí trusted.invocation_id: 0bfd50d366084aa4828f6ac260afc6aa
  ‚îÇ       ‚îÇ ‚îî‚îÄ1014611 /pause
  ‚îÇ       ‚îî‚îÄdocker-e4d2f81c40fc09a437ba15a3fd4f3da859744d91dd4819283c04e3a8ded0843e.scope ‚Ä¶ (#100108)
  ‚îÇ         ‚Üí trusted.delegate: 1
  ‚îÇ         ‚Üí trusted.invocation_id: b1a2b05b1a19412298ae8aa02d06919a
  ‚îÇ         ‚îú‚îÄ1014754 nginx: master process nginx -g daemon off;
  ‚îÇ         ‚îú‚îÄ1014792 nginx: worker process
  ‚îÇ         ‚îú‚îÄ1014793 nginx: worker process
  ‚îÇ         ‚îú‚îÄ1014794 nginx: worker process
  ‚îÇ         ‚îî‚îÄ1014795 nginx: worker process
...
----

Como puede verse en las salidas anteriores, y al menos en el caso de un cluster
de minikube sobre Docker, los contenedores de un mismo pod comparten los
namespaces de red, IPC, _time_ y _user_ (el que a√≠sla los UID, GID y las
capacidades de los procesos).  Dentro de la jerarqu√≠a de _cgroups_, comparten
un ancestro com√∫n (el `kubepods-besteffort-pod...`), lo que permite gestionar
los recursos globales asignados a ellos.

Los contenedores de un pod ven como _hostname_ el campo `name` configurado en
el pod.

[source,console]
----
$ kubectl exec pod/pod-2containers -- hostname
Defaulted container "nginx" out of: nginx, loop
pod-2containers
----

=== Ciclo de vida de los pods

Los pods siguen un ciclo de vida bien definido, representado por el campo
`phase` de su objeto `PodStatus`, que aparece en el apartado `Status:` de la
salida de `kubectl describe pod`.

Los pods empiezan en estado `Pending` cuando son aceptados por el cluster de
K8s, y pasan a estado `Running` cuando todos sus contenedores se han creado y
al menos uno de ellos est√° corriendo.  Los pod pueden terminar en los estados
`Succeeded` (todos los contenedores han terminado bien y no deben ser
reiniciados) o `Failed` (todos los contenedores han terminado, pero al menos
uno fallando, con un estado distinto de 0 o terminado por el sistema).  Tambi√©n
pueden estar en estado `Unknown`, si por cualquier raz√≥n no se puede obtener su
estado (por ejemplo, por no poder comunicarse con su nodo).

Los pods son ef√≠meros.  La ejecuci√≥n de un pod se programa una sola vez en toda
su vida, asign√°ndole un nodo.  Una vez que se asigna un nodo a un pod, se
ejecuta en √©l hasta que termina o se elimina.  Si un nodo falla, se programa la
finalizaci√≥n de sus pods pasado un tiempo de espera.

Cada pod tiene su propio UID.  Los pods no pueden reasignarse a otros nodos,
pero pueden sustituirse por otro pod casi id√©ntico en otro nodo, con su
propio UID.

==== Estado de los contenedores

Dentro de un pod, los contenedores pasan por los siguientes estados, que pueden
verse con `kubectl describe pod`:

* `Waiting`, cuando se est√° preparando el contenedor para que pase a alguno de
  los otros estados.

* `Running`, cuando el contenedor est√° funcionando sin problemas.  Si el
  contenedor tuviera un _hook_ `postStart`, se habr√° ejecutado antes de pasar a
  este estado.

* `Terminated`, cuando un contenedor que ha pasado a estado `Running` termina
  por cualquier motivo.  Antes de pasar a este estado, se ejecuta cualquier
  _hook_ `preStop` que tuviera configurado.

[[politica_reinicio,pol√≠tica de reinicio]]
==== Pol√≠tica de reinicio de los contenedores

`kubelet` es capaz de reiniciar los contenedores de un pod ante cierto tipo de
fallos y hacer que el pod vuelva a estar saludable (_healthy_).  Esto depende
de la pol√≠tica `restartPolicy` que tenga configurada el pod, que puede tener
los valores `Always` (por defecto), `OnFailure` o `Never`.  `kubelet` reinicia
los contenedores incrementando el tiempo de espera de forma exponencial (10,
20, 40 segundos...), hasta 5 minutos m√°ximo.  Si un contenedor lleva 10 minutos
corriendo sin problemas, se reinicia el tiempo de espera a su valor inicial.

==== Condiciones de los pods

El `PodStatus` de los pods tiene un array de condiciones por las que el pod ha
podido pasar:

* `PodScheduled`, si se le ha asignado un nodo.

* `ContainersReady`, si todos los contenedores del pod est√°n en estado `Ready`.

* `Initialized`, si todos los contenedores de inicializaci√≥n han terminado
  correctamente.

* `Ready`, si el pod puede atender peticiones y puede ser a√±adido a la pila de
  balanceadores de los `Services` pertinentes.

[source,console]
----
$ kubectl describe pod pod-2containers
Name:         pod-2containers
Namespace:    blas
Priority:     0
Node:         minikube-m03/192.168.49.4
Start Time:   Mon, 27 Jun 2022 18:18:56 +0200
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           10.244.4.2
IPs:
  IP:  10.244.4.2
Containers:
  nginx:
    Container ID:   docker://aaac06fcf79aa3f03f077c5043cda90caac73b4781db968593c8ee91fbcd894b
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:10f14ffa93f8dedf1057897b745e5ac72ac5655c299dade0aa434c71557697ea
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 27 Jun 2022 18:18:59 +0200
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-47sbt (ro)
  loop:
    Container ID:  docker://c4bf74188ceeaa6ae14cab8ecf0c4ad7356ed744870e68fb35894dee3e88aaf8
    Image:         nginx
    Image ID:      docker-pullable://nginx@sha256:10f14ffa93f8dedf1057897b745e5ac72ac5655c299dade0aa434c71557697ea
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      while true; do date; sleep 10s; done
    State:          Running
      Started:      Mon, 27 Jun 2022 18:19:00 +0200
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-47sbt (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-47sbt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
----

==== Condiciones personalizadas (readinessGates)

Podemos a√±adir a los pods condiciones adicionales que `kubelet` puede utilizar
para determinar si est√°n listos para recibir peticiones o no, usando
`readinessGates`:

[source,yaml]
----
kind: Pod
...
spec:
  readinessGates:
    - conditionType: "BalancerReady"
status:
  conditions:
    - type: "BalancerReady"
      status: "False"
      lastProbeTime: null
      lastTransitionTime: 2022-01-01T00:00:00Z
...
----

Se trata de condiciones cuyo valor se actualiza mediante la API e K8s, no desde
dentro del pod.  Por ejemplo, podemos utilizar un programa externo que
compruebe si un balanceador externo est√° listo para enviar tr√°fico a un pod y
utilice la API para actualizar el estado de esa condici√≥n.  Se puede ver un
ejemplo de c√≥mo hacer esto
https://towardsdatascience.com/improving-application-availability-with-pod-readiness-gates-4ebebc3fb28a[aqu√≠].

Los pods que tengan condiciones personalizadas solo estar√°n en estado `Ready`
cuando todos sus contenedores est√©n `Ready` y el `status` de todas sus
`readinessGates` sea `True`.  Si lo primero fuera cierto pero lo segundo no, el
estado del pod ser√≠a `ContainersReady`.

==== Pruebas de estado

Podemos configurar hasta tres pruebas distintas que `kubelet` puede hacer sobre
los contenedores de un pod:

* `livenessProbe`, que indica si el contenedor est√° funcionando.  Si esta
  prueba falla, `kubelet` elimina el contenedor y se aplica su
  <<politica_reinicio>>.  Si no se personaliza esta prueba, por defecto se
  considera que est√° en estado `Success`.

* `readinessProbe`, que determina si el contenedor est√° listo para atender
  peticiones.  Si la prueba falla, el controlador de _endpoints_ quita la
  direcci√≥n IP del pod de los endpoints de todos los servicios que coincidan
  con el pod.  El estado de esta prueba es `Failure` durante la pausa inicial
  que haya configurada, pasando despu√©s a `Success` si no se personaliza la
  prueba.  Se considera que un pod est√° listo para atender peticiones cuando
  todos sus contenedores est√°n listos.

* `startupProbe`, que determina si la aplicaci√≥n del contenedor ha arrancado.
  Si se personaliza esta prueba, las otras dos pruebas se mantienen
  deshabilitadas hasta que esta se pasa.  Si la prueba falla, `kubelet` mata el
  contenedor y se le aplica la <<politica_reinicio>> que tenga configurada.  Si
  no se personaliza esta prueba, por defecto se considera que est√° en estado
  `Success`.

`kubelet` es el encargado de lanzar las pruebas, que pueden ser de los
siguientes tipos:

* `exec`, que ejecuta una orden dentro del contenedor, y se supera si
  devuelve 0.

* `httpGet`, que lanza un HTTP GET contra la URL especificada, y se pasa si se
  devuelve un c√≥digo HTTP mayor o igual que 200 y menor que 400.  En versiones
  de K8s anteriores o iguales a la 1.13, `kubelet` utilizar√° el proxy
  configurado en las variables de entorno `http_proxy` o `HTTP_PROXY` del nodo
  para comunicarse con el contenedor, pero a partir de esa versi√≥n lo har√°
  directamente.

* `tcpSocket`, que abre una conexi√≥n TCP contra el puerto especificado.  La
  prueba se pasa si el puerto est√° abierto.

* `grpc`, que utiliza llamadas a procedimiento remoto https://grpc.io/[gRPC].
  Por el momento, esto est√° en estado alpha.  Este tipo de pruebas est√°
  disponible a partir de la versi√≥n 1.24 de K8s.

El resultado de cualquiera de las pruebas anteriores puede ser `Success`, si se
pasan, `Failure`, si no se pasan, o `Unknown` si ha habido problemas para
lanzar la prueba, en cuyo caso se seguir√° intentando.

Se puede modificar el comportamiento de las distintas pruebas de estado a
trav√©s de los siguientes par√°metros:

* `initialDelaySeconds`, que por defecto es 0.  Es el n√∫mero de segundos a
  esperar desde que el contenedor arranca para empezar a lanzar las pruebas.
  No aplica a `startupProbe`.

* `periodSeconds`, que por defecto es 10s.  Cada cu√°nto se ejecuta la prueba.

* `timeoutSeconds`, que por defecto es 1s.  Tiempo m√°ximo de espera para
  obtener un resultado de la prueba.

* `successThreshold`, que por defecto es 1.  N√∫mero de pruebas correctas
  consecutivas necesario para considerar que el contenedor pasa la prueba,
  despu√©s de haber fallado.

* `failureThreshold`, que *por defecto es 3*.  N√∫mero de reintentos que hace
  K8s cuando una prueba falla, antes de abandonar y actuar en consecuencia.

Adem√°s, las pruebas `httpGet` admiten los siguientes par√°metros:

* `host`, que por defecto es la IP del pod.  Es el nombre del host al que
  lanzar las pruebas.  Si se quiere cambiar el host de la cabecera HTTP, es
  mejor cambiar la cabecera `Host` con `httpHeaders`.

* `scheme`, que por defecto es HTTP, pero podemos cambiarlo a HTTPS.  *No se
  valida el certificado*.

* `path`, que por defecto es `/`.  Tiene la parte de ruta de la URL a usar.

* `httpHeaders`, con las cabeceras HTTP que queramos personalizar.  Por
  defecto, se env√≠an las cabeceras `User-Agent: kube-probe/1.24` y `Accept:
  \*/*`.

* `port`, con el n√∫mero de puerto del servidor.

En el siguiente ejemplo, lanzamos un pod con dos contenedores, y configuramos
en uno de ellos una prueba que falla aproximadamente el 10% de las veces,
adem√°s de configurar el valor `failureThreshold` a 1 para reiniciar el
contenedor en cuanto se detecte un fallo:

.pod-2containers-lp.yaml
[source,yaml]
----
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-2containers-lp
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  - name: loop
    image: nginx
    command:  ['sh', '-c', 'while true; do date; sleep 10s; done']
    livenessProbe:
     exec:
       command:
       - bash
       - -c
       - f() { return $(($RANDOM % 10 < 1)); }; f
     initialDelaySeconds: 5
     periodSeconds: 5
     failureThreshold: 1
----

Una vez aplicada esa configuraci√≥n, vemos c√≥mo el contenedor se reinicia cada
vez que falla:

[source,console]
----
$ kubectl get events --field-selector involvedObject.name=pod-2containers-lp -o custom-columns=LATSEEN:.lastTimestamp,COUNT:.count,TYPE:.type,REASON:.reason,OBJECT:.involvedObject.name,MESSAGE:.message --watch
LATSEEN                COUNT   TYPE      REASON      OBJECT               MESSAGE
2022-07-07T15:31:07Z   1       Normal    Scheduled   pod-2containers-lp   Successfully assigned blas/pod-2containers-lp to minikube-m02
2022-07-07T15:31:08Z   1       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:31:09Z   1       Normal    Pulled      pod-2containers-lp   Successfully pulled image "nginx" in 1.333973436s
2022-07-07T15:31:10Z   1       Normal    Created     pod-2containers-lp   Created container nginx
2022-07-07T15:31:10Z   1       Normal    Started     pod-2containers-lp   Started container nginx
2022-07-07T15:31:10Z   1       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:31:11Z   1       Normal    Pulled      pod-2containers-lp   Successfully pulled image "nginx" in 1.405459696s
2022-07-07T15:31:11Z   1       Normal    Created     pod-2containers-lp   Created container loop
2022-07-07T15:31:11Z   1       Normal    Started     pod-2containers-lp   Started container loop
2022-07-07T15:31:17Z   1       Warning   Unhealthy   pod-2containers-lp   Liveness probe failed:
2022-07-07T15:31:17Z   1       Normal    Killing     pod-2containers-lp   Container loop failed liveness probe, will be restarted
2022-07-07T15:31:48Z   2       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:31:49Z   1       Normal    Pulled      pod-2containers-lp   Successfully pulled image "nginx" in 1.371456844s
2022-07-07T15:31:49Z   2       Normal    Created     pod-2containers-lp   Created container loop
2022-07-07T15:31:49Z   2       Normal    Started     pod-2containers-lp   Started container loop
2022-07-07T15:31:57Z   2       Warning   Unhealthy   pod-2containers-lp   Liveness probe failed:
2022-07-07T15:31:57Z   2       Normal    Killing     pod-2containers-lp   Container loop failed liveness probe, will be restarted
2022-07-07T15:32:28Z   3       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:32:29Z   1       Normal    Pulled      pod-2containers-lp   Successfully pulled image "nginx" in 1.34310106s
2022-07-07T15:32:29Z   3       Normal    Created     pod-2containers-lp   Created container loop
2022-07-07T15:32:29Z   3       Normal    Started     pod-2containers-lp   Started container loop
2022-07-07T15:33:17Z   3       Warning   Unhealthy   pod-2containers-lp   Liveness probe failed:
2022-07-07T15:33:17Z   3       Normal    Killing     pod-2containers-lp   Container loop failed liveness probe, will be restarted
2022-07-07T15:33:48Z   4       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:33:49Z   1       Normal    Pulled      pod-2containers-lp   Successfully pulled image "nginx" in 1.327867822s
2022-07-07T15:33:49Z   4       Normal    Created     pod-2containers-lp   Created container loop
2022-07-07T14:34:20Z   8       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:36:13Z   6       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:41:19Z   8       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:46:19Z   1       Warning   BackOff     pod-2containers-lp   Back-off restarting failed container
----

[source,console]
----
$ kubectl get pod pod-2containers-lp --watch
NAME                 READY   STATUS              RESTARTS   AGE
pod-2containers-lp   0/2     Pending             0             0s
pod-2containers-lp   0/2     Pending             0             0s
pod-2containers-lp   0/2     ContainerCreating   0             0s
pod-2containers-lp   2/2     Running             0             5s
pod-2containers-lp   2/2     Running             1 (2s ago)    43s
pod-2containers-lp   2/2     Running             2 (1s ago)    82s
pod-2containers-lp   2/2     Running             3 (1s ago)    2m42s
pod-2containers-lp   2/2     Running             4 (2s ago)    4m18s
pod-2containers-lp   2/2     Running             5 (1s ago)    5m7s
pod-2containers-lp   2/2     Running             6 (2s ago)    6m43s
pod-2containers-lp   1/2     CrashLoopBackOff    6 (1s ago)    7m32s
pod-2containers-lp   2/2     Running             7 (2m43s ago)   10m
pod-2containers-lp   2/2     Running             8 (2s ago)      12m
pod-2containers-lp   1/2     CrashLoopBackOff    8 (1s ago)      13m
----

El estado `CrashLoopBackOff` indica que uno de los pods est√° fallando
intermitentemente y debe investigarse la causa.

== Control de los trabajos

La misi√≥n principal de K8s es asegurarse de los trabajos se ejecutan
adecuadamente, monitoriz√°ndolos y asign√°ndoles los recursos que necesiten.
Para ello disponemos de _workload resources_, recursos que gestionan los
trabajos, como _Deployments_, _ReplicaSets_, _Jobs_...

NOTE: Aunque solo queramos tener una instancia de un pod, en vez de lanzarla
manualmente es mejor utilizar siempre alg√∫n tipo de controlador para garantizar
su funcionamiento.

[[replicasets,_replica sets_]]
=== Replica Sets

Los _replica sets_ (`ReplicaSet`), garantizan que hay un n√∫mero determinado de
r√©plicas de un pod funcionando (levantados y disponibles), creando los que
falten o eliminando los que sobren.  Los pods se sustituyen autom√°ticamente si
fallan, se eliminan o terminan, utilizando para ello la plantilla del pod
especificada en su definici√≥n.  Se tiene en cuenta el estado de los pods en
todos los nodos.

NOTE: Aunque podemos utilizar directamente los _replica sets_, se mejor
utilizar <<deployments>>, que son conceptos de m√°s alto nivel que utilizan
_replica sets_ y proporcionan m√°s funcionalidades.

El siguiente ejemplo define un `ReplicaSet` con tres pods de nginx:

.rs-nginx.yaml
[source,yaml]
----
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
  labels:
    app: app-nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app-nginx
  template:
    metadata:
      name: nginx
      labels:
        app: app-nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
----

[source,console]
----
$ kubectl apply -f rs-nginx.yaml
replicaset.apps/nginx-rs created
----

Estos son los eventos que se producen al ejecutar la orden anterior:

[source,console]
----
$ kubectl get events --watch
1s          Normal    Scheduled          pod/nginx-rs-t46pz    Successfully assigned blas/nginx-rs-t46pz to minikube-m03
1s          Normal    SuccessfulCreate   replicaset/nginx-rs   Created pod: nginx-rs-t46pz
1s          Normal    SuccessfulCreate   replicaset/nginx-rs   Created pod: nginx-rs-z87k5
0s          Normal    Scheduled          pod/nginx-rs-58npq    Successfully assigned blas/nginx-rs-58npq to minikube
0s          Normal    SuccessfulCreate   replicaset/nginx-rs   Created pod: nginx-rs-58npq
0s          Normal    Scheduled          pod/nginx-rs-z87k5    Successfully assigned blas/nginx-rs-z87k5 to minikube-m02
0s          Normal    Pulling            pod/nginx-rs-z87k5    Pulling image "nginx"
0s          Normal    Pulling            pod/nginx-rs-58npq    Pulling image "nginx"
0s          Normal    Pulling            pod/nginx-rs-t46pz    Pulling image "nginx"
0s          Normal    Pulled             pod/nginx-rs-z87k5    Successfully pulled image "nginx" in 1.354562976s
0s          Normal    Pulled             pod/nginx-rs-58npq    Successfully pulled image "nginx" in 1.317435738s
0s          Normal    Created            pod/nginx-rs-z87k5    Created container nginx
0s          Normal    Created            pod/nginx-rs-58npq    Created container nginx
0s          Normal    Pulled             pod/nginx-rs-t46pz    Successfully pulled image "nginx" in 1.379200875s
0s          Normal    Created            pod/nginx-rs-t46pz    Created container nginx
0s          Normal    Started            pod/nginx-rs-58npq    Started container nginx
0s          Normal    Started            pod/nginx-rs-z87k5    Started container nginx
0s          Normal    Started            pod/nginx-rs-t46pz    Started container nginx
----

[source,console]
----
$ kubectl get rs
NAME       DESIRED   CURRENT   READY   AGE
nginx-rs   3         3         3       76s
----

[source,console]
----
$ kubectl describe rs nginx-rs
Name:         nginx-rs
Namespace:    blas
Selector:     app=app-nginx
Labels:       app=app-nginx
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=app-nginx
  Containers:
   nginx:
    Image:        nginx
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  93s   replicaset-controller  Created pod: nginx-rs-7gprq
  Normal  SuccessfulCreate  93s   replicaset-controller  Created pod: nginx-rs-2hlpr
  Normal  SuccessfulCreate  92s   replicaset-controller  Created pod: nginx-rs-ltzt7
----

[source,console]
----
$ kubectl get pods
nginx-rs-2hlpr   1/1     Running   0          2m
nginx-rs-7gprq   1/1     Running   0          2m
nginx-rs-ltzt7   1/1     Running   0          2m
----

El seleccionador `matchLabels` del _replica set_ identifica los pods que ser√°n
controlados por √©l.  Un _replica set_ est√° enlazado con sus pods mediante el
campo `metadata.ownerReferences` de estos, que especifica qu√© recurso es el
propietario de un objeto:

[source,console]
----
$ kubectl get pods nginx-rs-2hlpr -o yaml
kubectl get pods nginx-rs-6wxmb -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2022-06-17T12:10:39Z"
  generateName: nginx-rs-
  labels:
    app: app-nginx
  name: nginx-rs-6wxmb
  namespace: blas
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: nginx-rs
    uid: 0ca66e0f-5951-47dd-a1d3-b4c22a1db7b6
  resourceVersion: "20754"
  uid: 279d249e-668a-4968-80fd-01a45942f805
...
----

Si un nuevo pod cumple con el seleccionador de un _replica set_, ser√° adquirido
por √©l, siempre que no tenga ya un propietario o su propietario no sea un
controlador.  Podemos ver esto con el siguiente ejemplo, donde creamos un nuevo
pod manualmente con la etiqueta del seleccionador usado en nuestro _replica
set_.  El pod se crea, pero se destruye inmediatamente porque ya tenemos los
tres pods del _replica set_ funcionando:

.rs-new-pod.yaml
[source,source]
----
---
apiVersion: v1
kind: Pod
metadata:
  name: new-pod
  labels:
    app: app-nginx
spec:
  containers:
  - name: new-nginx
    image: nginx
    ports:
    - containerPort: 80
----

[source,console]
----
$ kubectl apply -f rs-new-pod.yaml
pod/new-pod created
----

[source,console]
----
$ kubectl get events --watch
0s          Normal    Scheduled          pod/new-pod           Successfully assigned blas/new-pod to minikube-m02
0s          Normal    SuccessfulDelete   replicaset/nginx-rs   Deleted pod: new-pod
0s          Normal    Pulling            pod/new-pod           Pulling image "nginx"
0s          Normal    Pulled             pod/new-pod           Successfully pulled image "nginx" in 1.452625358s
0s          Normal    Created            pod/new-pod           Created container new-nginx
0s          Normal    Started            pod/new-pod           Started container new-nginx
0s          Normal    Killing            pod/new-pod           Stopping container new-nginx
----

[source,console]
----
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
nginx-rs-2hlpr   1/1     Running   0          8m14s
nginx-rs-7gprq   1/1     Running   0          8m14s
nginx-rs-ltzt7   1/1     Running   0          8m14s
----

Si lo hacemos al rev√©s, primero creando el pod y luego el _replica set_, pasa
lo contrario, manteni√©ndose el pod que creamos manualmente y a√±adi√©ndose otros
dos:

[source,console]
----
$ kubectl apply -f rs-new-pod.yaml
pod/new-pod created
----

[source,console]
----
$ kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
new-pod   1/1     Running   0          9s
----

[source,console]
----
$ kubectl apply -f rs-nginx.yaml
replicaset.apps/nginx-rs created
----

[source,console]
----
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
new-pod          1/1     Running   0          35s
nginx-rs-9pg7q   1/1     Running   0          10s
nginx-rs-scjhn   1/1     Running   0          10s
----

Podemos comprobar que el pod creado manualmente ahora est√° controlado por el
_replica set_:

[source,console]
----
$ kubectl describe pod/new-pod
Name:         new-pod
Namespace:    blas
Priority:     0
Node:         minikube-m03/192.168.49.4
Start Time:   Fri, 17 Jun 2022 14:03:13 +0200
Labels:       app=app-nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.10
IPs:
  IP:           10.244.2.10
Controlled By:  ReplicaSet/nginx-rs
...
----

Si eliminamos cualquiera de los pods controlados por el _replica set_, se
sustituye por uno nuevo inmediatamente:

[source,console]
----
$ kubectl delete pod new-pod
pod "new-pod" deleted
----

[source,console]
----
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
nginx-rs-6wxmb   1/1     Running   0          8s
nginx-rs-9pg7q   1/1     Running   0          7m9s
nginx-rs-scjhn   1/1     Running   0          7m9s
----

Al eliminar un _replica set_, se cambia el n√∫mero de objetos controlados por √©l
a 0 para terminarlos, y despu√©s se elimina el propio _replica set_:

[source,console]
----
$ kubectl delete replicaset/nginx-rs
replicaset.apps "nginx-rs" deleted

$ kubectl get pods
No resources found in blas namespace.
----

Se puede eliminar un _replica set_ sin borrar los pods que controla usando la
opci√≥n `--cascade=orphan` de `kubectl delete`.  Esto permitir√≠a, por ejemplo,
sustituir un _replica set_ por otro nuevo para controlar los mismos pods,
aunque, si este tuviera una nueva plantilla para los pods, solo se utilizar√≠a
para los pods nuevos que hubiera que crear.

Otra cosa que puede ser √∫til es hacer que un pod deje de estar controlado por
un _replica set_, cambiando sus etiquetas.

Se puede cambiar al vuelo el n√∫mero de pods controlados por un _replica set_
cambiando su campo `.spec.replicas`.  Los pods se crear√°n o se destruir√°n seg√∫n
sea necesario.  Se puede automatizar esto utilizando un _horizontal pod
autoscaler_, como el siguiente:

[source,yaml]
----
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-scaler
spec:
  scaleTargetRef:
    kind: ReplicaSet
    name: nginx-rs
  minReplicas: 5
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
----

[source,console]
----
$ # Otra forma de crear el autoescalador sin usar un YAML.
$ kubectl autoscale rs nginx-rs --max=10 --min=5 --cpu-percent=50
horizontalpodautoscaler.autoscaling/nginx-rs autoscaled
----

Para que el ejemplo de autoescalado funcione, hace falta tener habilitado el
<<metrics_server>>.  Si no, podemos ver un error en autoescalador:

[source,console]
----
$ kubectl describe horizontalpodautoscalers.autoscaling nginx-rs
Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler
Name:                                                  nginx-rs
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Mon, 20 Jun 2022 12:10:56 +0200
Reference:                                             ReplicaSet/nginx-rs
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  <unknown> / 50%
Min replicas:                                          5
Max replicas:                                          10
ReplicaSet pods:                                       5 current / 5 desired
Conditions:
  Type           Status  Reason                   Message
  ----           ------  ------                   -------
  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target's current scale
  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
Events:
  Type     Reason                        Age   From                       Message
  ----     ------                        ----  ----                       -------
  Normal   SuccessfulRescale             26s   horizontal-pod-autoscaler  New size: 5; reason: Current number of replicas below Spec.MinReplicas
  Warning  FailedGetResourceMetric       10s   horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  Warning  FailedComputeMetricsReplicas  10s   horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
----

[source,console]
----
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
nginx-rs-2z8xx   1/1     Running   0          85s
nginx-rs-c5x48   1/1     Running   0          85s
nginx-rs-clx8v   1/1     Running   0          35s
nginx-rs-k4894   1/1     Running   0          85s
nginx-rs-znxwq   1/1     Running   0          35s
----

En este caso, se han creado dos pods adicionales porque no se cumpl√≠a con el
m√≠nimo pedido en el autoescalador, pero el escalado por uso de la CPU no
funcionar√°.

[[deployments,_deployments_]]
=== Deployments

Los despliegues (_deployments_), son un m√©todo declarativo de gestionar pods
utilizando por debajo <<replicasets>>.  Es la forma recomendada de gestionar
los pods en un cluster de K8s.

El siguiente es un ejemplo de un despliegue compuesto por tres pods de nginx:

.dep-nginx.yaml
[source,yaml]
----
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
----

Al aplicarlo, se crea el `Deployment`, los pods y el <<replicasets,_replica
set_>> que los gestiona:

[source,console]
----

$ kubectl apply -f dep-nginx.yaml
deployment.apps/nginx-deployment created

$ kubectl rollout status deployment/nginx-deployment
Waiting for deployment "nginx-deployment" rollout to finish: 0 of 3 updated replicas are available...
Waiting for deployment "nginx-deployment" rollout to finish: 1 of 3 updated replicas are available...
Waiting for deployment "nginx-deployment" rollout to finish: 2 of 3 updated replicas are available...
deployment "nginx-deployment" successfully rolled out

$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-74d589986c-lhwsg   1/1     Running   0          5s
nginx-deployment-74d589986c-qvqfm   1/1     Running   0          5s
nginx-deployment-74d589986c-vsmbz   1/1     Running   0          5s

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   3         3         3       22s

$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           35s
----

Los pods y los <<replicasets>> gestionados con despliegues tienen en sus
nombres el valor de la etiqueta `pod-template-hash` que el despliegue incluye
en ellos.  Esta etiqueta es un n√∫mero aleatorio calculado usando como semilla
el hash del `PodTemplate`:

[source,console]
----

$ kubectl describe pod/nginx-deployment-74d589986c-lhwsg
Name:         nginx-deployment-74d589986c-lhwsg
Namespace:    default
Priority:     0
Node:         minikube-m03/192.168.49.4
Start Time:   Mon, 20 Jun 2022 12:46:37 +0200
Labels:       app=nginx
              pod-template-hash=74d589986c
...
----

Al eliminar el despliegue, se eliminan todos los recursos que cre√≥:

[source,console]
----
$ kubectl delete deployment nginx-deployment
deployment.apps "nginx-deployment" deleted

$ kubectl get pods
No resources found in default namespace.
----

Los despliegues permiten usar como selectores `matchLabels` y/o
`matchExpressions`, con la condici√≥n de que la plantilla del pod cumpla con
ellos.

==== Actualizaci√≥n de un despliegue

Cuando se cambia la plantilla de los pods de un despliegue, se lanza el
despliegue para aplicar los cambios (se hace un _rollout_).  La forma de
aplicar los cambios depende de la estrategia configurada en el campo
`.spec.strategy` del despliegue, que puede ser `Recreate` o `RollingUpdate`,
que es el valor por defecto.

Con la estrategia `Recreate`, primero se eliminan todos los pods actuales y
despu√©s se crean los nuevos.  Esto no es muy recomendable, porque si falla la
creaci√≥n de los nuevos pods nos quedaremos sin servicio.

La estrategia `RollingUpdate` crea nuevos pods con la nueva plantilla y elimina
los antiguos por tandas, hasta sustituirlos todos.  Por defecto, y siempre sin
contar los pods que est√©n en estado _terminating_, se permite tener hasta un
125% m√°s de pods que el m√°ximo permitido (un 25% de aumento), y se garantiza
que al menos se tiene un 75% del n√∫mero deseado levantados (un 25% no
disponible).  Estos valores pueden configurarse en los campos
`.spec.strategy.rollingUpdate.maxUnavailable` y
`.spec.strategy.rollingUpdate.maxSurge` del despliegue, que pueden ser valores
absolutos o porcentajes sobre el n√∫mero de pods deseados, que se redondean
hacia abajo o hacia arriba, respectivamente, para calcular el valor final.

Las siguientes salidas se han obtenido justo despu√©s de editar la definici√≥n
del despliegue anterior para a√±adir una etiqueta en los pods.  Atenci√≥n a c√≥mo
cambia el nombre de las etiquetas de los nuevos recursos creados:

[source,console]
----
$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-74d589986c-c287l   1/1     Running   0          9m21s
nginx-deployment-74d589986c-fgz2b   1/1     Running   0          9m24s
nginx-deployment-74d589986c-qtwlc   1/1     Running   0          9m18s
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-deployment-74d589986c-c287l   1/1     Running             0          9m36s
nginx-deployment-74d589986c-fgz2b   1/1     Running             0          9m39s
nginx-deployment-74d589986c-qtwlc   1/1     Running             0          9m33s
nginx-deployment-795bc797c7-xjh4p   0/1     ContainerCreating   0          2s
$ kubectl get pods
NAME                                READY   STATUS        RESTARTS   AGE
nginx-deployment-74d589986c-c287l   1/1     Terminating   0          9m37s
nginx-deployment-74d589986c-fgz2b   1/1     Running       0          9m40s
nginx-deployment-74d589986c-qtwlc   1/1     Running       0          9m34s
nginx-deployment-795bc797c7-xjh4p   1/1     Running       0          3s
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-deployment-74d589986c-fgz2b   1/1     Running             0          9m41s
nginx-deployment-74d589986c-qtwlc   1/1     Running             0          9m35s
nginx-deployment-795bc797c7-q7x74   0/1     ContainerCreating   0          1s
nginx-deployment-795bc797c7-xjh4p   1/1     Running             0          4s
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-deployment-74d589986c-fgz2b   1/1     Running             0          9m43s
nginx-deployment-74d589986c-qtwlc   1/1     Running             0          9m37s
nginx-deployment-795bc797c7-q7x74   0/1     ContainerCreating   0          3s
nginx-deployment-795bc797c7-xjh4p   1/1     Running             0          6s
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-deployment-74d589986c-fgz2b   1/1     Terminating         0          9m44s
nginx-deployment-74d589986c-qtwlc   1/1     Running             0          9m38s
nginx-deployment-795bc797c7-nmst8   0/1     ContainerCreating   0          1s
nginx-deployment-795bc797c7-q7x74   1/1     Running             0          4s
nginx-deployment-795bc797c7-xjh4p   1/1     Running             0          7s
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-deployment-74d589986c-qtwlc   1/1     Running             0          9m39s
nginx-deployment-795bc797c7-nmst8   0/1     ContainerCreating   0          2s
nginx-deployment-795bc797c7-q7x74   1/1     Running             0          5s
nginx-deployment-795bc797c7-xjh4p   1/1     Running             0          8s
$ kubectl get pods
NAME                                READY   STATUS        RESTARTS   AGE
nginx-deployment-74d589986c-qtwlc   1/1     Terminating   0          9m40s
nginx-deployment-795bc797c7-nmst8   1/1     Running       0          3s
nginx-deployment-795bc797c7-q7x74   1/1     Running       0          6s
nginx-deployment-795bc797c7-xjh4p   1/1     Running       0          9s
$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-795bc797c7-nmst8   1/1     Running   0          4s
nginx-deployment-795bc797c7-q7x74   1/1     Running   0          7s
nginx-deployment-795bc797c7-xjh4p   1/1     Running   0          10s

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   0         0         0       3h58m
nginx-deployment-795bc797c7   3         3         3       42s
----

Se puede ver c√≥mo se van modificando los valores de los <<replicasets>>
gestionados por los despliegues viendo los eventos "scaled up" y "scaled down"
de estos:

[source,console]
----
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Mon, 20 Jun 2022 12:46:37 +0200
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 6
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
           etiqueta=blas
  Containers:
   nginx:
    Image:        nginx
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deployment-795bc797c7 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  76s   deployment-controller  Scaled up replica set nginx-deployment-795bc797c7 to 1
  Normal  ScalingReplicaSet  73s   deployment-controller  Scaled down replica set nginx-deployment-74d589986c to 2
  Normal  ScalingReplicaSet  73s   deployment-controller  Scaled up replica set nginx-deployment-795bc797c7 to 2
  Normal  ScalingReplicaSet  70s   deployment-controller  Scaled up replica set nginx-deployment-795bc797c7 to 3
  Normal  ScalingReplicaSet  70s   deployment-controller  Scaled down replica set nginx-deployment-74d589986c to 1
  Normal  ScalingReplicaSet  67s   deployment-controller  Scaled down replica set nginx-deployment-74d589986c to 0
----

Si deshacemos el cambio, la plantilla del pod vuelve a quedar como estaba, y el
valor del campo `pod-template-hash` de los nuevos recursos que se crean como
parte del lanzamiento del despliegue coincide con el que ten√≠amos
originalmente.

==== Historia de los despliegues

K8s guarda la historia de los lanzamientos hechos con un despliegue,
conservando los <<replicasets>> correspondientes:

[source,console]
----
$ kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
2         <none>
5         <none>
6         <none>
----

La columna `CHANGE-CAUSE` se obtiene del campo `kubernetes.io/change-cause` del
despliegue, que se puede establecer a√±adiendo la opci√≥n `--record` a la orden
`kubectl` que provoc√≥ el cambio, en cuyo caso se guardar√° la orden en la
descripci√≥n del cambio, o cambiando la anotaci√≥n de la versi√≥n actual del
despliegue con `kubectl annotate deployment/XXXXX
kubernetes.io/change-cause="blablabla"`.

Por defecto, se guardan 10 versiones, pero se puede cambiar este valor
cambiando el campo del despliegue `.spec.revisionHistoryLimit`:

[source,console]
----
$ kubectl get deploy/nginx-deployment -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
...
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
...
----

Se puede ver c√≥mo es cada versi√≥n a√±adiendo la opci√≥n `--revision=<n>`.  En
este caso, solo cambian las etiquetas entre versiones:

[source,console]
----
$ kubectl rollout history deployment/nginx-deployment --revision=2
deployment.apps/nginx-deployment with revision #2
Pod Template:
  Labels:       app=nginx
        otra=blas
        pod-template-hash=7678d86c77
  Containers:
   nginx:
    Image:      nginx
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

$ kubectl rollout history deployment/nginx-deployment --revision=5
deployment.apps/nginx-deployment with revision #5
Pod Template:
  Labels:       app=nginx
        pod-template-hash=74d589986c
  Containers:
   nginx:
    Image:      nginx
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

$ kubectl rollout history deployment/nginx-deployment --revision=6
deployment.apps/nginx-deployment with revision #6
Pod Template:
  Labels:       app=nginx
        etiqueta=blas
        pod-template-hash=795bc797c7
  Containers:
   nginx:
    Image:      nginx
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
----

Se puede volver a la versi√≥n anterior de un despliegue as√≠:

[source,console]
----
$ kubectl rollout undo deployment/nginx-deployment
deployment.apps/nginx-deployment rolled back
----

Se puede volver a una versi√≥n concreta a√±adiendo `--to-revision=<n>` a la orden
anterior.

==== Escalado

Se puede cambiar los par√°metros de escalado de un despliegue con `kubectl
scale`.  Como eso con cambia la plantilla del pod del despliegue, no se generan
nuevas versiones de la historia:

[source,console]
----
$ kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
2         <none>
10        <none>
11        <none>

$ kubectl scale deployment/nginx-deployment --replicas=10
deployment.apps/nginx-deployment scaled

$ kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
2         <none>
10        <none>
11        <none>

$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-74d589986c-4rwmk   1/1     Running   0          15s
nginx-deployment-74d589986c-8r858   1/1     Running   0          15s
nginx-deployment-74d589986c-9dncr   1/1     Running   0          4m26s
nginx-deployment-74d589986c-fvqgv   1/1     Running   0          15s
nginx-deployment-74d589986c-kvzwv   1/1     Running   0          15s
nginx-deployment-74d589986c-lmnlk   1/1     Running   0          4m23s
nginx-deployment-74d589986c-p5cpp   1/1     Running   0          15s
nginx-deployment-74d589986c-s7kpl   1/1     Running   0          15s
nginx-deployment-74d589986c-v8kdw   1/1     Running   0          15s
nginx-deployment-74d589986c-xg2jl   1/1     Running   0          4m20s
----

Si tenemos habilitado el autoescalado horizontal en el cluster, se puede
configurar un autoescalado basado en el consumo de CPU:

[source,console]
----
$ kubectl autoscale deployment/nginx-deployment --min=3 --max=10 --cpu-percent=2
horizontalpodautoscaler.autoscaling/nginx-deployment autoscaled

$ kubectl get horizontalpodautoscalers.autoscaling
NAME               REFERENCE                     TARGETS        MINPODS   MAXPODS   REPLICAS   AGE
nginx-deployment   Deployment/nginx-deployment   <unknown>/2%   3         10        3          43s
----

Como puede verse, la orden anterior crea un `HorizontalPodAutoscaler`
(abreviado, `hpa`), del que podemos ver los detalles con `kubectl get`, en YAML
o en JSON:

[source,console]
----
$ kubectl get hpa nginx-deployment -o=yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: "2022-06-21T15:24:36Z"
  name: nginx-deployment
  namespace: blas
  resourceVersion: "64059"
  uid: 3652214e-da75-4848-aa3e-ef4b59a181f0
spec:
  maxReplicas: 10
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 2
        type: Utilization
    type: Resource
  minReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
status:
  conditions:
  - lastTransitionTime: "2022-06-21T15:24:51Z"
    message: the HPA controller was able to get the target's current scale
    reason: SucceededGetScale
    status: "True"
    type: AbleToScale
  - lastTransitionTime: "2022-06-21T15:24:51Z"
    message: 'the HPA was unable to compute the replica count: failed to get cpu utilization:
      missing request for cpu'
    reason: FailedGetResourceMetric
    status: "False"
    type: ScalingActive
  currentMetrics: null
  currentReplicas: 3
  desiredReplicas: 0
----

[source,console]
----
$ kubectl get horizontalpodautoscalers/nginx-deployment -o=json
{
    "apiVersion": "autoscaling/v2",
    "kind": "HorizontalPodAutoscaler",
    "metadata": {
        "creationTimestamp": "2022-06-21T15:24:36Z",
        "name": "nginx-deployment",
        "namespace": "blas",
        "resourceVersion": "64059",
        "uid": "3652214e-da75-4848-aa3e-ef4b59a181f0"
    },
    "spec": {
        "maxReplicas": 10,
        "metrics": [
            {
                "resource": {
                    "name": "cpu",
                    "target": {
                        "averageUtilization": 2,
                        "type": "Utilization"
                    }
                },
                "type": "Resource"
            }
        ],
        "minReplicas": 3,
        "scaleTargetRef": {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "name": "nginx-deployment"
        }
    },
    "status": {
        "conditions": [
            {
                "lastTransitionTime": "2022-06-21T15:24:51Z",
                "message": "the HPA controller was able to get the target's current scale",
                "reason": "SucceededGetScale",
                "status": "True",
                "type": "AbleToScale"
            },
            {
                "lastTransitionTime": "2022-06-21T15:24:51Z",
                "message": "the HPA was unable to compute the replica count: failed to get cpu utilization: missing request for cpu",
                "reason": "FailedGetResourceMetric",
                "status": "False",
                "type": "ScalingActive"
            }
        ],
        "currentMetrics": null,
        "currentReplicas": 3,
        "desiredReplicas": 0
    }
}
----

Podemos cambiar los par√°metros del autoescalador en JSON:

[source,console]
----
$ kubectl patch hpa nginx-deployment --patch '{"spec":{"minReplicas":5}}'
horizontalpodautoscaler.autoscaling/nginx-deployment patched

$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-74d589986c-26ch9   1/1     Running   0          8s
nginx-deployment-74d589986c-fgz2t   1/1     Running   0          24m
nginx-deployment-74d589986c-ngph9   1/1     Running   0          24m
nginx-deployment-74d589986c-p7q86   1/1     Running   0          24m
nginx-deployment-74d589986c-r6dlq   1/1     Running   0          8s
----

==== Escalado proporcional

Si pidi√©ramos escalar un despliegue que estuviera en mitad de un lanzamiento,
por ejemplo, incrementando el n√∫mero de r√©plicas deseadas, las nuevas
instancias se repartir√≠an entre los `ReplicaSet` que estuvieran activos de
manera proporcional al n√∫mero de r√©plicas deseado en cada uno de ellos.  A esto
se le llama _proportional scaling_:

[source,console]
----
$ kubectl get deployments
No resources found in blas namespace.

$ kubectl apply -f dep-nginx.yaml
deployment.apps/nginx-deployment created

$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           8s

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   3         3         3       17s

$ kubectl scale deployment nginx-deployment --replicas=10
deployment.apps/nginx-deployment scaled

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   10        10        10      57s

$ # Actualizamos el despliegue con una imagen que no existe para mantener
$ # activos dos ReplicaSets.
$ kubectl set image deployment/nginx-deployment nginx=nginx:blas
deployment.apps/nginx-deployment image updated

$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   8/10    5            8           91s

$ # Vemos los dos ReplicaSets activos, el antiguo con m√°s r√©plicas deseadas
$ # que el nuevo, por estar en mitad de un rollout.
$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   8         8         8       95s
nginx-deployment-dd56879bf    5         5         0       16s

$ # Subimos el n√∫mero de r√©plicas deseadas del despliegue a 20.
$ kubectl scale deployment nginx-deployment --replicas=20
deployment.apps/nginx-deployment scaled

$ # Y comprobamos que se han asignado a los dos ReplicaSets de forma
$ # propocional.
$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   15        15        15      2m4s
nginx-deployment-dd56879bf    10        10        0       45s
----

==== Pausa y reanudaci√≥n de lanzamientos

Es posible poner en pausa los lanzamientos de un despliegue, lo que es √∫til si
queremos hacer varios cambios en √©l y evitar que cada uno de los cambios
provoque un lanzamiento.  Para hacerlo, se usa la orden `kubectl rollout pause
<despliegue>`, y para reanudarlos de nuevo se usa `kubectl rollour resume
<despliegue>`.

== CLI de kubectl

`kubectl` es el cliente m√°s habitual para trabajar con la API de K8s.  Funciona
por l√≠nea de comandos, y su configuraci√≥n se guarda en `~/.kube/config`,
incluyendo la URL del cluster y las credenciales de autenticaci√≥n.

Los archivos de configuraci√≥n de `kubectl` se conocen como _kubeconfigs_.  Se
puede decir a `kubectl` qu√© archivo usar con la opci√≥n global
`--kubeconfig=<archivo>`.

=== Autocompletado

`kubectl completion <shell>` genera las √≥rdenes necesarias para tener
autocompletado con distintos shells.  Para `fish`, basta con meter lo siguiente
en `~/.config/fish/config.fish`:

[source]
----
kubectl completion fish | source
----

=== Informaci√≥n sobre el cluster

[[kubectl_api_resources,`kubectl api-resources`]]
==== kubectl api-resources

Muestra los recursos disponibles a trav√©s de la API del cluster:

[source,console]
----
$ kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
pods                              po           v1                                     true         Pod
podtemplates                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount
services                          svc          v1                                     true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v2                         true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1                               true         CronJob
jobs                                           batch/v1                               true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1                 true         Lease
endpointslices                                 discovery.k8s.io/v1                    true         EndpointSlice
events                            ev           events.k8s.io/v1                       true         Event
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
ingresses                         ing          networking.k8s.io/v1                   true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1                              true         PodDisruptionBudget
podsecuritypolicies               psp          policy/v1beta1                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1           true         Role
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
csistoragecapacities                           storage.k8s.io/v1beta1                 true         CSIStorageCapacity
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
----

[[kubectl_api_versions,`kubectl api-versions`]]
==== kubectl api-versions

Muestra las API soportadas por un cluster de K8s:

[source,console]
----
$ kubectl api-versions
admissionregistration.k8s.io/v1
apiextensions.k8s.io/v1
apiregistration.k8s.io/v1
apps/v1
authentication.k8s.io/v1
authorization.k8s.io/v1
autoscaling/v1
autoscaling/v2
autoscaling/v2beta1
autoscaling/v2beta2
batch/v1
batch/v1beta1
certificates.k8s.io/v1
coordination.k8s.io/v1
discovery.k8s.io/v1
discovery.k8s.io/v1beta1
events.k8s.io/v1
events.k8s.io/v1beta1
flowcontrol.apiserver.k8s.io/v1beta1
flowcontrol.apiserver.k8s.io/v1beta2
networking.k8s.io/v1
node.k8s.io/v1
node.k8s.io/v1beta1
policy/v1
policy/v1beta1
rbac.authorization.k8s.io/v1
scheduling.k8s.io/v1
storage.k8s.io/v1
storage.k8s.io/v1beta1
v1
----

[[kubectl_cluster_info,kubectl cluster-info]]
==== kubectl cluster-info [dump]

Muestra informaci√≥n sobre el cluster, incluyendo el punto de entrada a la API.
Con la opci√≥n `dump`, se muestra informaci√≥n completa en formato JSON:

[source,console]
----
$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.49.2:8443
CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
----

[[kubectl_describe,`kubectl describe`]]
==== kubectl describe

Muestra los detalles de un recurso o de un grupo de recursos:

[source,console]
----
$ kubectl describe node minikube
Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_06_16T16_57_10_0700
                    minikube.k8s.io/version=v1.25.2
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 16 Jun 2022 16:57:05 +0200
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 17 Jun 2022 09:39:46 +0200
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 17 Jun 2022 09:39:44 +0200   Thu, 16 Jun 2022 16:57:03 +0200   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 17 Jun 2022 09:39:44 +0200   Thu, 16 Jun 2022 16:57:03 +0200   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 17 Jun 2022 09:39:44 +0200   Thu, 16 Jun 2022 16:57:03 +0200   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 17 Jun 2022 09:39:44 +0200   Thu, 16 Jun 2022 16:57:40 +0200   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  228250020Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16313948Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  228250020Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16313948Ki
  pods:               110
System Info:
  Machine ID:                 b6a262faae404a5db719705fd34b5c8b
  System UUID:                b37357ed-52bd-4e93-81cd-d9d47eff6cd3
  Boot ID:                    b7cc6ccb-cb87-4399-b045-b6f3a8511c4c
  Kernel Version:             5.18.0-1-amd64
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.12
  Kubelet Version:            v1.23.4-rc.0
  Kube-Proxy Version:         v1.23.4-rc.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-64897985d-xrxk5             100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     16h
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         16h
  kube-system                 kindnet-mb27w                       100m (2%)     100m (2%)   50Mi (0%)        50Mi (0%)      16h
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         16h
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         16h
  kube-system                 kube-proxy-7bdr4                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         16h
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%)  100m (2%)
  memory             220Mi (1%)  220Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>
----

[source,console]
----
$ kubectl describe pod coredns-64897985d-xrxk5 -n kube-system
Name:                 coredns-64897985d-xrxk5
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 minikube/192.168.49.2
Start Time:           Thu, 16 Jun 2022 16:57:40 +0200
Labels:               k8s-app=kube-dns
                      pod-template-hash=64897985d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/coredns-64897985d
Containers:
  coredns:
    Container ID:  docker://594e4896eef7d215bd27cc94c092441b3e7ef6b74f96eb9a5b51697d229d50ec
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      docker-pullable://k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Thu, 16 Jun 2022 16:57:46 +0200
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zvnwb (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-zvnwb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
----

[[kubectl_get,`kubectl get`]]
==== kubectl get

Devuelve distinta informaci√≥n sobre el cluster, como los nodos, los
<<pod,pods>> que hay corriendo...

[source,console]
----
$ kubectl get nodes -o wide

NAME           STATUS   ROLES                  AGE   VERSION        INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
minikube       Ready    control-plane,master   16h   v1.23.4-rc.0   192.168.49.2   <none>        Ubuntu 20.04.2 LTS   5.18.0-1-amd64   docker://20.10.12
minikube-m02   Ready    <none>                 16h   v1.23.4-rc.0   192.168.49.3   <none>        Ubuntu 20.04.2 LTS   5.18.0-1-amd64   docker://20.10.12
minikube-m03   Ready    <none>                 16h   v1.23.4-rc.0   192.168.49.4   <none>        Ubuntu 20.04.2 LTS   5.18.0-1-amd64   docker://20.10.12
----

[source,console]
----
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-64897985d-xrxk5            1/1     Running   0          16h
kube-system   etcd-minikube                      1/1     Running   0          16h
kube-system   kindnet-kmx5v                      1/1     Running   0          16h
kube-system   kindnet-llx4l                      1/1     Running   0          16h
kube-system   kindnet-mb27w                      1/1     Running   0          16h
kube-system   kube-apiserver-minikube            1/1     Running   0          16h
kube-system   kube-controller-manager-minikube   1/1     Running   0          16h
kube-system   kube-proxy-2ldqz                   1/1     Running   0          16h
kube-system   kube-proxy-75x8c                   1/1     Running   0          16h
kube-system   kube-proxy-7bdr4                   1/1     Running   0          16h
kube-system   kube-scheduler-minikube            1/1     Running   0          16h
kube-system   storage-provisioner                1/1     Running   0          16h
----

[source,console]
----
$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
kube-system   coredns-64897985d-xrxk5            1/1     Running   0          16h   10.244.0.2     minikube       <none>           <none>
kube-system   etcd-minikube                      1/1     Running   0          16h   192.168.49.2   minikube       <none>           <none>
kube-system   kindnet-kmx5v                      1/1     Running   0          16h   192.168.49.4   minikube-m03   <none>           <none>
kube-system   kindnet-llx4l                      1/1     Running   0          16h   192.168.49.3   minikube-m02   <none>           <none>
kube-system   kindnet-mb27w                      1/1     Running   0          16h   192.168.49.2   minikube       <none>           <none>
kube-system   kube-apiserver-minikube            1/1     Running   0          16h   192.168.49.2   minikube       <none>           <none>
kube-system   kube-controller-manager-minikube   1/1     Running   0          16h   192.168.49.2   minikube       <none>           <none>
kube-system   kube-proxy-2ldqz                   1/1     Running   0          16h   192.168.49.4   minikube-m03   <none>           <none>
kube-system   kube-proxy-75x8c                   1/1     Running   0          16h   192.168.49.3   minikube-m02   <none>           <none>
kube-system   kube-proxy-7bdr4                   1/1     Running   0          16h   192.168.49.2   minikube       <none>           <none>
kube-system   kube-scheduler-minikube            1/1     Running   0          16h   192.168.49.2   minikube       <none>           <none>
kube-system   storage-provisioner                1/1     Running   0          16h   192.168.49.2   minikube       <none>           <none>
----

[source,console]
----
$ kubectl get pod coredns-64897985d-xrxk5 -n kube-system
NAME                      READY   STATUS    RESTARTS   AGE
coredns-64897985d-xrxk5   1/1     Running   0          16h
----

[source,console]
----
$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   16h
----

Por defecto, hay algunos tipos de recursos dentro de los namespaces que no se
muestran en la salida de `kubectl get all`.  Se puede utilizar lo siguiente
para verlos todos:

[source,console]
----
$ kubectl api-resources --verbs=list --namespaced -o name | xargs -n1 kubectl get --show-kind --ignore-not-found --all-namespaces
NAMESPACE         NAME                                           DATA   AGE
default           configmap/kube-root-ca.crt                     1      16h
kube-node-lease   configmap/kube-root-ca.crt                     1      16h
kube-public       configmap/cluster-info                         4      16h
kube-public       configmap/kube-root-ca.crt                     1      16h
kube-system       configmap/coredns                              1      16h
kube-system       configmap/extension-apiserver-authentication   6      16h
kube-system       configmap/kube-proxy                           2      16h
kube-system       configmap/kube-root-ca.crt                     1      16h
kube-system       configmap/kubeadm-config                       1      16h
kube-system       configmap/kubelet-config-1.23                  1      16h
NAMESPACE     NAME                                 ENDPOINTS                                     AGE
default       endpoints/kubernetes                 192.168.49.2:8443                             16h
kube-system   endpoints/k8s.io-minikube-hostpath   <none>                                        16h
kube-system   endpoints/kube-dns                   10.244.0.2:53,10.244.0.2:53,10.244.0.2:9153   16h
...
----

[[kubectl_get_events,`kubectl_get events`]]
===== kubectl get events

Muestra los eventos que se han producido en el cluster:

[source,console]
----
$ kubectl get events
LAST SEEN   TYPE      REASON      OBJECT                    MESSAGE
32m         Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
27m         Warning   BackOff     pod/pod-2containers-lp   Back-off restarting failed container
24m         Normal    Scheduled   pod/pod-2containers-lp   Successfully assigned blas/pod-2containers-lp to minikube-m03
24m         Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
24m         Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.386338018s
24m         Normal    Created     pod/pod-2containers-lp   Created container nginx
24m         Normal    Started     pod/pod-2containers-lp   Started container nginx
13m         Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
24m         Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.398490586s
18m         Normal    Created     pod/pod-2containers-lp   Created container loop
18m         Normal    Started     pod/pod-2containers-lp   Started container loop
18m         Warning   Unhealthy   pod/pod-2containers-lp   Liveness probe failed:
18m         Normal    Killing     pod/pod-2containers-lp   Container loop failed liveness probe, will be restarted
20m         Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.403924337s
19m         Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.35347142s
18m         Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.342802789s
4m14s       Warning   BackOff     pod/pod-2containers-lp   Back-off restarting failed container
----

Podemos filtrar los eventos utilizando la opci√≥n `--field-selector`:

[source,console]
----
$ kubectl get event --field-selector involvedObject.name=pod-2containers-lp --watch
LAST SEEN   TYPE      REASON      OBJECT                    MESSAGE
15m         Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
10m         Warning   BackOff     pod/pod-2containers-lp   Back-off restarting failed container
7m26s       Normal    Scheduled   pod/pod-2containers-lp   Successfully assigned blas/pod-2containers-lp to minikube-m03
7m25s       Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
7m23s       Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.386338018s
7m23s       Normal    Created     pod/pod-2containers-lp   Created container nginx
7m23s       Normal    Started     pod/pod-2containers-lp   Started container nginx
70s         Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
7m22s       Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.398490586s
69s         Normal    Created     pod/pod-2containers-lp   Created container loop
69s         Normal    Started     pod/pod-2containers-lp   Started container loop
101s        Warning   Unhealthy   pod/pod-2containers-lp   Liveness probe failed:
101s        Normal    Killing     pod/pod-2containers-lp   Container loop failed liveness probe, will be restarted
3m29s       Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.403924337s
2m39s       Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.35347142s
69s         Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.342802789s
0s          Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
0s          Warning   BackOff     pod/pod-2containers-lp   Back-off restarting failed container
0s          Warning   BackOff     pod/pod-2containers-lp   Back-off restarting failed container
0s          Warning   BackOff     pod/pod-2containers-lp   Back-off restarting failed container
----

Para ver los campos disponibles, podemos ver la salida de la orden en formato
YAML o JSON:

[source,console]
----
$ kubectl get events --output yaml
----

[source,yaml]
----
apiVersion: v1
items:
- apiVersion: v1
  count: 31
  eventTime: null
  firstTimestamp: "2022-07-07T08:40:33Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{loop}
    kind: Pod
    name: pod-2containers-lp
    namespace: blas
    resourceVersion: "150054"
    uid: b0f82f82-f36a-49a7-912d-d18e0adc7263
  kind: Event
  lastTimestamp: "2022-07-07T10:05:44Z"
  message: Pulling image "nginx"
  metadata:
    creationTimestamp: "2022-07-07T08:40:33Z"
    name: pod-2containers-lp.16ff7f5ef5fe36ae
    namespace: blas
    resourceVersion: "154763"
    uid: 8aa20f78-0609-4906-a00b-4e21a6b85c3f
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: minikube-m03
  type: Normal
...
----

=== Manipulaci√≥n del cluster

[[kubectl_apply,`kubectl apply`]]
==== kubectl apply

Aplica al cluster la configuraci√≥n indicada en el archivo YAML o JSON
especificado con `-f` (o desde la entrada est√°ndar, con `-f{nbsp}-`), haciendo
los cambios necesarios sobre la configuraci√≥n actual.

Tambi√©n se puede utilizar la opci√≥n `-k` para especificar un archivo
`kustomization.yaml`, que permite hacer referencia a varios archivos donde
especificar los recursos, y asignarles valores comunes, como el namespace o
etiquetas.  Los siguientes ejemplos son de la
https://kubectl.docs.kubernetes.io/references/kubectl/apply/[documentaci√≥n de
`kubectl`]:

[source,yaml]
----
# kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# list of Resource Config to be Applied
resources:
- deployment.yaml

# namespace to deploy all Resources to
namespace: default

# labels added to all Resources
commonLabels:
  app: example
  env: test
----

[source,yaml]
----
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: the-deployment
spec:
  replicas: 5
  template:
    containers:
      - name: the-container
        image: registry/container:latest
----

Se puede usar la orden `edit-last-applied` para editar la √∫ltima configuraci√≥n
aplicada, y `view-last-applied` para mostrarla.

Con la opci√≥n `--prune`, se eliminan los objetos del cluster que no est√©n en la
configuraci√≥n aplicada.


[[kubectl_create,`kubectl create`]]
==== kubectl create

Crea los recursos especificados en el archivo YAML o JSON pasado con la opci√≥n
`-f` (o desde la entrada est√°ndar, con `-f{nbsp}-`).

[[kubectl_delete,`kubectl delete`]]
==== kubectl delete

Elimina los recursos especificados en el archivo YAML o JSON pasado con la
opci√≥n `-f` (o desde la entrada est√°ndar, con `-f{nbsp}-`), o los indicados por
nombre o por etiqueta.

[[kubectl_edit,`kubectl edit`]]
==== kubectl edit

Edita el objeto especificado en el archivo YAML o JSON pasado con la opci√≥n
`-f` (o desde la entrada est√°ndar, con `-f{nbsp}-`), o los indicados por nombre
o por etiqueta.  Utiliza el editor especificado en las variables de entorno
`EDITOR` o `KUBE_EDITOR`, o con `vi` si no est√°n definidas.  Puede editar
varios objetos, pero de uno en uno.

=== Namespaces

Los _namespaces_ son una forma de hacer compartimentos dentro de Kubernetes, de
manera que se puede limitar la visibilidad de los recursos.

Los nombres de los recursos deben de ser √∫nicos dentro de un namespace, pero se
pueden repetir entre namespaces.

El prefijo `kube-` est√° reservado para uso interno de K8s.

Por defecto, hay cuatro namespaces:

* *default*, para los objetos que no est√°n en ning√∫n otro namespace.

* *kube-system*, para los objetos creados y gestionados por K8s.

* *kube-public*, para objetos p√∫blicos que puede ver cualquier usuario, incluso
   sin estar autenticado, y para los recursos que deban ser vistos por el
   cluster completo.

* *kube-node-lease*, para guardar informaci√≥n sobre los heartbeats de los
   nodos, de manera que el plano de control pueda detectar su ca√≠da.

El nombre que los servicios tienen en el DNS de K8s incluye el namespace
(`<servicio>.<namespace>.svc.cluster.local`), por lo que los contenedores solo
ver√°n los servicios que tengan en su propio namespace, a menos que especifiquen
el dominio DNS completo.  Como el nombre de los namespaces se usa en el DNS,
solo deben de tener caracteres v√°lidos para DNS (63 caracteres m√°ximo, solo
letras min√∫sculas o guiones, y empezar y terminar con un car√°cter
alfanum√©rico).

No todos los tipos de recursos pueden estar dentro de un namespace, como los
nodos o los propios namespaces (no se pueden anidar).  Se puede ver la lista
completa as√≠:

[source,console]
----
$ kubectl api-resources --namespaced=false
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
componentstatuses                 cs           v1                                     false        ComponentStatus
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumes                 pv           v1                                     false        PersistentVolume
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
podsecuritypolicies               psp          policy/v1beta1                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
----

==== Namespace por defecto

Se utiliza la opci√≥n global `--namespace` para indicar a `kubectl` el namespace
sobre el que queremos actuar.  Podemos especificar el namespace por defecto
sobre el que queremos actuar en el contexto actual haciendo `kubectl config
set-context --current --namespace=<namespace>`.

==== kubectl create namespace

Permite crear un namespace desde la l√≠nea de comandos, sin necesidad de
utilizar un archivo YAML o JSON:

[source,console]
----
$ kubectl create namespace blas
namespace/blas created

$ kubectl config set-context --current --namespace=blas
Context "minikube" modified.

$ kubectl get pods
No resources found in blas namespace.
----

=== Pods

[[kubectl_attach,`kubectl attach`]]
==== kubectl attach (POD | TYPE/NAME) -c CONTAINER [options]

Conecta los `stdout` y `stderr` del terminal actual con uno de los contenedores
de un pod en ejecuci√≥n.  Se puede especificar el contenedor con la opci√≥n
`--container` (si no se especifica ninguno, se elige el que tenga el nombre
contenido en la anotaci√≥n `kubectl.kubernetes.io/default-container` del pod, o
el primer contenedor del pod si esa anotaci√≥n no est√° definida).

La forma de interrumpir la conexi√≥n depender√° del _runtime_ de contenedores que
estemos usando, pero normalmente se hace pulsando `Ctrl-P`+`Ctrl-Q`, aunque
suele ser configurable.

Por defecto *no* conecta la entrada est√°ndar, pero puede hacerse con la opci√≥n
`--stdin`, con la que podemos usar adem√°s `--tty` para indicar que queremos que
funcione en modo interactivo, como una terminal, para poder pasar las se√±ales
de control generadas con el teclado.

[[kubectl_exec,`kubectl exec`]]
==== kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...]

Ejecuta una orden en uno de los contenedores de un pod.  Como ocurre con
<<kubectl_attach>>, solo conecta las corrientes `stdout` y `stderr` del
terminal a la orden, a menos que se ejecute con la opci√≥n `--stdin` y,
opcionalmente, con `--tty` si queremos conectar la terminal actual.

La elecci√≥n del contenedor donde se ejecuta la orden se hace igual que con
<<kubectl_attach>>.

[[kubectl_logs,`kubectl logs`]]
==== kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER] [options]

Muestra los registros de uno de los contenedores de un pod o del recurso que se
especifique.

Con la opci√≥n `--all-containers=true`, podemos ver los registros de todos los
contenedores de un pod.

Con `-f`, el proceso seguir√° mostrando los registros a medida que se vayan
generando.

Con `-p`, podemos ver los registros de la instancia previa del contenedor, si
es que hubo una.

Con `-l`, podemos elegir los contenedores utilizando seleccionadores de
igualdad (ver <<seleccionadores>>).

Con `--since`, podemos especificar que queremos ver los registros desde el
tiempo relativo que especifiquemos (p. ej, `--since=10m` para ver los de los
√∫tlimos 10 minutos).

[[kubectl_top,`kubectl top`]]
==== kubectl top (node | pod) [NAME | -l label]

Muestra el uso de los recursos de un nodo o de un pod.  Solo funciona si
tenemos corriendo en el cluster la API de m√©tricas proporcionada por
<<metrics_server>>.

[source,console]
----
$ kubectl top node
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
minikube       196m         4%     1456Mi          9%
minikube-m02   51m          1%     405Mi           2%
minikube-m03   44m          1%     357Mi           2%
----

=== Contextos

La informaci√≥n de la configuraci√≥n de `kubectl` se agrupa en _contextos_ con
nombre.  `kubectl` permite consultar el contexto actual y cambiar de contexto.

==== kubectl config current-context

Muestra el contexto que usa `kubectl`:

[source,console]
----
$ kubectl config current-context
minikube
----


==== kubectl config get-contexts <contexto>

Muestra los contextos disponibles en la configuraci√≥n, o la informaci√≥n de uno
concreto:

[source,console]
----
$ kubectl config get-contexts
CURRENT   NAME       CLUSTER    AUTHINFO   NAMESPACE
*         minikube   minikube   minikube   default

$ kubectl config get-contexts minikube
CURRENT   NAME       CLUSTER    AUTHINFO   NAMESPACE
*         minikube   minikube   minikube   default
----

==== kubectl config use-context <contexto>

Alias: `kubectl config use`.

Cambia el contexto actual.

==== kubectl config set-context <contexto>

Modifica un contexto:


[source,console]
----
$ kubectl config set-context minikube --namespace=blas
Context "minikube" modified.
----

==== kubectl config view

Muestra el archivo _kubeconfig_ actual:

[source,console]
----
$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/jcouto/.minikube/ca.crt
    extensions:
    - extension:
        last-update: Thu, 16 Jun 2022 16:57:10 CEST
        provider: minikube.sigs.k8s.io
        version: v1.25.2
      name: cluster_info
    server: https://192.168.49.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    extensions:
    - extension:
        last-update: Thu, 16 Jun 2022 16:57:10 CEST
        provider: minikube.sigs.k8s.io
        version: v1.25.2
      name: context_info
    namespace: default
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/jcouto/.minikube/profiles/minikube/client.crt
    client-key: /home/jcouto/.minikube/profiles/minikube/client.key

$ diff ~/.kube/config (kubectl config view | psub)
$
----

== Troubleshooting

Las √≥rdenes m√°s √∫tiles para ver lo que ocurre dentro del cluster de K8s son
<<kubectl_get>>,  <<kubectl_get_events>> y <<kubectl_describe>>.

== Otras herramientas

=== kustomize

https://github.com/kubernetes-sigs/kustomize[kustomize] sirve para aplicar
cambios a plantillas YAML de K8s sin tener que modificar los archivos
originales, combinando funcionalidades de herramientas como `make` y `sed`.

=== helm

WARNING: TODO

== Para saber m√°s

* https://kubernetes.io/docs/home/[Documentaci√≥n oficial de Kubernetes].

== Glosario

kubeconfig:: Archivo de configuraci√≥n de `kubectl`, generalmente ubicado en
`~/.kube/config`.

[[label,_label_]]
label:: Las etiquetas son parejas de clave/valor que se asignan a los objetos
de K8s, y se pueden utilizar en los seleccionadores para hacer referencia a los
objetos que tengan determinadas etiquetas.

[[pod,pod]]
pod:: Unidad m√≠nima de proceso de Kubernetes, consistente en un entorno para
ejecutar contenedores donde comparten vol√∫menes, _namespaces_ y _cgroups_.  El
contenido de un pod se lanza en un √∫nico nodo, y se gestiona como un todo.
Todos los contenedores de un pod comparten la direcci√≥n IP 127.0.0.1 y la
pueden usar para comunicarse entre ellos.

[[selector,_selector_]]
selector:: Filtro que utiliza etiquetas para elegir objetos.  Por ejemplo, se
puede utilizar `nodeSelector` en la definici√≥n de un pod para indicar que solo
debe ejecutarse en los nodos que tengan las etiquetas indicadas.
