= Kubernetes
:tags: Publish
:author: Jose Couto
:email: jcouto
:date: junio 2022
:revdate: 20220602
:source-highlighter: rouge
:toc:
:toc-title: √çndice
:toclevels: 3
:numbered:
:appendix-caption: Ap√©ndice
:figure-caption: Figura
:table-caption: Tabla

== Kubernetes

Kubernetes (abreviado, K8s), es un orquestador de contenedores.  Se encarga de
ejecutarlos cuando se cumplan las condiciones adecuadas y de vigilar que tengan
los recursos que necesiten.  Corre en un cluster de _nodos_, en el que uno o
varios de ellos (los _masters_), ejecutan el __control plane_, que controla el
cluster y tiene los siguientes componentes:

* `etcd`, que guarda el estado completo del cluster: los par√°metros de
   configuraci√≥n, las especificaciones y el estado de los trabajos.

* `kube-controller-manager`, encargado de ejecutar los distintos bucles de
   control o _controllers_ necesarios para alcanzar el estado deseado del
   cluster, como los siguientes:

** _Node controller_, que vigila los nodos y responde ante sus cambios.

** _Job controller_, que vigila si es necesario lanzar trabajos y crea
   <<pod,pods>> para hacerlo.

** _Endpoints controller_, que asocia los servicios con los <<pod,pods>>.

** _Service accounts_ y _Token controlers_, que crean las cuentas y los tokens
   de acceso para las API de nuevos espacios de nombres.

* `kube-scheduler`, que decide en qu√© nodo ejecutar un nuevo <<pod>>
   ajust√°ndose lo m√°s posible a sus necesidades.

* `kube-apiserver`, que proporciona una API para trabajar con K8s, y cuyo
   principal cliente es la orden `kubectl`.

Los clusters que corren en un proveedor en la nube tambi√©n tienen un componente
llamado `cloud-controller-manager`, que se encarga de ejecutar los
controladores espec√≠ficos para gestionar los recursos del proveedor.

Todos los nodos son capaces de ejecutar trabajos, aunque suele evitarse hacer
esto en los nodos que corren el control plane.  Para ejecutar trabajos, los
nodos tienen:

* Un proceso llamado `kubelet` que se encarga de comunicarse con el control
  plane y ejecutar lo que les pidan.

* El proceso `kube-proxy`, que se encarga de gestionar las reglas de red de los
  nodos para permitir la comunicaci√≥n con los <<pod,pods>> tanto dentro del
  cluster como fuera.  Implementan el concepto de _servicio_.  Utiliza las
  reglas de filtrado del sistema operativo si est√°n disponibles, o hace de
  proxy a nivel de aplicaci√≥n si no.

* Un _runtime_ para ejecutar los contenedores, como Docker, containerd, CRI-O,
  rktlet...

== Instalaci√≥n de un cluster de pruebas

Podemos instalar un cluster de K8s en un equipo con Linux y Docker o Podman
(para contenedores _rootless_), utilizando herramientas como
https://kind.sigs.k8s.io/[kind] o https://minikube.sigs.k8s.io[minikube].  Kind
funciona mejor con Podman, pero solo crea clusters con un nodo.  Para mis
pruebas, utilizar√© minikube con Docker, con un nodo master y dos adicionales,
todo ello corriendo en una distribuci√≥n Debian Sid..

=== Instalaci√≥n de minikube sobre docker

Instalamos Docker desde los repositorios de Debian:

[source,console]
----
$ sudo apt install docker.io
...

$ docker version
Client:
 Version:           20.10.14+dfsg1
 API version:       1.41
 Go version:        go1.18.1
 Git commit:        a224086
 Built:             Sun May  1 19:59:40 2022
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      true

Server:
 Engine:
  Version:          20.10.14+dfsg1
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.18.1
  Git commit:       87a90dc
  Built:            Sun May  1 19:59:40 2022
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.6~ds1
  GitCommit:        1.6.6~ds1-1
 runc:
  Version:          1.1.1+ds1
  GitCommit:        1.1.1+ds1-1+b1
 docker-init:
  Version:          0.19.0
  GitCommit:
----

Descargamos e instalamos el paquete de Debian de minikube, que solo tiene el
ejecutable.

[source,console]
----
$ cd /tmp
$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 25.3M  100 25.3M    0     0  9556k      0  0:00:02  0:00:02 --:--:-- 9556k

$ dpkg -I minikube_latest_amd64.deb
 new Debian package, version 2.0.
 size 26558662 bytes: control archive=409 bytes.
     406 bytes,    12 lines      control
 Package: minikube
 Version: 1.26.0-0
 Section: base
 Priority: optional
 Architecture: amd64
 Recommends: virtualbox
 Maintainer: Thomas Str√∂mberg <t+minikube@stromberg.org>
 Description: Minikube
  minikube is a tool that makes it easy to run Kubernetes locally.
  minikube runs a single-node Kubernetes cluster inside a VM on your
  laptop for users looking to try out Kubernetes or develop with it
  day-to-day.

$ sudo dpkg -i minikube_latest_amd64.deb
(Reading database ... 214618 files and directories currently installed.)
Preparing to unpack minikube_latest_amd64.deb ...
Unpacking minikube (1.26.0-0) over (1.25.2-0) ...
Setting up minikube (1.26.0-0) ...

$ dpkg -L minikube
/.
/usr
/usr/bin
/usr/bin/minikube
----

Lanzamos `minikube` para que levante tres nodos sobre Docker:

[source,console]
----
$ minikube start --kubernetes-version=latest --driver=docker --nodes=3
üòÑ  minikube v1.26.0 on Debian bookworm/sid
‚ú®  Using the docker driver based on user configuration
üìå  Using Docker driver with root privileges
üëç  Starting control plane node minikube in cluster minikube
üöú  Pulling base image ...
üíæ  Downloading Kubernetes v1.24.1 preload ...
    > preloaded-images-k8s-v18-v1...: 405.83 MiB / 405.83 MiB  100.00% 5.38 MiB
    > gcr.io/k8s-minikube/kicbase: 386.00 MiB / 386.00 MiB  100.00% 3.95 MiB p/
    > gcr.io/k8s-minikube/kicbase: 0 B [_________________________] ?% ? p/s 53s
üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîó  Configuring CNI (Container Networking Interface) ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass

üëç  Starting worker node minikube-m02 in cluster minikube
üöú  Pulling base image ...
üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=192.168.49.2
üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    ‚ñ™ env NO_PROXY=192.168.49.2
üîé  Verifying Kubernetes components...

üëç  Starting worker node minikube-m03 in cluster minikube
üöú  Pulling base image ...
üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=192.168.49.2,192.168.49.3
üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    ‚ñ™ env NO_PROXY=192.168.49.2
    ‚ñ™ env NO_PROXY=192.168.49.2,192.168.49.3
üîé  Verifying Kubernetes components...
üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
----

minikube crea una configuraci√≥n para `kubectl` en `~/kube/config` para
permitirle conectarse al cluster reci√©n creado.

[[metrics_server,`metrics-server`]]
=== Instalaci√≥n de metrics server

Algunas funciones de K8s, como la obtenci√≥n de m√©tricas de los pods con
<<kubectl_top>> o el autoescalado horizontal, necesitan que est√© instalado el
paquete https://github.com/kubernetes-sigs/metrics-server[Kubernetes Metrics
Server], que se puede desplegar sobre minikube siguiendo un remiendo
documentado
https://github.com/kubernetes-sigs/metrics-server/issues/196#issuecomment-451061841[aqu√≠],
que hace falta porque minikube usa certificados digitales autofirmados para los
`kubelet` de los nodos:

[source,console]
----
$ curl -sL https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml | sed -e '/cert-dir/p' -e '0,/cert-dir/s/cert-dir.*/kubelet-insecure-tls/'| kubectl apply -f -
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
----

=== Instalaci√≥n de kubectl

Aunque recomienda definir el alias `kubectl='minikube kubectl --'` para
utilizar su propio cliente de `kubectl`, para garantizar que usamos la misma
versi√≥n del cliente y del servidor, pero con √©l
https://github.com/kubernetes/minikube/issues/12938[no funciona el
autocompletado].  En Debian, podemos instalar `kubectl` con un snap, aunque la
versi√≥n es distinta que la de minikube:

[source,console]
----
$ sudo snap install kubectl --classic
2022-06-10T18:41:03+02:00 INFO Waiting for automatic snapd restart...
kubectl 1.24.0 from Canonical‚úì installed

$ kubectl version --output=yaml
clientVersion:
  buildDate: "2022-07-14T02:31:37Z"
  compiler: gc
  gitCommit: aef86a93758dc3cb2c658dd9657ab4ad4afc21cb
  gitTreeState: clean
  gitVersion: v1.24.3
  goVersion: go1.18.3
  major: "1"
  minor: "24"
  platform: linux/amd64
kustomizeVersion: v4.5.4
serverVersion:
  buildDate: "2022-05-24T12:18:48Z"
  compiler: gc
  gitCommit: 3ddd0f45aa91e2f30c70734b175631bec5b5825a
  gitTreeState: clean
  gitVersion: v1.24.1
  goVersion: go1.18.2
  major: "1"
  minor: "24"
  platform: linux/amd64
----

=== Servicios de Kubernetes expuestos por minikube

Al instalar el cluster de minikube sobre Docker, se lanza un contenedor por
cada nodo:

[source,console]
----
$ docker ps
CONTAINER ID   IMAGE                                 COMMAND                  CREATED      STATUS      PORTS                                                                                                                                  NAMES
365c9ccc54af   gcr.io/k8s-minikube/kicbase:v0.0.32   "/usr/local/bin/entr‚Ä¶"   2 days ago   Up 2 days   127.0.0.1:49177->22/tcp, 127.0.0.1:49176->2376/tcp, 127.0.0.1:49175->5000/tcp, 127.0.0.1:49174->8443/tcp, 127.0.0.1:49173->32443/tcp   minikube-m03
bf74d36b2b9f   gcr.io/k8s-minikube/kicbase:v0.0.32   "/usr/local/bin/entr‚Ä¶"   2 days ago   Up 2 days   127.0.0.1:49172->22/tcp, 127.0.0.1:49171->2376/tcp, 127.0.0.1:49170->5000/tcp, 127.0.0.1:49169->8443/tcp, 127.0.0.1:49168->32443/tcp   minikube-m02
0b6f58cb11c3   gcr.io/k8s-minikube/kicbase:v0.0.32   "/usr/local/bin/entr‚Ä¶"   2 days ago   Up 2 days   127.0.0.1:49167->22/tcp, 127.0.0.1:49166->2376/tcp, 127.0.0.1:49165->5000/tcp, 127.0.0.1:49164->8443/tcp, 127.0.0.1:49163->32443/tcp   minikube
----

Cada uno de los nodos tiene expuestos varios puertos mediante reglas de
`iptables`, tanto de filtrado como de NAT:

[source,console]
----
$ sudo iptables -nvL
Chain INPUT (policy ACCEPT 2267K packets, 15G bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain FORWARD (policy DROP 41 packets, 3444 bytes)
 pkts bytes target     prot opt in     out     source               destination
 352K  369M DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0
 352K  369M DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
    0     0 DOCKER     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  docker0 docker0  0.0.0.0/0            0.0.0.0/0
 341K  368M ACCEPT     all  --  *      br-c37e90db80de  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
   98  6864 DOCKER     all  --  *      br-c37e90db80de  0.0.0.0/0            0.0.0.0/0
11692  700K ACCEPT     all  --  br-c37e90db80de !br-c37e90db80de  0.0.0.0/0            0.0.0.0/0
   52  3120 ACCEPT     all  --  br-c37e90db80de br-c37e90db80de  0.0.0.0/0            0.0.0.0/0

Chain OUTPUT (policy ACCEPT 849K packets, 113M bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain DOCKER (2 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.2         tcp dpt:32443
    1    60 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.2         tcp dpt:8443
    1    60 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.2         tcp dpt:5000
    0     0 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.2         tcp dpt:2376
    1    60 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.2         tcp dpt:22
    0     0 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.3         tcp dpt:32443
    0     0 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.3         tcp dpt:8443
    0     0 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.3         tcp dpt:5000
    0     0 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.3         tcp dpt:2376
    1    60 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.3         tcp dpt:22
    0     0 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.4         tcp dpt:32443
    0     0 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.4         tcp dpt:8443
    0     0 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.4         tcp dpt:5000
    0     0 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.4         tcp dpt:2376
    1    60 ACCEPT     tcp  --  !br-c37e90db80de br-c37e90db80de  0.0.0.0/0            192.168.49.4         tcp dpt:22

Chain DOCKER-ISOLATION-STAGE-1 (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DOCKER-ISOLATION-STAGE-2  all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0
11692  700K DOCKER-ISOLATION-STAGE-2  all  --  br-c37e90db80de !br-c37e90db80de  0.0.0.0/0            0.0.0.0/0
 352K  369M RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0

Chain DOCKER-ISOLATION-STAGE-2 (2 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DROP       all  --  *      docker0  0.0.0.0/0            0.0.0.0/0
    0     0 DROP       all  --  *      br-c37e90db80de  0.0.0.0/0            0.0.0.0/0
11692  700K RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0

Chain DOCKER-USER (1 references)
 pkts bytes target     prot opt in     out     source               destination
 352K  369M RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0
----

[source,console]
----
$ sudo iptables -L -nv -t nat
Chain PREROUTING (policy ACCEPT 20097 packets, 6260K bytes)
 pkts bytes target     prot opt in     out     source               destination
 1364  118K DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT 2926 packets, 519K bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain OUTPUT (policy ACCEPT 17981 packets, 2940K bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DOCKER     all  --  *      *       0.0.0.0/0           !127.0.0.0/8          ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT 18042 packets, 2943K bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 MASQUERADE  all  --  *      !docker0  172.17.0.0/16        0.0.0.0/0
  395 24397 MASQUERADE  all  --  *      !br-c37e90db80de  192.168.49.0/24      0.0.0.0/0
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.2         192.168.49.2         tcp dpt:32443
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.2         192.168.49.2         tcp dpt:8443
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.2         192.168.49.2         tcp dpt:5000
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.2         192.168.49.2         tcp dpt:2376
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.2         192.168.49.2         tcp dpt:22
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.3         192.168.49.3         tcp dpt:32443
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.3         192.168.49.3         tcp dpt:8443
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.3         192.168.49.3         tcp dpt:5000
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.3         192.168.49.3         tcp dpt:2376
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.3         192.168.49.3         tcp dpt:22
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.4         192.168.49.4         tcp dpt:32443
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.4         192.168.49.4         tcp dpt:8443
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.4         192.168.49.4         tcp dpt:5000
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.4         192.168.49.4         tcp dpt:2376
    0     0 MASQUERADE  tcp  --  *      *       192.168.49.4         192.168.49.4         tcp dpt:22

Chain DOCKER (2 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 RETURN     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0
    0     0 RETURN     all  --  br-c37e90db80de *       0.0.0.0/0            0.0.0.0/0
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49163 to:192.168.49.2:32443
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49164 to:192.168.49.2:8443
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49165 to:192.168.49.2:5000
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49166 to:192.168.49.2:2376
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49167 to:192.168.49.2:22
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49168 to:192.168.49.3:32443
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49169 to:192.168.49.3:8443
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49170 to:192.168.49.3:5000
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49171 to:192.168.49.3:2376
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49172 to:192.168.49.3:22
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49173 to:192.168.49.4:32443
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49174 to:192.168.49.4:8443
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49175 to:192.168.49.4:5000
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49176 to:192.168.49.4:2376
    0     0 DNAT       tcp  --  !br-c37e90db80de *       0.0.0.0/0            127.0.0.1            tcp dpt:49177 to:192.168.49.4:22
----

Los puertos que aparecen en las salidas anteriores corresponden a los distintos
servicios de Kubernetes que est√°n aceptando peticiones por red, como `kubelet`,
en el 10250, o el API server, en el 8433.  La siguiente salida muestra los
servicios que est√°n escuchando en los puertos TCP del master del cluster de
minikube:

[source,console]
----
$ docker exec -it minikube ss -utanp | grep LIST
tcp   LISTEN    0      4096               127.0.0.1:10248               0.0.0.0:*                     users:(("kubelet",pid=1843,fd=20))
tcp   LISTEN    0      4096            192.168.49.2:2379                0.0.0.0:*                     users:(("etcd",pid=1607,fd=9))
tcp   LISTEN    0      4096               127.0.0.1:2379                0.0.0.0:*                     users:(("etcd",pid=1607,fd=8))
tcp   LISTEN    0      4096            192.168.49.2:2380                0.0.0.0:*                     users:(("etcd",pid=1607,fd=7))
tcp   LISTEN    0      4096               127.0.0.1:2381                0.0.0.0:*                     users:(("etcd",pid=1607,fd=13))
tcp   LISTEN    0      4096               127.0.0.1:10257               0.0.0.0:*                     users:(("kube-controller",pid=1675,fd=7))
tcp   LISTEN    0      4096               127.0.0.1:10259               0.0.0.0:*                     users:(("kube-scheduler",pid=1606,fd=7))
tcp   LISTEN    0      4096              127.0.0.11:35443               0.0.0.0:*
tcp   LISTEN    0      128                  0.0.0.0:22                  0.0.0.0:*                     users:(("sshd",pid=120,fd=3))
tcp   LISTEN    0      4096            192.168.49.2:10010               0.0.0.0:*                     users:(("containerd",pid=114,fd=11))
tcp   LISTEN    0      4096                       *:2376                      *:*                     users:(("dockerd",pid=635,fd=9))
tcp   LISTEN    0      4096                       *:10249                     *:*                     users:(("kube-proxy",pid=2229,fd=21))
tcp   LISTEN    0      4096                       *:10250                     *:*                     users:(("kubelet",pid=1843,fd=25))
tcp   LISTEN    0      4096                       *:10256                     *:*                     users:(("kube-proxy",pid=2229,fd=20))
tcp   LISTEN    0      4096                       *:40885                     *:*                     users:(("cri-dockerd",pid=546,fd=10))
tcp   LISTEN    0      128                     [::]:22                     [::]:*                     users:(("sshd",pid=120,fd=4))
tcp   LISTEN    0      4096                       *:8443                      *:*                     users:(("kube-apiserver",pid=1603,fd=7))
----

El API del cluster de minikube se puede alcanzar desde otra m√°quina que est√© en
la misma red que el host donde se despliegue con solo configurar una ruta hacia
la red de los nodos.  En el siguiente ejemplo, el cluster est√° instalado en la
direcci√≥n IP 192.168.1.55, y queremos alcanzar la API desde la 192.168.1.5 de
la misma red:

[source,console]
----
$ ip -c -br -4 a
lo               UNKNOWN        127.0.0.1/8
eth0             UP             192.168.1.5/24

$ ip route
default via 192.168.1.1 dev eth0 proto static metric 100
192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.5 metric 100

$ sudo ip route add 192.168.49.0/24 via 192.168.1.55

$ ip route
default via 192.168.1.1 dev eth0 proto static metric 100
192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.5 metric 100
192.168.49.0/24 via 192.168.1.55 dev eth0

$ nc -v 192.168.49.2 8443
Ncat: Version 7.80 ( https://nmap.org/ncat )
Ncat: Connected to 192.168.49.2:8443.
^C
----

== API de Kubernetes

`kube-apiserver` implementa un servicio API REST que utilizan los usuarios,
partes del cluster y los componentes externos para interactuar con K8s.  La API
permite consultar y manipular el estado de los _API objects_ de K8s, como
<<pod,pods>>, namespaces, <<configmaps>>, eventos...  Todas las entradas tienen
el formato `<punto_de_entrada_a_API>/<group>/<version>/<resource>`

Se puede ver qu√© APIs soporta un cluster con <<kubectl_api_versions>>, y qu√©
recursos podemos manipular con <<kubectl_api_resources>>.

La API de K8s requiere que los objetos se pasen en formato JSON. `kubectl` se
encarga de convertir los objetos especificados como YAML a JSON.

Para poder manipular un objeto en K8s, necesitamos:

* *apiVersion*, la versi√≥n de la API que utiliza el objeto.

* *kind*, la clase del objeto.

* *metadata.name*, el nombre √∫nico del objeto en su namespace.

* *metadata.namespace*, el namespace donde est√° definido el objeto (por
   defecto, el actual o _current_).

* *metadata.uid*, el identificador √∫nico generado para el objeto.

En YAML, esto tendr√≠a el siguiente aspecto:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
    name: mypod
    namespace: default
    uid: '145c2436-e0bb-11ec-b44c-e7f1d45f0a43'
----

Los objetos de K8s pueden examinarse con <<kubectl_get>>.

Las versiones de API `apiVersion` tienen tres niveles de soporte:

* Alpha, para todos los nombres que contienen `alpha`, como `v1alpha2`.  No hay
  ning√∫n tipo de garant√≠a sobre estas API: pueden cambiar o desaparecer en
  cualquier momento.

* Beta, para todos los nombres que contienen `beta`, como `v2beta1`.  Son API
  probadas, aunque puede que se introduzcan peque√±os cambios en versiones
  posteriores beta o estables, que obliguen a recrear los objetos afectados.
  Hay garant√≠as de que no desaparecer√°n.  No se recomienda que se usen estas
  API en producci√≥n, salvo que tengamos varios clusters que se puedan
  actualizar de forma independiente.

* Estable, que se refieren a todos los nombres que no contienen `alpha` ni
  `beta`.

=== Control de acceso a la API

WARNING: https://kubernetes.io/docs/concepts/security/controlling-access/[TODO].

Por defecto, la API de K8s est√° accesible en dos direcciones, una insegura y
otra segura.  La direcci√≥n insegura est√° pensada para hacer diagn√≥stico, y se
encuentra en la direcci√≥n `localhost:8080` de los nodos que tienen el control
plane. Utiliza HTTP en claro y no requiere autenticaci√≥n ni autorizaci√≥n,
aunque s√≠ que aplican los m√≥dulos de control de entrada (_admission control_).
La direcci√≥n segura es la que usamos habitualmente con `kubectl`.

=== Creaci√≥n de plantillas YAML

Cada recurso de K8s se puede definir en YAML o en JSON.  Aunque `kubectl` no
tiene forma directa de crear las plantillas con todas las opciones de un
recurso, se puede sacar suficiente informaci√≥n con `kubectl explain <recurso>`,
y generar una base bastante parecida a YAML, y demasiado extensa, con la opci√≥n
`--recursive`:

[source,console]
----
$ kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status
...

$ kubectl explain pod.spec
KIND:     Pod
VERSION:  v1

RESOURCE: spec <Object>

DESCRIPTION:
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

     PodSpec is a description of a pod.

FIELDS:
   activeDeadlineSeconds        <integer>
     Optional duration in seconds the pod may be active on the node relative to
     StartTime before the system will actively try to mark it failed and kill
     associated containers. Value must be a positive integer.

   affinity     <Object>
     If specified, the pod's scheduling constraints

   automountServiceAccountToken <boolean>
     AutomountServiceAccountToken indicates whether a service account token
     should be automatically mounted.

   containers   <[]Object> -required-
     List of containers belonging to the pod. Containers cannot currently be
     added or removed. There must be at least one container in a Pod. Cannot be
     updated.
...

$ kubectl explain pod --recursive
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
   kind <string>
   metadata     <Object>
      annotations       <map[string]string>
      clusterName       <string>
      creationTimestamp <string>
      deletionGracePeriodSeconds        <integer>
      deletionTimestamp <string>
      finalizers        <[]string>
      generateName      <string>
      generation        <integer>
      labels    <map[string]string>
      managedFields     <[]Object>
         apiVersion     <string>
         fieldsType     <string>
         fieldsV1       <map[string]>
         manager        <string>
         operation      <string>
         subresource    <string>
         time   <string>
      name      <string>
      namespace <string>
      ownerReferences   <[]Object>
         apiVersion     <string>
         blockOwnerDeletion     <boolean>
         controller     <boolean>
         kind   <string>
         name   <string>
         uid    <string>
      resourceVersion   <string>
      selfLink  <string>
      uid       <string>
   spec <Object>
      activeDeadlineSeconds     <integer>
      affinity  <Object>
         nodeAffinity   <Object>
            preferredDuringSchedulingIgnoredDuringExecution     <[]Object>
               preference       <Object>
                  matchExpressions      <[]Object>
                     key        <string>
                     operator   <string>
                     values     <[]string>
                  matchFields   <[]Object>
                     key        <string>
                     operator   <string>
                     values     <[]string>
               weight   <integer>
...
----

Para los recursos que se pueden crear con `kubectl create`, tambi√©n se puede
hacer una prueba de la creaci√≥n de un objeto con la opci√≥n `--dry-run=client`:

[source,console]
----
$ kubectl create deployment trospido --image=nginx --dry-run=client -o=yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: trospido
  name: trospido
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trospido
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: trospido
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
----

== Etiquetas

Todos los objetos de K8s pueden tener etiquetas asociadas (<<label,_labels_>>),
que se utilizan para agruparlos de forma l√≥gica, pudi√©ndose utilizar en los
seleccionadores (<<selector,_selectors_>>).  Podemos crear o modificar Las
etiquetas de los objetos en cualquier momento.

Las etiquetas y los seleccionadores pueden usarse para cosas como decidir en
qu√© nodos del cluster deben ejecutarse determinados servicios o el tipo de
almacenamiento a utilizar.

Las etiquetas se asignan como parte de los metadatos de un objeto:

[source,yaml]
----
metadata:
  labels:
    key1: value1
    key2: value2
----

Las claves tienen la forma `[prefijo/]nombre`, con un prefijo opcional que
tiene la forma de un dominio DNS, y un nombre obligatorio que empieza y termina
por un car√°cter alfanum√©rico y que puede incluir entre medias eso mismo m√°s
`-`, `_` y `.`.  Se entiende que las claves sin prefijo son privadas para los
usuarios.  Todas las etiquetas que utilizan los componentes propios de K8s
tienen prefijo.  Los prefijos `kubernetes.io` y `k8s.io` est√°n reservados para
ellos.

K8s
https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/[recomienda]
utilizar algunas etiquetas para agrupar objetos, todas con el prefijo
`app.kubernetes.io`.

NOTE: Es importante que las organizaciones definan un conjunto est√°ndar de
etiquetas para facilitar la gesti√≥n de los objetos de sus clusters, y que se
utilicen en las plantillas de los distintos objetos.

[[seleccionadores,_seleccionadores_]]
=== Seleccionadores

Son filtros que permiten elegir objetos de K8s bas√°ndose en valores de sus
etiquetas.  Los hay de dos tipos, los basados en la igualdad y los que permiten
buscar en conjuntos de valores.

.Seleccionador basado en la igualdad
[source,yaml]
----
selector:
  matchLabels:
    key1: value
----

Los seleccionadores basados en la igualdad admiten tres operadores, `=` e `==`,
que son equivalentes y requieren que las etiquetas sean iguales a un valor, y
`!=`, para requerir que sean distintas a un valor *o que el objeto no tenga esa
etiqueta*.  Pueden tener uno o varios requisitos separados por comas, que
act√∫an como un AND l√≥gico (deben cumplirse todos los requisitos):

[source,console]
----
$ get pods --selector environment=pro,tier!=frontend
----

WARNING: Parece que no hay forma de conseguir el efecto de `!=` en YAML con los
seleccionadores basados en igualdad.  Se puede conseguir algo similar con los
seleccionadores basados en conjuntos y el operador `NotIn`, pero no todos los
objetos de K8s soportan este tipo de seleccionadores.

WARNING: No hay operador OR para ninguno de los dos tipos de seleccionadores.

.Seleccionador basado en conjuntos [source,yaml]
----
selector:
  matchExpressions:
  - key: key1
    operator: In
    values:
    - value1
    - value2
----

Este tipo de seleccionadores admite los operadores `In`, `NotIn`, `Exists`,
`DoesNotExist`, `Gt` y `Lt`.

== Pods

Un _pod_ (en el sentido de "manada"), es la unidad m√≠nima de proceso de
Kubernetes.  Consiste en un grupo de contenedores que comparten ciertos
recursos, como los vol√∫menes (aunque cada uno tenga su propio _mount
namespace_), el _namespace_ de red y el de IPC (comunicaci√≥n entre procesos
Posix y System V).  El contenido de un pod se lanza en un √∫nico nodo, y se
gestiona como un todo.  Se puede pensar en ellos como en hosts virtuales para
ejecutar procesos fuertemente acoplados.

Al compartir el _namespace_ de red, todos los procesos de un pod pueden
comunicarse mediante la direcci√≥n IP del localhost (127.0.0.1).  Como comparten
los n√∫meros de puertos, es necesario que los contenedores de un pod utilicen
puertos distintos para prestar sus servicios.

Para comprobar qu√© _namespaces_ comparten dos procesos que forman parte del
mismo pod en el cluster de minikube creado antes, lanzamos el siguiente pod:

.pod-2containers.yaml
[source,yaml]
----
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-2containers
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  - name: loop
    image: nginx
    command:  ['sh', '-c', 'while true; do date; sleep 10s; done']
----

[source,console]
----
$ kubectl apply -f pod-2containers.yaml
pod/pod-2containers created

$ kubectl get pods
NAME               READY   STATUS    RESTARTS   AGE
pod-2containers   2/2     Running   0          5s

$  ps -ef | grep -iE 'nginx|sleep'
root     1014754 1014734  0 18:15 ?        00:00:00 nginx: master process nginx -g daemon off;
systemd+ 1014792 1014754  0 18:15 ?        00:00:00 nginx: worker process
systemd+ 1014793 1014754  0 18:15 ?        00:00:00 nginx: worker process
systemd+ 1014794 1014754  0 18:15 ?        00:00:00 nginx: worker process
systemd+ 1014795 1014754  0 18:15 ?        00:00:00 nginx: worker process
root     1014841 1014820  0 18:15 ?        00:00:00 sh -c while true; do date; sleep 10s; done
root     1049523 1014841  0 18:52 ?        00:00:00 sleep 10s

$ pstree -pslT 1014754
systemd(1)‚îÄ‚îÄ‚îÄcontainerd-shim(12954)‚îÄ‚îÄ‚îÄsystemd(12978)‚îÄ‚îÄ‚îÄcontainerd-shim(1014734)‚îÄ‚îÄ‚îÄnginx(1014754)‚îÄ‚î¨‚îÄnginx(1014792)
                                                                                                 ‚îú‚îÄnginx(1014793)
                                                                                                 ‚îú‚îÄnginx(1014794)
                                                                                                 ‚îî‚îÄnginx(1014795)

$ pstree -plT 12978
systemd(12978)‚îÄ‚î¨‚îÄcontainerd(13170)
               ‚îú‚îÄcontainerd-shim(15374)‚îÄ‚îÄ‚îÄpause(15394)
               ‚îú‚îÄcontainerd-shim(15416)‚îÄ‚îÄ‚îÄpause(15437)
               ‚îú‚îÄcontainerd-shim(15465)‚îÄ‚îÄ‚îÄkube-proxy(15514)
               ‚îú‚îÄcontainerd-shim(15494)‚îÄ‚îÄ‚îÄkindnetd(15522)
               ‚îú‚îÄcontainerd-shim(1014591)‚îÄ‚îÄ‚îÄpause(1014611)
               ‚îú‚îÄcontainerd-shim(1014734)‚îÄ‚îÄ‚îÄnginx(1014754)‚îÄ‚î¨‚îÄnginx(1014792)
               ‚îÇ                                           ‚îú‚îÄnginx(1014793)
               ‚îÇ                                           ‚îú‚îÄnginx(1014794)
               ‚îÇ                                           ‚îî‚îÄnginx(1014795)
               ‚îú‚îÄcontainerd-shim(1014820)‚îÄ‚îÄ‚îÄsh(1014841)‚îÄ‚îÄ‚îÄsleep(1088462)
               ‚îú‚îÄdbus-daemon(13166)
               ‚îú‚îÄdockerd(13196)
               ‚îú‚îÄkubelet(14943)
               ‚îú‚îÄsshd(13183)
               ‚îî‚îÄsystemd-journal(13145)

# diff -y (readlink /proc/1014754/ns/* | psub) (readlink /proc/1014841/ns/* | psub)
cgroup:[4026534600]                                           | cgroup:[4026534604]
ipc:[4026534462]                                                ipc:[4026534462]
mnt:[4026534597]                                              | mnt:[4026534601]
net:[4026534464]                                                net:[4026534464]
pid:[4026534599]                                              | pid:[4026534603]
pid:[4026534599]                                              | pid:[4026534603]
time:[4026531834]                                               time:[4026531834]
time:[4026531834]                                               time:[4026531834]
user:[4026531837]                                               user:[4026531837]
uts:[4026534598]                                              | uts:[4026534602]

# systemd-cgls -l
...
  ‚îÇ     ‚îî‚îÄkubepods-besteffort-podd9d2bfea_9b77_43db_9741_e5f9ad6a70ec.slice (#99978)
  ‚îÇ       ‚Üí trusted.invocation_id: 26cce36417ae4549bf775fb45a9c2bf8
  ‚îÇ       ‚îú‚îÄdocker-6b5120debb47b88bef33a471edf1ce451f679587033b44f4fe83ac4e2be5e190.scope ‚Ä¶ (#100173)
  ‚îÇ       ‚îÇ ‚Üí trusted.delegate: 1
  ‚îÇ       ‚îÇ ‚Üí trusted.invocation_id: 76b501c6ec94475f81fa407e21cfe218
  ‚îÇ       ‚îÇ ‚îú‚îÄ1014841 sh -c while true; do date; sleep 10s; done
  ‚îÇ       ‚îÇ ‚îî‚îÄ1068429 sleep 10s
  ‚îÇ       ‚îú‚îÄdocker-f34da3d995e1fc06f1d71e22b960e7b4d16fb1cefe71c40c1184229c1f62b0b2.scope ‚Ä¶ (#100043)
  ‚îÇ       ‚îÇ ‚Üí trusted.delegate: 1
  ‚îÇ       ‚îÇ ‚Üí trusted.invocation_id: 0bfd50d366084aa4828f6ac260afc6aa
  ‚îÇ       ‚îÇ ‚îî‚îÄ1014611 /pause
  ‚îÇ       ‚îî‚îÄdocker-e4d2f81c40fc09a437ba15a3fd4f3da859744d91dd4819283c04e3a8ded0843e.scope ‚Ä¶ (#100108)
  ‚îÇ         ‚Üí trusted.delegate: 1
  ‚îÇ         ‚Üí trusted.invocation_id: b1a2b05b1a19412298ae8aa02d06919a
  ‚îÇ         ‚îú‚îÄ1014754 nginx: master process nginx -g daemon off;
  ‚îÇ         ‚îú‚îÄ1014792 nginx: worker process
  ‚îÇ         ‚îú‚îÄ1014793 nginx: worker process
  ‚îÇ         ‚îú‚îÄ1014794 nginx: worker process
  ‚îÇ         ‚îî‚îÄ1014795 nginx: worker process
...
----

Como puede verse en las salidas anteriores, y al menos en el caso de un cluster
de minikube sobre Docker, los contenedores de un mismo pod comparten los
namespaces de red, IPC, _time_ y _user_ (el que a√≠sla los UID, GID y las
capacidades de los procesos).  Dentro de la jerarqu√≠a de _cgroups_, comparten
un ancestro com√∫n (el `kubepods-besteffort-pod...`), lo que permite gestionar
los recursos globales asignados a ellos.

Los contenedores de un pod ven como _hostname_ el campo `name` configurado en
el pod.

[source,console]
----
$ kubectl exec pod/pod-2containers -- hostname
Defaulted container "nginx" out of: nginx, loop
pod-2containers
----

Se puede hacer que los contenedores de un pod compartan el namespace de
procesos incluyendo en su definici√≥n la propiedad `shareProcessNaespace:
true``.  Esto permite ver los procesos desde otros contenedores del pod, lo que
es √∫til para usar contenedores ef√≠meros (_ephemeral containers_) para
diagnosticar problemas en contenedores _distroless_, por ejemplo.

[[app_containers,contenedores de aplicaci√≥n]]
=== Contenedores de aplicaci√≥n

La carga de trabajo de los pods se hace dentro de los contenedores de
aplicaci√≥n (_app containers_), que se definen dentro del array `containers`.
Con la orden `kubectl explain pod`, se puede ver todos los campos que admiten,
pero los m√°s importantes son la imagen del contenedor (`image`), la orden que
debe ejecutarse y sus argumentos (`command` y `args`), las variables de entorno
(`env` o `envFrom`), los scripts a ejecutar en ciertos momentos del
<<ciclo_vida_pods>>, las <<pruebas_estado>>, y los puertos de servicio
(`ports`).

Tambi√©n es importante especificar los recursos que necesitan para funcionar,
(memoria y CPU).  Podemos indicar los recursos m√≠nimos (`resources.request`),
que K8s utilizar√° para asignar el nodo que ejecutar√° el pod, y tambi√©n los
m√°ximos que el contenedor no podr√° sobrepasar (`resources.limits`).  Si se
establecen l√≠mites pero no se indican m√≠nimos, K8s usa los l√≠mites como
m√≠nimos.  Si no se especifican l√≠mites, K8s permite usar la capacidad completa
del nodo.

La CPU se especifica en _unidades de cpu_.  Un 1 es una CPU completa, y se
puede usar el sufijo `m` para indicar mil√©simas (por ejemplo, `cpu: 250m` es
equivalente a `0.250`, un cuarto de CPU).  La memoria se especifica en bytes,
pero se pueden usar los sufijos habituales (T, Ti, G, Gi, M, Mi, K, Ki...).

=== Contenedores de inicializaci√≥n

Un pod puede tener uno o m√°s contenedores de inicializaci√≥n (_init
containers_), que se ejecutan antes de lanzar los contenedores de aplicaci√≥n
(_app containers_).  Se define dentro del array `initContainers`, que es
similar al `containers` de los <<app_containers>>.  Se lanzan de forma
secuencial y se espera a que terminen antes de pasar al siguiente.  Cuando el
√∫ltimo termina, se lanzan los contenedores de aplicaci√≥n del pod.  Pueden
utilizarse para comprobar que el entorno es el adecuado antes de lanzar los
trabajos principales del pod o para prepararlo (p. ej, creando bases de datos u
otros servicios).

Los contenedores de inicializaci√≥n deben devolver un c√≥digo de resultado.  En
caso de que devuelvan un error, se considerar√° que el pod ha fallado y se
aplicar√° su <<politica_reinicio>>, aunque si est√° fijada a `Always` se tratar√°
como si fuera `OnFailure`.

Los contenedores de inicializaci√≥n se vuelven a ejecutar si hay que reiniciar
el pod, por lo que deben escribirse de forma que no intenten volver a hacer un
trabajo que ya est√© hecho en ejecuciones previas.

[[ciclo_vida_pods,ciclo de vida de los pods]]
=== Ciclo de vida de los pods

Los pods siguen un ciclo de vida bien definido, representado por el campo
`phase` de su objeto `PodStatus`, que aparece en el apartado `Status:` de la
salida de `kubectl describe pod`.

Los pods empiezan en estado `Pending` cuando son aceptados por el cluster de
K8s, y pasan a estado `Running` cuando todos sus contenedores se han creado y
al menos uno de ellos est√° corriendo.  Los pod pueden terminar en los estados
`Succeeded` (todos los contenedores han terminado bien y no deben ser
reiniciados) o `Failed` (todos los contenedores han terminado, pero al menos
uno fallando, con un estado distinto de 0 o terminado por el sistema).  Tambi√©n
pueden estar en estado `Unknown`, si por cualquier raz√≥n no se puede obtener su
estado (por ejemplo, por no poder comunicarse con su nodo).

Los pods son ef√≠meros.  La ejecuci√≥n de un pod se programa una sola vez en toda
su vida, asign√°ndole un nodo.  Una vez que se asigna un nodo a un pod, se
ejecuta en √©l hasta que termina o se elimina.  Si un nodo falla, se programa la
finalizaci√≥n de sus pods pasado un tiempo de espera.

Cada pod tiene su propio UID.  Los pods no pueden reasignarse a otros nodos,
pero pueden sustituirse por otro pod casi id√©ntico en otro nodo, con su
propio UID.

==== Estado de los contenedores

Dentro de un pod, los contenedores pasan por los siguientes estados, que pueden
verse con `kubectl describe pod`:

* `Waiting`, cuando se est√° preparando el contenedor para que pase a alguno de
  los otros estados.

* `Running`, cuando el contenedor est√° funcionando sin problemas.  Si el
  contenedor tuviera un _hook_ `postStart`, se habr√° ejecutado antes de pasar a
  este estado, aunque _no hay garant√≠as de que se ejecute antes que el punto de
  entrada del contenedor_ (se ejecutan de forma as√≠ncrona).

* `Terminated`, cuando un contenedor que ha pasado a estado `Running` termina
  por cualquier motivo.  Antes de pasar a este estado, se ejecuta cualquier
  _hook_ `preStop` que tuviera configurado.

[[politica_reinicio,pol√≠tica de reinicio]]
==== Pol√≠tica de reinicio de los contenedores

`kubelet` es capaz de reiniciar los contenedores de un pod ante cierto tipo de
fallos y hacer que el pod vuelva a estar saludable (_healthy_).  Esto depende
de la pol√≠tica `restartPolicy` que tenga configurada el pod, que puede tener
los valores `Always` (por defecto), `OnFailure` o `Never`.  `kubelet` reinicia
los contenedores incrementando el tiempo de espera de forma exponencial (10,
20, 40 segundos...), hasta 5 minutos m√°ximo.  Si un contenedor lleva 10 minutos
corriendo sin problemas, se reinicia el tiempo de espera a su valor inicial.

==== Condiciones de los pods

El `PodStatus` de los pods tiene un array de condiciones por las que el pod ha
podido pasar:

* `PodScheduled`, si se le ha asignado un nodo.

* `ContainersReady`, si todos los contenedores del pod est√°n en estado `Ready`.

* `Initialized`, si todos los contenedores de inicializaci√≥n han terminado
  correctamente.

* `Ready`, si el pod puede atender peticiones y puede ser a√±adido a la pila de
  balanceadores de los `Services` pertinentes.

[source,console]
----
$ kubectl describe pod pod-2containers
Name:         pod-2containers
Namespace:    blas
Priority:     0
Node:         minikube-m03/192.168.49.4
Start Time:   Mon, 27 Jun 2022 18:18:56 +0200
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           10.244.4.2
IPs:
  IP:  10.244.4.2
Containers:
  nginx:
    Container ID:   docker://aaac06fcf79aa3f03f077c5043cda90caac73b4781db968593c8ee91fbcd894b
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:10f14ffa93f8dedf1057897b745e5ac72ac5655c299dade0aa434c71557697ea
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 27 Jun 2022 18:18:59 +0200
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-47sbt (ro)
  loop:
    Container ID:  docker://c4bf74188ceeaa6ae14cab8ecf0c4ad7356ed744870e68fb35894dee3e88aaf8
    Image:         nginx
    Image ID:      docker-pullable://nginx@sha256:10f14ffa93f8dedf1057897b745e5ac72ac5655c299dade0aa434c71557697ea
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      while true; do date; sleep 10s; done
    State:          Running
      Started:      Mon, 27 Jun 2022 18:19:00 +0200
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-47sbt (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-47sbt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
----

==== Condiciones personalizadas (readinessGates)

Podemos a√±adir a los pods condiciones adicionales que `kubelet` puede utilizar
para determinar si est√°n listos para recibir peticiones o no, usando
`readinessGates`:

[source,yaml]
----
kind: Pod
...
spec:
  readinessGates:
    - conditionType: "BalancerReady"
status:
  conditions:
    - type: "BalancerReady"
      status: "False"
      lastProbeTime: null
      lastTransitionTime: 2022-01-01T00:00:00Z
...
----

Se trata de condiciones cuyo valor se actualiza mediante la API e K8s, no desde
dentro del pod.  Por ejemplo, podemos utilizar un programa externo que
compruebe si un balanceador externo est√° listo para enviar tr√°fico a un pod y
utilice la API para actualizar el estado de esa condici√≥n.  Se puede ver un
ejemplo de c√≥mo hacer esto
https://towardsdatascience.com/improving-application-availability-with-pod-readiness-gates-4ebebc3fb28a[aqu√≠].

Los pods que tengan condiciones personalizadas solo estar√°n en estado `Ready`
cuando todos sus contenedores est√©n `Ready` y el `status` de todas sus
`readinessGates` sea `True`.  Si lo primero fuera cierto pero lo segundo no, el
estado del pod ser√≠a `ContainersReady`.

[[pruebas_estado,pruebas de estado]]
==== Pruebas de estado

Podemos configurar hasta tres pruebas distintas que `kubelet` puede hacer sobre
los contenedores de un pod:

* `livenessProbe`, que indica si el contenedor est√° funcionando.  Si esta
  prueba falla, `kubelet` elimina el contenedor y se aplica su
  <<politica_reinicio>>.  Si no se personaliza esta prueba, por defecto se
  considera que est√° en estado `Success`.  Esta prueba no es necesaria en
  contenedores que terminan autom√°ticamente cuando dejan de funcionar, porque
  se les aplicar√° directamente la <<politica_reinicio>> que tengan configurada.

* `readinessProbe`, que determina si el contenedor est√° listo para atender
  peticiones.  Si la prueba falla, el controlador de _endpoints_ quita la
  direcci√≥n IP del pod de los _endpoints_ de todos los servicios que coincidan
  con el pod.  El estado de esta prueba es `Failure` durante la pausa inicial
  que haya configurada, pasando despu√©s a `Success` si no se personaliza la
  prueba.  Se considera que un pod est√° listo para atender peticiones cuando
  todos sus contenedores est√°n listos.

* `startupProbe`, que determina si la aplicaci√≥n del contenedor ha arrancado.
  Si se personaliza esta prueba, las otras dos pruebas se mantienen
  deshabilitadas hasta que esta se pasa.  Si la prueba falla, `kubelet` mata el
  contenedor y se le aplica la <<politica_reinicio>> que tenga configurada.  Si
  no se personaliza esta prueba, por defecto se considera que est√° en estado
  `Success`.  Este tipo de pruebas son √∫tiles para contenedores que tardan un
  tiempo considerable en arrancar, mayor que
  `initialDelaySeconds`+`failureThreshold`*`periodSeconds`.

`kubelet` es el encargado de lanzar las pruebas, que pueden ser de los
siguientes tipos:

* `exec`, que ejecuta una orden dentro del contenedor, y se supera si
  devuelve 0.

* `httpGet`, que lanza un HTTP GET contra la URL especificada, y se pasa si se
  devuelve un c√≥digo HTTP mayor o igual que 200 y menor que 400.  En versiones
  de K8s anteriores o iguales a la 1.13, `kubelet` utilizar√° el proxy
  configurado en las variables de entorno `http_proxy` o `HTTP_PROXY` del nodo
  para comunicarse con el contenedor, pero a partir de esa versi√≥n lo har√°
  directamente.

* `tcpSocket`, que abre una conexi√≥n TCP contra el puerto especificado.  La
  prueba se pasa si el puerto est√° abierto.

* `grpc`, que utiliza llamadas a procedimiento remoto https://grpc.io/[gRPC].
  Por el momento, esto est√° en estado alpha.  Este tipo de pruebas est√°
  disponible a partir de la versi√≥n 1.24 de K8s.

El resultado de cualquiera de las pruebas anteriores puede ser `Success`, si se
pasan, `Failure`, si no se pasan, o `Unknown` si ha habido problemas para
lanzar la prueba, en cuyo caso se seguir√° intentando.

Se puede modificar el comportamiento de las distintas pruebas de estado a
trav√©s de los siguientes par√°metros:

* `initialDelaySeconds`, que por defecto es 0.  Es el n√∫mero de segundos a
  esperar desde que el contenedor arranca para empezar a lanzar las pruebas.
  No aplica a `startupProbe`.

* `periodSeconds`, que por defecto es 10s.  Cada cu√°nto se ejecuta la prueba.

* `timeoutSeconds`, que por defecto es 1s.  Tiempo m√°ximo de espera para
  obtener un resultado de la prueba.

* `successThreshold`, que por defecto es 1.  N√∫mero de pruebas correctas
  consecutivas necesario para considerar que el contenedor pasa la prueba,
  despu√©s de haber fallado.

* `failureThreshold`, que *por defecto es 3*.  N√∫mero de reintentos que hace
  K8s cuando una prueba falla, antes de abandonar y actuar en consecuencia.

Adem√°s, las pruebas `httpGet` admiten los siguientes par√°metros:

* `host`, que por defecto es la IP del pod.  Es el nombre del host al que
  lanzar las pruebas.  Si se quiere cambiar el host de la cabecera HTTP, es
  mejor cambiar la cabecera `Host` con `httpHeaders`.

* `scheme`, que por defecto es HTTP, pero podemos cambiarlo a HTTPS.  *No se
  valida el certificado*.

* `path`, que por defecto es `/`.  Tiene la parte de ruta de la URL a usar.

* `httpHeaders`, con las cabeceras HTTP que queramos personalizar.  Por
  defecto, se env√≠an las cabeceras `User-Agent: kube-probe/1.24` y `Accept:
  \*/*`.

* `port`, con el n√∫mero de puerto del servidor.

En el siguiente ejemplo, lanzamos un pod con dos contenedores, y configuramos
en uno de ellos una prueba que falla aproximadamente el 10% de las veces,
adem√°s de configurar el valor `failureThreshold` a 1 para reiniciar el
contenedor en cuanto se detecte un fallo:

.pod-2containers-lp.yaml
[source,yaml]
----
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-2containers-lp
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  - name: loop
    image: nginx
    command:  ['sh', '-c', 'while true; do date; sleep 10s; done']
    livenessProbe:
     exec:
       command:
       - bash
       - -c
       - f() { return $(($RANDOM % 10 < 1)); }; f
     initialDelaySeconds: 5
     periodSeconds: 5
     failureThreshold: 1
----

Una vez aplicada esa configuraci√≥n, vemos c√≥mo el contenedor se reinicia cada
vez que falla:

[source,console]
----
$ kubectl get events --field-selector involvedObject.name=pod-2containers-lp -o custom-columns=LATSEEN:.lastTimestamp,COUNT:.count,TYPE:.type,REASON:.reason,OBJECT:.involvedObject.name,MESSAGE:.message --watch
LATSEEN                COUNT   TYPE      REASON      OBJECT               MESSAGE
2022-07-07T15:31:07Z   1       Normal    Scheduled   pod-2containers-lp   Successfully assigned blas/pod-2containers-lp to minikube-m02
2022-07-07T15:31:08Z   1       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:31:09Z   1       Normal    Pulled      pod-2containers-lp   Successfully pulled image "nginx" in 1.333973436s
2022-07-07T15:31:10Z   1       Normal    Created     pod-2containers-lp   Created container nginx
2022-07-07T15:31:10Z   1       Normal    Started     pod-2containers-lp   Started container nginx
2022-07-07T15:31:10Z   1       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:31:11Z   1       Normal    Pulled      pod-2containers-lp   Successfully pulled image "nginx" in 1.405459696s
2022-07-07T15:31:11Z   1       Normal    Created     pod-2containers-lp   Created container loop
2022-07-07T15:31:11Z   1       Normal    Started     pod-2containers-lp   Started container loop
2022-07-07T15:31:17Z   1       Warning   Unhealthy   pod-2containers-lp   Liveness probe failed:
2022-07-07T15:31:17Z   1       Normal    Killing     pod-2containers-lp   Container loop failed liveness probe, will be restarted
2022-07-07T15:31:48Z   2       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:31:49Z   1       Normal    Pulled      pod-2containers-lp   Successfully pulled image "nginx" in 1.371456844s
2022-07-07T15:31:49Z   2       Normal    Created     pod-2containers-lp   Created container loop
2022-07-07T15:31:49Z   2       Normal    Started     pod-2containers-lp   Started container loop
2022-07-07T15:31:57Z   2       Warning   Unhealthy   pod-2containers-lp   Liveness probe failed:
2022-07-07T15:31:57Z   2       Normal    Killing     pod-2containers-lp   Container loop failed liveness probe, will be restarted
2022-07-07T15:32:28Z   3       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:32:29Z   1       Normal    Pulled      pod-2containers-lp   Successfully pulled image "nginx" in 1.34310106s
2022-07-07T15:32:29Z   3       Normal    Created     pod-2containers-lp   Created container loop
2022-07-07T15:32:29Z   3       Normal    Started     pod-2containers-lp   Started container loop
2022-07-07T15:33:17Z   3       Warning   Unhealthy   pod-2containers-lp   Liveness probe failed:
2022-07-07T15:33:17Z   3       Normal    Killing     pod-2containers-lp   Container loop failed liveness probe, will be restarted
2022-07-07T15:33:48Z   4       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:33:49Z   1       Normal    Pulled      pod-2containers-lp   Successfully pulled image "nginx" in 1.327867822s
2022-07-07T15:33:49Z   4       Normal    Created     pod-2containers-lp   Created container loop
2022-07-07T14:34:20Z   8       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:36:13Z   6       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:41:19Z   8       Normal    Pulling     pod-2containers-lp   Pulling image "nginx"
2022-07-07T15:46:19Z   1       Warning   BackOff     pod-2containers-lp   Back-off restarting failed container
----

[source,console]
----
$ kubectl get pod pod-2containers-lp --watch
NAME                 READY   STATUS              RESTARTS   AGE
pod-2containers-lp   0/2     Pending             0             0s
pod-2containers-lp   0/2     Pending             0             0s
pod-2containers-lp   0/2     ContainerCreating   0             0s
pod-2containers-lp   2/2     Running             0             5s
pod-2containers-lp   2/2     Running             1 (2s ago)    43s
pod-2containers-lp   2/2     Running             2 (1s ago)    82s
pod-2containers-lp   2/2     Running             3 (1s ago)    2m42s
pod-2containers-lp   2/2     Running             4 (2s ago)    4m18s
pod-2containers-lp   2/2     Running             5 (1s ago)    5m7s
pod-2containers-lp   2/2     Running             6 (2s ago)    6m43s
pod-2containers-lp   1/2     CrashLoopBackOff    6 (1s ago)    7m32s
pod-2containers-lp   2/2     Running             7 (2m43s ago)   10m
pod-2containers-lp   2/2     Running             8 (2s ago)      12m
pod-2containers-lp   1/2     CrashLoopBackOff    8 (1s ago)      13m
----

El estado `CrashLoopBackOff` indica que uno de los pods est√° fallando
intermitentemente y debe investigarse la causa.

==== Parada de los pods

Para detener un pod, `kubelet` env√≠a la se√±al `TERM` a sus procesos principales
(con PID 1), o, en los _runtimes_ que lo soporten, la se√±al especificada en el
valor `STOPSIGNAL` de la imagen de los contenedores, y espera un tiempo de
gracia `terminationGracePeriodSeconds` (por defecto, de 30 segundos), tras el
que mata todos los procesos de los contenedores con una se√±al `KILL`.  Una vez
muertos, se elimina el pod del servidor de API.

Se puede forzar la terminaci√≥n inmediata de un pod poniendo el per√≠odo de
gracia a cero con `kubectl delete <pod> --force --grace-period=0`.  Esto
eliminar√° el pod directamente de la API, pero tendr√°n un peque√±o tiempo de
gracia en el nodo para terminar antes de ser eliminados con la se√±al `KILL`.

Los pods que fallan permanecen en la API hasta que se eliminan manualmente o a
trav√©s de su controlador, hasta un m√°ximo de `terminated-pod-gc-threshold`.

Para los contenedores que tengan definido el _hook_ `preStop`, `kubelet`
ejecuta el _hook_ dentro de ellos antes de enviar la se√±al anterior.  El
objetivo de ese c√≥digo es preparar el contenedor para ser eliminado, no tiene
por qu√© terminar el proceso.  Por ejemplo, puede usarse para pasar los datos
que se mantengan en una cach√© a almacenamiento permanente.

El _hook_ `preStop` tiene `terminationGracePeriodSeconds` m√°s dos segundos para
hacer su trabajo.  Si no termina en ese tiempo, se mata y aparece un error en
los eventos:

.pod-prestop.yaml
[source,yaml]
----
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-prestop
spec:
  containers:
  - name: loop
    image: nginx
    command:  ['sh', '-c', 'while [ ! -f /tmp/blas ]; do date; sleep 1; done; echo "My work is done. Waiting to be killed..."; sleep 3600s']
    lifecycle:
      preStop:
        exec:
          command: ['sh', '-c', 'touch /tmp/blas; sleep 3600s']
----

Los siguientes eventos aparecen desde que se aplica el c√≥digo anterior hasta
que se elimina con `kubectl delete`.  Puede verse que se considera que el
_hook_ ha fallado por timeout:

[source,console]
----
$ kubectl get events -o custom-columns=LATSEEN:.lastTimestamp,COUNT:.count,TYPE:.type,REASON:.reason,OBJECT:.involvedObject.name,MESSAGE:.message --watch
LATSEEN                COUNT   TYPE      REASON              OBJECT        MESSAGE
2022-07-08T16:26:00Z   1       Normal    Scheduled           pod-prestop   Successfully assigned blas/pod-prestop to minikube-m02
2022-07-08T16:26:01Z   1       Normal    Pulling             pod-prestop   Pulling image "nginx"
2022-07-08T16:26:02Z   1       Normal    Pulled              pod-prestop   Successfully pulled image "nginx" in 1.410463062s
2022-07-08T16:26:02Z   1       Normal    Created             pod-prestop   Created container loop
2022-07-08T16:26:02Z   1       Normal    Started             pod-prestop   Started container loop
2022-07-08T16:26:20Z   1       Normal    Killing             pod-prestop   Stopping container loop
2022-07-08T16:27:20Z   1       Warning   FailedPreStopHook   pod-prestop   Exec lifecycle hook ([sh -c touch /tmp/blas; sleep 3600s]) for Container "loop" in Pod "pod-prestop_blas(80a91549-811c-48e1-97fa-4149528da8ed)" failed - error: command 'sh -c touch /tmp/blas; sleep 3600s' exited with 137: , message: ""
----

Si quitamos el `sleep 3600s` del final del c√≥digo del _hook_, se elimina el
contenedor sin ning√∫n tipo de error, aunque el proceso principal no haya
terminado por s√≠ mismo:

[source,console]
----
$ kubectl get events -o custom-columns=LATSEEN:.lastTimestamp,COUNT:.count,TYPE:.type,REASON:.reason,OBJECT:.involvedObject.name,MESSAGE:.message --watch
LATSEEN                COUNT   TYPE      REASON              OBJECT        MESSAGE
2022-07-08T16:33:28Z   1       Normal    Scheduled           pod-prestop   Successfully assigned blas/pod-prestop to minikube-m02
2022-07-08T16:33:29Z   1       Normal    Pulling             pod-prestop   Pulling image "nginx"
2022-07-08T16:33:30Z   1       Normal    Pulled              pod-prestop   Successfully pulled image "nginx" in 1.495145804s
2022-07-08T16:33:30Z   1       Normal    Created             pod-prestop   Created container loop
2022-07-08T16:33:30Z   1       Normal    Started             pod-prestop   Started container loop
2022-07-08T16:33:47Z   1       Normal    Killing             pod-prestop   Stopping container loop
----

El proceso principal tambi√©n puede terminar por s√≠ mismo sin esperar a recibir
una se√±al para terminar, pero en ese caso puede aparecer un error debido a que
K8s no encuentra el contenedor que quiere eliminar, aunque no es nada grave:

[source,yaml]
----
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-prestop
spec:
  containers:
  - name: loop
    image: nginx
    command:  ['sh', '-c', 'while [ ! -f /tmp/blas ]; do date; sleep 1; done; echo "My work is done."']
    lifecycle:
      preStop:
        exec:
          command: ['sh', '-c', 'touch /tmp/blas']
----

[source,console]
----
$ kubectl get events -o custom-columns=LATSEEN:.lastTimestamp,COUNT:.count,TYPE:.type,REASON:.reason,OBJECT:.involvedObject.name,MESSAGE:.message --watch
LATSEEN                COUNT   TYPE      REASON              OBJECT        MESSAGE
2022-07-08T16:43:48Z   1       Normal    Scheduled           pod-prestop   Successfully assigned blas/pod-prestop to minikube-m02
2022-07-08T16:43:48Z   1       Normal    Pulling             pod-prestop   Pulling image "nginx"
2022-07-08T16:43:50Z   1       Normal    Pulled              pod-prestop   Successfully pulled image "nginx" in 1.409953675s
2022-07-08T16:43:50Z   1       Normal    Created             pod-prestop   Created container loop
2022-07-08T16:43:50Z   1       Normal    Started             pod-prestop   Started container loop
2022-07-08T16:44:02Z   1       Normal    Killing             pod-prestop   Stopping container loop
2022-07-08T16:44:05Z   2       Normal    Killing             pod-prestop   Stopping container loop
2022-07-08T16:44:05Z   1       Warning   FailedKillPod       pod-prestop   error killing pod: failed to "KillContainer" for "loop" with KillContainerError: "rpc error: code = Unknown desc = Error response from daemon: No such container: 94da5b1db2e66dcb401de89732778a058eb6a91ba99a23c5f9f55ea63e2b19b3"
----

== Control de los trabajos

La misi√≥n principal de K8s es asegurarse de los trabajos se ejecutan
adecuadamente, monitoriz√°ndolos y asign√°ndoles los recursos que necesiten.
Para ello disponemos de _workload resources_, recursos que gestionan los
trabajos, como _Deployments_, _ReplicaSets_, _Jobs_...

NOTE: Aunque solo queramos tener una instancia de un pod, en vez de lanzarla
manualmente es mejor utilizar siempre alg√∫n tipo de controlador para garantizar
su funcionamiento.

Algunos controladores, como los <<deployments>> y los <<replicasets>>, permiten
cambiar el n√∫mero de r√©plicas que gestionan mediante la orden `kubectl scale
--replicas=<n>`.  Los que guardan informaci√≥n sobre las versiones desplegadas,
como los <<deployments>>, permiten gestionar los despliegues con `kubectl
rollout`.

[[replicasets,_replica sets_]]
=== Replica Sets

Los _replica sets_ (`ReplicaSet`), garantizan que hay un n√∫mero determinado de
r√©plicas de un pod funcionando (levantados y disponibles), creando los que
falten o eliminando los que sobren.  Los pods se sustituyen autom√°ticamente si
fallan, se eliminan o terminan, utilizando para ello la plantilla del pod
especificada en su definici√≥n.  Se tiene en cuenta el estado de los pods en
todos los nodos.

NOTE: Aunque podemos utilizar directamente los _replica sets_, es mejor
utilizar <<deployments>>, que son conceptos de m√°s alto nivel que utilizan
_replica sets_ y proporcionan m√°s funcionalidades.

El siguiente ejemplo define un `ReplicaSet` con tres pods de nginx:

.rs-nginx.yaml
[source,yaml]
----
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
  labels:
    app: app-nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app-nginx
  template:
    metadata:
      name: nginx
      labels:
        app: app-nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
----

[source,console]
----
$ kubectl apply -f rs-nginx.yaml
replicaset.apps/nginx-rs created
----

Estos son los eventos que se producen al ejecutar la orden anterior:

[source,console]
----
$ kubectl get events --watch
1s          Normal    Scheduled          pod/nginx-rs-t46pz    Successfully assigned blas/nginx-rs-t46pz to minikube-m03
1s          Normal    SuccessfulCreate   replicaset/nginx-rs   Created pod: nginx-rs-t46pz
1s          Normal    SuccessfulCreate   replicaset/nginx-rs   Created pod: nginx-rs-z87k5
0s          Normal    Scheduled          pod/nginx-rs-58npq    Successfully assigned blas/nginx-rs-58npq to minikube
0s          Normal    SuccessfulCreate   replicaset/nginx-rs   Created pod: nginx-rs-58npq
0s          Normal    Scheduled          pod/nginx-rs-z87k5    Successfully assigned blas/nginx-rs-z87k5 to minikube-m02
0s          Normal    Pulling            pod/nginx-rs-z87k5    Pulling image "nginx"
0s          Normal    Pulling            pod/nginx-rs-58npq    Pulling image "nginx"
0s          Normal    Pulling            pod/nginx-rs-t46pz    Pulling image "nginx"
0s          Normal    Pulled             pod/nginx-rs-z87k5    Successfully pulled image "nginx" in 1.354562976s
0s          Normal    Pulled             pod/nginx-rs-58npq    Successfully pulled image "nginx" in 1.317435738s
0s          Normal    Created            pod/nginx-rs-z87k5    Created container nginx
0s          Normal    Created            pod/nginx-rs-58npq    Created container nginx
0s          Normal    Pulled             pod/nginx-rs-t46pz    Successfully pulled image "nginx" in 1.379200875s
0s          Normal    Created            pod/nginx-rs-t46pz    Created container nginx
0s          Normal    Started            pod/nginx-rs-58npq    Started container nginx
0s          Normal    Started            pod/nginx-rs-z87k5    Started container nginx
0s          Normal    Started            pod/nginx-rs-t46pz    Started container nginx
----

[source,console]
----
$ kubectl get rs
NAME       DESIRED   CURRENT   READY   AGE
nginx-rs   3         3         3       76s
----

[source,console]
----
$ kubectl describe rs nginx-rs
Name:         nginx-rs
Namespace:    blas
Selector:     app=app-nginx
Labels:       app=app-nginx
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=app-nginx
  Containers:
   nginx:
    Image:        nginx
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  93s   replicaset-controller  Created pod: nginx-rs-7gprq
  Normal  SuccessfulCreate  93s   replicaset-controller  Created pod: nginx-rs-2hlpr
  Normal  SuccessfulCreate  92s   replicaset-controller  Created pod: nginx-rs-ltzt7
----

[source,console]
----
$ kubectl get pods
nginx-rs-2hlpr   1/1     Running   0          2m
nginx-rs-7gprq   1/1     Running   0          2m
nginx-rs-ltzt7   1/1     Running   0          2m
----

El seleccionador `matchLabels` del _replica set_ identifica los pods que ser√°n
controlados por √©l.  Un _replica set_ est√° enlazado con sus pods mediante el
campo `metadata.ownerReferences` de estos, que especifica qu√© recurso es el
propietario de un objeto:

[source,console]
----
$ kubectl get pods nginx-rs-2hlpr -o yaml
kubectl get pods nginx-rs-6wxmb -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2022-06-17T12:10:39Z"
  generateName: nginx-rs-
  labels:
    app: app-nginx
  name: nginx-rs-6wxmb
  namespace: blas
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: nginx-rs
    uid: 0ca66e0f-5951-47dd-a1d3-b4c22a1db7b6
  resourceVersion: "20754"
  uid: 279d249e-668a-4968-80fd-01a45942f805
...
----

Si un nuevo pod cumple con el seleccionador de un _replica set_, ser√° adquirido
por √©l, siempre que no tenga ya un propietario o su propietario no sea un
controlador.  Podemos ver esto con el siguiente ejemplo, donde creamos un nuevo
pod manualmente con la etiqueta del seleccionador usado en nuestro _replica
set_.  El pod se crea, pero se destruye inmediatamente porque ya tenemos los
tres pods del _replica set_ funcionando:

.rs-new-pod.yaml
[source,source]
----
---
apiVersion: v1
kind: Pod
metadata:
  name: new-pod
  labels:
    app: app-nginx
spec:
  containers:
  - name: new-nginx
    image: nginx
    ports:
    - containerPort: 80
----

[source,console]
----
$ kubectl apply -f rs-new-pod.yaml
pod/new-pod created
----

[source,console]
----
$ kubectl get events --watch
0s          Normal    Scheduled          pod/new-pod           Successfully assigned blas/new-pod to minikube-m02
0s          Normal    SuccessfulDelete   replicaset/nginx-rs   Deleted pod: new-pod
0s          Normal    Pulling            pod/new-pod           Pulling image "nginx"
0s          Normal    Pulled             pod/new-pod           Successfully pulled image "nginx" in 1.452625358s
0s          Normal    Created            pod/new-pod           Created container new-nginx
0s          Normal    Started            pod/new-pod           Started container new-nginx
0s          Normal    Killing            pod/new-pod           Stopping container new-nginx
----

[source,console]
----
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
nginx-rs-2hlpr   1/1     Running   0          8m14s
nginx-rs-7gprq   1/1     Running   0          8m14s
nginx-rs-ltzt7   1/1     Running   0          8m14s
----

Si lo hacemos al rev√©s, primero creando el pod y luego el _replica set_, pasa
lo contrario, manteni√©ndose el pod que creamos manualmente y a√±adi√©ndose otros
dos:

[source,console]
----
$ kubectl apply -f rs-new-pod.yaml
pod/new-pod created
----

[source,console]
----
$ kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
new-pod   1/1     Running   0          9s
----

[source,console]
----
$ kubectl apply -f rs-nginx.yaml
replicaset.apps/nginx-rs created
----

[source,console]
----
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
new-pod          1/1     Running   0          35s
nginx-rs-9pg7q   1/1     Running   0          10s
nginx-rs-scjhn   1/1     Running   0          10s
----

Podemos comprobar que el pod creado manualmente ahora est√° controlado por el
_replica set_:

[source,console]
----
$ kubectl describe pod/new-pod
Name:         new-pod
Namespace:    blas
Priority:     0
Node:         minikube-m03/192.168.49.4
Start Time:   Fri, 17 Jun 2022 14:03:13 +0200
Labels:       app=app-nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.10
IPs:
  IP:           10.244.2.10
Controlled By:  ReplicaSet/nginx-rs
...
----

Si eliminamos cualquiera de los pods controlados por el _replica set_, se
sustituye por uno nuevo inmediatamente:

[source,console]
----
$ kubectl delete pod new-pod
pod "new-pod" deleted
----

[source,console]
----
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
nginx-rs-6wxmb   1/1     Running   0          8s
nginx-rs-9pg7q   1/1     Running   0          7m9s
nginx-rs-scjhn   1/1     Running   0          7m9s
----

Al eliminar un _replica set_, se cambia el n√∫mero de objetos controlados por √©l
a 0 para terminarlos, y despu√©s se elimina el propio _replica set_:

[source,console]
----
$ kubectl delete replicaset/nginx-rs
replicaset.apps "nginx-rs" deleted

$ kubectl get pods
No resources found in blas namespace.
----

Se puede eliminar un _replica set_ sin borrar los pods que controla usando la
opci√≥n `--cascade=orphan` de `kubectl delete`.  Esto permitir√≠a, por ejemplo,
sustituir un _replica set_ por otro nuevo para controlar los mismos pods,
aunque, si este tuviera una nueva plantilla para los pods, solo se utilizar√≠a
para los pods nuevos que hubiera que crear.

Otra cosa que puede ser √∫til es hacer que un pod deje de estar controlado por
un _replica set_, cambiando sus etiquetas.

Se puede cambiar al vuelo el n√∫mero de pods controlados por un _replica set_
cambiando su campo `.spec.replicas`.  Los pods se crear√°n o se destruir√°n seg√∫n
sea necesario.  Se puede automatizar esto utilizando un _horizontal pod
autoscaler_, como el siguiente:

[source,yaml]
----
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-scaler
spec:
  scaleTargetRef:
    kind: ReplicaSet
    name: nginx-rs
  minReplicas: 5
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
----

[source,console]
----
$ # Otra forma de crear el autoescalador sin usar un YAML.
$ kubectl autoscale rs nginx-rs --max=10 --min=5 --cpu-percent=50
horizontalpodautoscaler.autoscaling/nginx-rs autoscaled
----

Para que el ejemplo de autoescalado funcione, hace falta tener habilitado el
<<metrics_server>>.  Si no, podemos ver un error en autoescalador:

[source,console]
----
$ kubectl describe horizontalpodautoscalers.autoscaling nginx-rs
Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler
Name:                                                  nginx-rs
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Mon, 20 Jun 2022 12:10:56 +0200
Reference:                                             ReplicaSet/nginx-rs
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  <unknown> / 50%
Min replicas:                                          5
Max replicas:                                          10
ReplicaSet pods:                                       5 current / 5 desired
Conditions:
  Type           Status  Reason                   Message
  ----           ------  ------                   -------
  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target's current scale
  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
Events:
  Type     Reason                        Age   From                       Message
  ----     ------                        ----  ----                       -------
  Normal   SuccessfulRescale             26s   horizontal-pod-autoscaler  New size: 5; reason: Current number of replicas below Spec.MinReplicas
  Warning  FailedGetResourceMetric       10s   horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  Warning  FailedComputeMetricsReplicas  10s   horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
----

[source,console]
----
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
nginx-rs-2z8xx   1/1     Running   0          85s
nginx-rs-c5x48   1/1     Running   0          85s
nginx-rs-clx8v   1/1     Running   0          35s
nginx-rs-k4894   1/1     Running   0          85s
nginx-rs-znxwq   1/1     Running   0          35s
----

En este caso, se han creado dos pods adicionales porque no se cumpl√≠a con el
m√≠nimo pedido en el autoescalador, pero el escalado por uso de la CPU no
funcionar√°.

[[deployments,_deployments_]]
=== Deployments

Los despliegues (_deployments_), son un m√©todo declarativo de gestionar pods
utilizando por debajo <<replicasets>>.  Es la forma recomendada de gestionar
los pods en un cluster de K8s.

El siguiente es un ejemplo de un despliegue compuesto por tres pods de nginx:

.dep-nginx.yaml
[source,yaml]
----
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
----

Al aplicarlo, se crea el `Deployment`, los pods y el <<replicasets,_replica
set_>> que los gestiona:

[source,console]
----

$ kubectl apply -f dep-nginx.yaml
deployment.apps/nginx-deployment created

$ kubectl rollout status deployment/nginx-deployment
Waiting for deployment "nginx-deployment" rollout to finish: 0 of 3 updated replicas are available...
Waiting for deployment "nginx-deployment" rollout to finish: 1 of 3 updated replicas are available...
Waiting for deployment "nginx-deployment" rollout to finish: 2 of 3 updated replicas are available...
deployment "nginx-deployment" successfully rolled out

$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-74d589986c-lhwsg   1/1     Running   0          5s
nginx-deployment-74d589986c-qvqfm   1/1     Running   0          5s
nginx-deployment-74d589986c-vsmbz   1/1     Running   0          5s

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   3         3         3       22s

$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           35s
----

Los pods y los <<replicasets>> gestionados con despliegues tienen en sus
nombres el valor de la etiqueta `pod-template-hash` que el despliegue incluye
en ellos.  Esta etiqueta es un n√∫mero aleatorio calculado usando como semilla
el hash del `PodTemplate`:

[source,console]
----

$ kubectl describe pod/nginx-deployment-74d589986c-lhwsg
Name:         nginx-deployment-74d589986c-lhwsg
Namespace:    default
Priority:     0
Node:         minikube-m03/192.168.49.4
Start Time:   Mon, 20 Jun 2022 12:46:37 +0200
Labels:       app=nginx
              pod-template-hash=74d589986c
...
----

Al eliminar el despliegue, se eliminan todos los recursos que cre√≥:

[source,console]
----
$ kubectl delete deployment nginx-deployment
deployment.apps "nginx-deployment" deleted

$ kubectl get pods
No resources found in default namespace.
----

Los despliegues permiten usar como selectores `matchLabels` y/o
`matchExpressions`, con la condici√≥n de que la plantilla del pod cumpla con
ellos.

==== Actualizaci√≥n de un despliegue

Cuando se cambia la plantilla de los pods de un despliegue, se lanza el
despliegue para aplicar los cambios (se hace un _rollout_).  La forma de
aplicar los cambios depende de la estrategia configurada en el campo
`.spec.strategy` del despliegue, que puede ser `Recreate` o `RollingUpdate`,
que es el valor por defecto.

Con la estrategia `Recreate`, primero se eliminan todos los pods actuales y
despu√©s se crean los nuevos.  Esto no es muy recomendable, porque si falla la
creaci√≥n de los nuevos pods nos quedaremos sin servicio.

La estrategia `RollingUpdate` crea nuevos pods con la nueva plantilla y elimina
los antiguos por tandas, hasta sustituirlos todos.  Por defecto, y siempre sin
contar los pods que est√©n en estado _terminating_, se permite tener hasta un
125% m√°s de pods que el m√°ximo permitido (un 25% de aumento), y se garantiza
que al menos se tiene un 75% del n√∫mero deseado levantados (un 25% no
disponible).  Estos valores pueden configurarse en los campos
`.spec.strategy.rollingUpdate.maxUnavailable` y
`.spec.strategy.rollingUpdate.maxSurge` del despliegue, que pueden ser valores
absolutos o porcentajes sobre el n√∫mero de pods deseados, que se redondean
hacia abajo o hacia arriba, respectivamente, para calcular el valor final.

Las siguientes salidas se han obtenido justo despu√©s de editar la definici√≥n
del despliegue anterior para a√±adir una etiqueta en los pods.  Atenci√≥n a c√≥mo
cambia el nombre de las etiquetas de los nuevos recursos creados:

[source,console]
----
$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-74d589986c-c287l   1/1     Running   0          9m21s
nginx-deployment-74d589986c-fgz2b   1/1     Running   0          9m24s
nginx-deployment-74d589986c-qtwlc   1/1     Running   0          9m18s
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-deployment-74d589986c-c287l   1/1     Running             0          9m36s
nginx-deployment-74d589986c-fgz2b   1/1     Running             0          9m39s
nginx-deployment-74d589986c-qtwlc   1/1     Running             0          9m33s
nginx-deployment-795bc797c7-xjh4p   0/1     ContainerCreating   0          2s
$ kubectl get pods
NAME                                READY   STATUS        RESTARTS   AGE
nginx-deployment-74d589986c-c287l   1/1     Terminating   0          9m37s
nginx-deployment-74d589986c-fgz2b   1/1     Running       0          9m40s
nginx-deployment-74d589986c-qtwlc   1/1     Running       0          9m34s
nginx-deployment-795bc797c7-xjh4p   1/1     Running       0          3s
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-deployment-74d589986c-fgz2b   1/1     Running             0          9m41s
nginx-deployment-74d589986c-qtwlc   1/1     Running             0          9m35s
nginx-deployment-795bc797c7-q7x74   0/1     ContainerCreating   0          1s
nginx-deployment-795bc797c7-xjh4p   1/1     Running             0          4s
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-deployment-74d589986c-fgz2b   1/1     Running             0          9m43s
nginx-deployment-74d589986c-qtwlc   1/1     Running             0          9m37s
nginx-deployment-795bc797c7-q7x74   0/1     ContainerCreating   0          3s
nginx-deployment-795bc797c7-xjh4p   1/1     Running             0          6s
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-deployment-74d589986c-fgz2b   1/1     Terminating         0          9m44s
nginx-deployment-74d589986c-qtwlc   1/1     Running             0          9m38s
nginx-deployment-795bc797c7-nmst8   0/1     ContainerCreating   0          1s
nginx-deployment-795bc797c7-q7x74   1/1     Running             0          4s
nginx-deployment-795bc797c7-xjh4p   1/1     Running             0          7s
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-deployment-74d589986c-qtwlc   1/1     Running             0          9m39s
nginx-deployment-795bc797c7-nmst8   0/1     ContainerCreating   0          2s
nginx-deployment-795bc797c7-q7x74   1/1     Running             0          5s
nginx-deployment-795bc797c7-xjh4p   1/1     Running             0          8s
$ kubectl get pods
NAME                                READY   STATUS        RESTARTS   AGE
nginx-deployment-74d589986c-qtwlc   1/1     Terminating   0          9m40s
nginx-deployment-795bc797c7-nmst8   1/1     Running       0          3s
nginx-deployment-795bc797c7-q7x74   1/1     Running       0          6s
nginx-deployment-795bc797c7-xjh4p   1/1     Running       0          9s
$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-795bc797c7-nmst8   1/1     Running   0          4s
nginx-deployment-795bc797c7-q7x74   1/1     Running   0          7s
nginx-deployment-795bc797c7-xjh4p   1/1     Running   0          10s

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   0         0         0       3h58m
nginx-deployment-795bc797c7   3         3         3       42s
----

Se puede ver c√≥mo se van modificando los valores de los <<replicasets>>
gestionados por los despliegues viendo los eventos "scaled up" y "scaled down"
de estos:

[source,console]
----
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Mon, 20 Jun 2022 12:46:37 +0200
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 6
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
           etiqueta=blas
  Containers:
   nginx:
    Image:        nginx
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deployment-795bc797c7 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  76s   deployment-controller  Scaled up replica set nginx-deployment-795bc797c7 to 1
  Normal  ScalingReplicaSet  73s   deployment-controller  Scaled down replica set nginx-deployment-74d589986c to 2
  Normal  ScalingReplicaSet  73s   deployment-controller  Scaled up replica set nginx-deployment-795bc797c7 to 2
  Normal  ScalingReplicaSet  70s   deployment-controller  Scaled up replica set nginx-deployment-795bc797c7 to 3
  Normal  ScalingReplicaSet  70s   deployment-controller  Scaled down replica set nginx-deployment-74d589986c to 1
  Normal  ScalingReplicaSet  67s   deployment-controller  Scaled down replica set nginx-deployment-74d589986c to 0
----

Si deshacemos el cambio, la plantilla del pod vuelve a quedar como estaba, y el
valor del campo `pod-template-hash` de los nuevos recursos que se crean como
parte del lanzamiento del despliegue coincide con el que ten√≠amos
originalmente.

==== Historia de los despliegues

K8s guarda la historia de los lanzamientos hechos con un despliegue,
conservando los <<replicasets>> correspondientes:

[source,console]
----
$ kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
2         <none>
5         <none>
6         <none>
----

La columna `CHANGE-CAUSE` se obtiene del campo `kubernetes.io/change-cause` del
despliegue, que se puede establecer a√±adiendo la opci√≥n `--record` a la orden
`kubectl` que provoc√≥ el cambio, en cuyo caso se guardar√° la orden en la
descripci√≥n del cambio, o cambiando la anotaci√≥n de la versi√≥n actual del
despliegue con `kubectl annotate deployment/XXXXX
kubernetes.io/change-cause="blablabla"`.

Por defecto, se guardan 10 versiones, pero se puede cambiar este valor
cambiando el campo del despliegue `.spec.revisionHistoryLimit`:

[source,console]
----
$ kubectl get deploy/nginx-deployment -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
...
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
...
----

Se puede ver c√≥mo es cada versi√≥n a√±adiendo la opci√≥n `--revision=<n>`.  En
este caso, solo cambian las etiquetas entre versiones:

[source,console]
----
$ kubectl rollout history deployment/nginx-deployment --revision=2
deployment.apps/nginx-deployment with revision #2
Pod Template:
  Labels:       app=nginx
        otra=blas
        pod-template-hash=7678d86c77
  Containers:
   nginx:
    Image:      nginx
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

$ kubectl rollout history deployment/nginx-deployment --revision=5
deployment.apps/nginx-deployment with revision #5
Pod Template:
  Labels:       app=nginx
        pod-template-hash=74d589986c
  Containers:
   nginx:
    Image:      nginx
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

$ kubectl rollout history deployment/nginx-deployment --revision=6
deployment.apps/nginx-deployment with revision #6
Pod Template:
  Labels:       app=nginx
        etiqueta=blas
        pod-template-hash=795bc797c7
  Containers:
   nginx:
    Image:      nginx
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
----

Se puede volver a la versi√≥n anterior de un despliegue as√≠:

[source,console]
----
$ kubectl rollout undo deployment/nginx-deployment
deployment.apps/nginx-deployment rolled back
----

Se puede volver a una versi√≥n concreta a√±adiendo `--to-revision=<n>` a la orden
anterior.

==== Escalado

Se puede cambiar los par√°metros de escalado de un despliegue con `kubectl
scale`.  Como eso con cambia la plantilla del pod del despliegue, no se generan
nuevas versiones de la historia:

[source,console]
----
$ kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
2         <none>
10        <none>
11        <none>

$ kubectl scale deployment/nginx-deployment --replicas=10
deployment.apps/nginx-deployment scaled

$ kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
2         <none>
10        <none>
11        <none>

$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-74d589986c-4rwmk   1/1     Running   0          15s
nginx-deployment-74d589986c-8r858   1/1     Running   0          15s
nginx-deployment-74d589986c-9dncr   1/1     Running   0          4m26s
nginx-deployment-74d589986c-fvqgv   1/1     Running   0          15s
nginx-deployment-74d589986c-kvzwv   1/1     Running   0          15s
nginx-deployment-74d589986c-lmnlk   1/1     Running   0          4m23s
nginx-deployment-74d589986c-p5cpp   1/1     Running   0          15s
nginx-deployment-74d589986c-s7kpl   1/1     Running   0          15s
nginx-deployment-74d589986c-v8kdw   1/1     Running   0          15s
nginx-deployment-74d589986c-xg2jl   1/1     Running   0          4m20s
----

Si tenemos habilitado el autoescalado horizontal en el cluster, se puede
configurar un autoescalado basado en el consumo de CPU:

[source,console]
----
$ kubectl autoscale deployment/nginx-deployment --min=3 --max=10 --cpu-percent=2
horizontalpodautoscaler.autoscaling/nginx-deployment autoscaled

$ kubectl get horizontalpodautoscalers.autoscaling
NAME               REFERENCE                     TARGETS        MINPODS   MAXPODS   REPLICAS   AGE
nginx-deployment   Deployment/nginx-deployment   <unknown>/2%   3         10        3          43s
----

Como puede verse, la orden anterior crea un `HorizontalPodAutoscaler`
(abreviado, `hpa`), del que podemos ver los detalles con `kubectl get`, en YAML
o en JSON:

[source,console]
----
$ kubectl get hpa nginx-deployment -o=yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: "2022-06-21T15:24:36Z"
  name: nginx-deployment
  namespace: blas
  resourceVersion: "64059"
  uid: 3652214e-da75-4848-aa3e-ef4b59a181f0
spec:
  maxReplicas: 10
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 2
        type: Utilization
    type: Resource
  minReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
status:
  conditions:
  - lastTransitionTime: "2022-06-21T15:24:51Z"
    message: the HPA controller was able to get the target's current scale
    reason: SucceededGetScale
    status: "True"
    type: AbleToScale
  - lastTransitionTime: "2022-06-21T15:24:51Z"
    message: 'the HPA was unable to compute the replica count: failed to get cpu utilization:
      missing request for cpu'
    reason: FailedGetResourceMetric
    status: "False"
    type: ScalingActive
  currentMetrics: null
  currentReplicas: 3
  desiredReplicas: 0
----

[source,console]
----
$ kubectl get horizontalpodautoscalers/nginx-deployment -o=json
{
    "apiVersion": "autoscaling/v2",
    "kind": "HorizontalPodAutoscaler",
    "metadata": {
        "creationTimestamp": "2022-06-21T15:24:36Z",
        "name": "nginx-deployment",
        "namespace": "blas",
        "resourceVersion": "64059",
        "uid": "3652214e-da75-4848-aa3e-ef4b59a181f0"
    },
    "spec": {
        "maxReplicas": 10,
        "metrics": [
            {
                "resource": {
                    "name": "cpu",
                    "target": {
                        "averageUtilization": 2,
                        "type": "Utilization"
                    }
                },
                "type": "Resource"
            }
        ],
        "minReplicas": 3,
        "scaleTargetRef": {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "name": "nginx-deployment"
        }
    },
    "status": {
        "conditions": [
            {
                "lastTransitionTime": "2022-06-21T15:24:51Z",
                "message": "the HPA controller was able to get the target's current scale",
                "reason": "SucceededGetScale",
                "status": "True",
                "type": "AbleToScale"
            },
            {
                "lastTransitionTime": "2022-06-21T15:24:51Z",
                "message": "the HPA was unable to compute the replica count: failed to get cpu utilization: missing request for cpu",
                "reason": "FailedGetResourceMetric",
                "status": "False",
                "type": "ScalingActive"
            }
        ],
        "currentMetrics": null,
        "currentReplicas": 3,
        "desiredReplicas": 0
    }
}
----

Podemos cambiar los par√°metros del autoescalador en JSON:

[source,console]
----
$ kubectl patch hpa nginx-deployment --patch '{"spec":{"minReplicas":5}}'
horizontalpodautoscaler.autoscaling/nginx-deployment patched

$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-74d589986c-26ch9   1/1     Running   0          8s
nginx-deployment-74d589986c-fgz2t   1/1     Running   0          24m
nginx-deployment-74d589986c-ngph9   1/1     Running   0          24m
nginx-deployment-74d589986c-p7q86   1/1     Running   0          24m
nginx-deployment-74d589986c-r6dlq   1/1     Running   0          8s
----

==== Escalado proporcional

Si pidi√©ramos escalar un despliegue que estuviera en mitad de un lanzamiento,
por ejemplo, incrementando el n√∫mero de r√©plicas deseadas, las nuevas
instancias se repartir√≠an entre los `ReplicaSet` que estuvieran activos de
manera proporcional al n√∫mero de r√©plicas deseado en cada uno de ellos.  A esto
se le llama _proportional scaling_:

[source,console]
----
$ kubectl get deployments
No resources found in blas namespace.

$ kubectl apply -f dep-nginx.yaml
deployment.apps/nginx-deployment created

$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           8s

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   3         3         3       17s

$ kubectl scale deployment nginx-deployment --replicas=10
deployment.apps/nginx-deployment scaled

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   10        10        10      57s

$ # Actualizamos el despliegue con una imagen que no existe para mantener
$ # activos dos ReplicaSets.
$ kubectl set image deployment/nginx-deployment nginx=nginx:blas
deployment.apps/nginx-deployment image updated

$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   8/10    5            8           91s

$ # Vemos los dos ReplicaSets activos, el antiguo con m√°s r√©plicas deseadas
$ # que el nuevo, por estar en mitad de un rollout.
$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   8         8         8       95s
nginx-deployment-dd56879bf    5         5         0       16s

$ # Subimos el n√∫mero de r√©plicas deseadas del despliegue a 20.
$ kubectl scale deployment nginx-deployment --replicas=20
deployment.apps/nginx-deployment scaled

$ # Y comprobamos que se han asignado a los dos ReplicaSets de forma
$ # propocional.
$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-74d589986c   15        15        15      2m4s
nginx-deployment-dd56879bf    10        10        0       45s
----

==== Pausa y reanudaci√≥n de lanzamientos

Es posible poner en pausa los lanzamientos de un despliegue, lo que es √∫til si
queremos hacer varios cambios en √©l y evitar que cada uno de los cambios
provoque un lanzamiento.  Para hacerlo, se usa la orden `kubectl rollout pause
<despliegue>`, y para reanudarlos de nuevo se usa `kubectl rollour resume
<despliegue>`.

== El modelo de red de Kubernetes

K8s implanta un modelo de red en el que cada pod dispone de su propia direcci√≥n
IP dentro del cluster, completamente independiente de las direcciones IP de los
nodos.  Los pods se pueden comunicar entre s√≠ sin NAT, incluso estando en nodos
distintos.  Los nodos pueden comunicarse con los pods sin NAT, y los agentes
del nodo, como kubelet o los demonios del sistema, pueden comunicarse
libremente con todos los pods de su nodo.

Como los contenedores de un pod comparten el espacio de nombres de red, se
pueden comunicar entre ellos utilizando la direcci√≥n IP del localhost
(127.0.0.1).  Para que haya comunicaci√≥n entre pods, es necesario que los
contenedores sean capaces de encontrar sus direcciones IP, para lo que se
pueden usar <<services>>.

[[services,_services_]]
=== Servicios

Los servicios (_services_) permiten referirse a puertos de comunicaciones de un
conjunto de pods, sin tener que conocer exactamente qu√© contenedores los
componen ni qu√© direcciones IP tienen asignadas.  Son abstracciones que definen
servicios de red en un grupo l√≥gico de pods y una pol√≠tica para acceder a
ellos.  Por lo general, los servicios utilizan <<seleccionadores>> para
determinar a qu√© pods deben dirigirse las peticiones, aunque tambi√©n podemos
tener servicios sin seleccionadores.

Los servicios suelen utilizarse con balanceadores de carga.  Los servicios se
encargan de reconfigurar los balanceadores para que las solicitudes lleguen
hasta los pods que est√©n disponibles en ese momento.

Los servicios se definen como cualquier otro objeto de K8s.  Cuando se crean,
K8s les asigna su propia direcci√≥n IP interna, conocida como la _cluster IP_,
en la que escucha `kube-proxy` en los puertos especificados.  Esta direcci√≥n
solo est√° accesible desde los pods y los nodos.  El controlador del servicio
monitoriza los pods que concuerden con el seleccionador para reenviarles las
peticiones que lleguen al servicio a los puertos configurados de alguno de
ellos.

El protocolo por defecto es TCP, pero est√°n soportados los siguientes:

* TCP.

* UDP.

* SCTP, si el plugin de red lo soporta.

* HTTP, para los proveedores cloud que lo soporten con balanceadores de carga.
  Tambi√©n se pueden utilizar <<ingress>> para esto.

* PROXY, para los proveedores cloud que lo soporten con balanceadores de carga.

Lo siguiente es un ejemplo de un servicio b√°sico que redirige las peticiones
que le llegan al puerto TCP 3306 al mismo puerto de los pods que tengan las
etiquetas `app` y `service` adecuadas.  Al no especificar el `targetPort` de
los pods, se utiliza para el puerto de destino el mismo valor de `port`:

[source,yaml]
----
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
spec:
  selector:
    app: wp
    service: mysql
  ports:
  - protocol: TCP
    port: 3306
----

Como los puertos se pueden nombrar, en vez de un n√∫mero de puerto en
`targetPort`, se puede especificar un nombre que cada contenedor puede definir
con un n√∫mero distinto.  Al evitar valores absolutos, se mejora la flexibilidad
de las configuraciones.

Cuando definimos un servicio, dentro de los pods se definen unas variables de
entorno cuyo nombre empieza por el nombre del servicio, y que permiten ubicar
tanto su direcci√≥n como el puerto.  La direcci√≥n tambi√©n est√° accesible
por DNS.  Por ejemplo, con la definici√≥n de servicio anterior desplegado en el
namespace _blas_, podemos ver las siguientes variables de entorno en cualquiera
de los pods del namespace:

----
$ kubectl get services
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
mysql-service   ClusterIP   10.106.99.227   <none>        3306/TCP   26h

$ kubectl exec -ti dep-wordpress-02-76d5696858-h2bpz -- bash
root@dep-wordpress-02-76d5696858-h2bpz:/var/www/html# env | grep MYSQ
MYSQL_SERVICE_PORT_3306_TCP_ADDR=10.106.99.227
MYSQL_SERVICE_SERVICE_HOST=10.106.99.227
MYSQL_SERVICE_PORT=tcp://10.106.99.227:3306
MYSQL_SERVICE_PORT_3306_TCP=tcp://10.106.99.227:3306
MYSQL_SERVICE_PORT_3306_TCP_PORT=3306
MYSQL_SERVICE_SERVICE_PORT=3306
MYSQL_SERVICE_PORT_3306_TCP_PROTO=tcp

root@dep-wordpress-02-76d5696858-h2bpz:/var/www/html# dig +short  mysql-service.blas.svc.cluster.local
10.106.99.227
----

Los servicios utilizan otro tipo de objeto de K8s para decidir a d√≥nde enviar
el tr√°fico, los _endpoints_.  Cada servicio tiene un _endpoint_ asociado, que se
llama como √©l, y que el servicio actualiza cuando cambian los pods que cumplen
con el seleccionador.  Los _endpoints_ pueden verse con la orden `kubectl get
endpoints`:

[source,console]
----
$ kubectl get services
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
mysql-service   ClusterIP   10.101.219.16   <none>        3306/TCP   16h

$ kubectl get endpoints
NAME            ENDPOINTS          AGE
mysql-service   10.244.3.22:3306   16h
----

Como puede verse en las salidas anteriores, las direcciones de red son
distintas para el servicio, tambi√©n llamadas _cluster IP_, que se implementan
con `kube-proxy`, que para los _endpoints_, que usan las direcciones de los
pods.

Podemos definir servicios sin seleccionadores y escribir directamente el
_endpoint_ asociado.  Esto es √∫til para cosas como que los pod puedan invocar a
servicios implantados fuera del cluster o en namespaces distintos a los suyos.
Es necesario que el _endpoint_ se llame exactamente igual que el servicio, como
puede verse en este ejemplo:

[source,yaml]
----
---
apiVersion: v1
kind: Service
metadata:
  name: external-db
spec:
  ports:
    - protocol: TCP
      port: 3306

---
apiVersion: v1
kind: Endpoints
metadata:
  name: external-db
subsets:
  - addresses:
      - ip: 10.1.1.20
    ports:
      - port: 3306
----

Los _endpoints_ son arrays y pueden tener hasta 1000 entradas.

Los _endpoints_ no escalan bien y producen demasiada sobrecarga en el _control
plane_ de K8s, por lo que se implant√≥ un tipo de objeto similar, los
_EndpointSlices_, que tambi√©n gestionan autom√°ticamente los controladores de
los servicios.

=== Funcionamiento de kube-proxy

En cada nodo de K8s hay un `kube-proxy` funcionando, responsable de implementar
los puntos de entrada de los <<services>>.  Se puede arrancar de varios modos,
seg√∫n su configuraci√≥n guardada en un <<configmaps,_ConfigMap_>>.  En todos los
modos, `kube-proxy` monitoriza la adici√≥n o eliminaci√≥n de servicios y
_endpoints_ en el _control plane_ para actuar en consecuencia.

En el modo _user space proxy_, ya obsoleto, `kube-proxy` abre un puerto
aleatorio en el nodo por servicio para recibir las peticiones, y configura una
regla de `iptables` para poder recibir todo lo que vaya hacia la IP y el puerto
del cluster `clusterIP:port`.  Esta IP es virtual, *no es la IP del nodo*.

[[configmaps,_ConfigMaps_]]
== ConfigMaps

WARNING: TODO

== CLI de kubectl

`kubectl` es el cliente m√°s habitual para trabajar con la API de K8s.  Funciona
por l√≠nea de comandos, y su configuraci√≥n se guarda en `~/.kube/config`,
incluyendo la URL del cluster y las credenciales de autenticaci√≥n.

Los archivos de configuraci√≥n de `kubectl` se conocen como _kubeconfigs_.  Se
puede decir a `kubectl` qu√© archivo usar con la opci√≥n global
`--kubeconfig=<archivo>`.

=== Autocompletado

`kubectl completion <shell>` genera las √≥rdenes necesarias para tener
autocompletado con distintos shells.  Para `fish`, basta con meter lo siguiente
en `~/.config/fish/config.fish`:

[source]
----
kubectl completion fish | source
----

=== Informaci√≥n sobre el cluster

[[kubectl_api_resources,`kubectl api-resources`]]
==== kubectl api-resources

Muestra los recursos disponibles a trav√©s de la API del cluster:

[source,console]
----
$ kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
pods                              po           v1                                     true         Pod
podtemplates                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount
services                          svc          v1                                     true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v2                         true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1                               true         CronJob
jobs                                           batch/v1                               true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1                 true         Lease
endpointslices                                 discovery.k8s.io/v1                    true         EndpointSlice
events                            ev           events.k8s.io/v1                       true         Event
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
ingresses                         ing          networking.k8s.io/v1                   true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1                              true         PodDisruptionBudget
podsecuritypolicies               psp          policy/v1beta1                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1           true         Role
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
csistoragecapacities                           storage.k8s.io/v1beta1                 true         CSIStorageCapacity
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
----

[[kubectl_api_versions,`kubectl api-versions`]]
==== kubectl api-versions

Muestra las API soportadas por un cluster de K8s:

[source,console]
----
$ kubectl api-versions
apiextensions.k8s.io/v1
apiregistration.k8s.io/v1
apps/v1
authentication.k8s.io/v1
authorization.k8s.io/v1
autoscaling/v1
autoscaling/v2
autoscaling/v2beta1
autoscaling/v2beta2
batch/v1
batch/v1beta1
certificates.k8s.io/v1
coordination.k8s.io/v1
discovery.k8s.io/v1
discovery.k8s.io/v1beta1
events.k8s.io/v1
events.k8s.io/v1beta1
flowcontrol.apiserver.k8s.io/v1beta1
flowcontrol.apiserver.k8s.io/v1beta2
metrics.k8s.io/v1beta1
networking.k8s.io/v1
node.k8s.io/v1
node.k8s.io/v1beta1
policy/v1
policy/v1beta1
rbac.authorization.k8s.io/v1
scheduling.k8s.io/v1
storage.k8s.io/v1
storage.k8s.io/v1beta1
v1
----

[[kubectl_cluster_info,kubectl cluster-info]]
==== kubectl cluster-info [dump]

Muestra informaci√≥n sobre el cluster, incluyendo el punto de entrada a la API.
Con la opci√≥n `dump`, se muestra informaci√≥n completa en formato JSON:

[source,console]
----
$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.49.2:8443
CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
----

[[kubectl_describe,`kubectl describe`]]
==== kubectl describe

Muestra los detalles de un recurso o de un grupo de recursos:

[source,console]
----
$ kubectl describe node minikube
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f4b412861bb746be73053c9f6d2895f12cf78565
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_07_18T13_28_21_0700
                    minikube.k8s.io/version=v1.26.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 18 Jul 2022 13:28:17 +0200
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 18 Jul 2022 14:08:33 +0200
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 18 Jul 2022 14:04:36 +0200   Mon, 18 Jul 2022 13:28:14 +0200   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 18 Jul 2022 14:04:36 +0200   Mon, 18 Jul 2022 13:28:14 +0200   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 18 Jul 2022 14:04:36 +0200   Mon, 18 Jul 2022 13:28:14 +0200   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 18 Jul 2022 14:04:36 +0200   Mon, 18 Jul 2022 13:28:51 +0200   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  243998164Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16313940Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  243998164Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16313940Ki
  pods:               110
System Info:
  Machine ID:                 d8902d1345bb469697278da23257a8d2
  System UUID:                45be27bb-9f73-426a-a9e4-abb2dceca00d
  Boot ID:                    9782c3a5-9fa6-4e33-8e2b-969f9b81b3da
  Kernel Version:             5.18.0-2-amd64
  OS Image:                   Ubuntu 20.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.24.1
  Kube-Proxy Version:         v1.24.1
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-6d4b75cb6d-bmghz            100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     40m
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         40m
  kube-system                 kindnet-ppdxr                       100m (2%)     100m (2%)   50Mi (0%)        50Mi (0%)      40m
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         40m
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         40m
  kube-system                 kube-proxy-rrf8b                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         40m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%)  100m (2%)
  memory             220Mi (1%)  220Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 40m                kube-proxy
  Normal  NodeHasSufficientMemory  40m (x5 over 40m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    40m (x5 over 40m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     40m (x4 over 40m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasNoDiskPressure    40m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  Starting                 40m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  40m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasSufficientPID     40m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  40m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           40m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeReady                39m                kubelet          Node minikube status is now: NodeReady
----

[source,console]
----
$ kubectl -n kube-system describe coredns-6d4b75cb6d-bmghz
Name:                 coredns-6d4b75cb6d-bmghz
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 minikube/192.168.49.2
Start Time:           Mon, 18 Jul 2022 13:28:51 +0200
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  docker://cdc91589b7f6e409c2d499f3fd4b62cd6fc7e85aeef319814d541966d2bfc4e4
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      docker-pullable://k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Mon, 18 Jul 2022 13:28:52 +0200
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f99hc (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-f99hc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         42m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-bmghz to minikube
  Normal   Pulled            42m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           42m   kubelet            Created container coredns
  Normal   Started           42m   kubelet            Started container coredns
----

[[kubectl_get,`kubectl get`]]
==== kubectl get

Devuelve distinta informaci√≥n sobre el cluster, como los nodos, los
<<pod,pods>> que hay corriendo...

[source,console]
----
$ kubectl get nodes -o wide
NAME           STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
minikube       Ready    control-plane   43m   v1.24.1   192.168.49.2   <none>        Ubuntu 20.04.4 LTS   5.18.0-2-amd64   docker://20.10.17
minikube-m02   Ready    <none>          43m   v1.24.1   192.168.49.3   <none>        Ubuntu 20.04.4 LTS   5.18.0-2-amd64   docker://20.10.17
minikube-m03   Ready    <none>          42m   v1.24.1   192.168.49.4   <none>        Ubuntu 20.04.4 LTS   5.18.0-2-amd64   docker://20.10.17
----

[source,console]
----
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-6d4b75cb6d-bmghz           1/1     Running   0          43m
kube-system   etcd-minikube                      1/1     Running   0          44m
kube-system   kindnet-c88sw                      1/1     Running   0          43m
kube-system   kindnet-gw87k                      1/1     Running   0          43m
kube-system   kindnet-ppdxr                      1/1     Running   0          43m
kube-system   kube-apiserver-minikube            1/1     Running   0          44m
kube-system   kube-controller-manager-minikube   1/1     Running   0          44m
kube-system   kube-proxy-j6wvq                   1/1     Running   0          43m
kube-system   kube-proxy-rrf8b                   1/1     Running   0          43m
kube-system   kube-proxy-t8b25                   1/1     Running   0          43m
kube-system   kube-scheduler-minikube            1/1     Running   0          44m
kube-system   metrics-server-58d4b776f5-v6l7c    1/1     Running   0          29m
kube-system   storage-provisioner                1/1     Running   0          44m
----

[source,console]
----
$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
kube-system   coredns-6d4b75cb6d-bmghz           1/1     Running   0          44m   10.244.0.2     minikube       <none>           <none>
kube-system   etcd-minikube                      1/1     Running   0          44m   192.168.49.2   minikube       <none>           <none>
kube-system   kindnet-c88sw                      1/1     Running   0          43m   192.168.49.4   minikube-m03   <none>           <none>
kube-system   kindnet-gw87k                      1/1     Running   0          44m   192.168.49.3   minikube-m02   <none>           <none>
kube-system   kindnet-ppdxr                      1/1     Running   0          44m   192.168.49.2   minikube       <none>           <none>
kube-system   kube-apiserver-minikube            1/1     Running   0          44m   192.168.49.2   minikube       <none>           <none>
kube-system   kube-controller-manager-minikube   1/1     Running   0          44m   192.168.49.2   minikube       <none>           <none>
kube-system   kube-proxy-j6wvq                   1/1     Running   0          43m   192.168.49.4   minikube-m03   <none>           <none>
kube-system   kube-proxy-rrf8b                   1/1     Running   0          44m   192.168.49.2   minikube       <none>           <none>
kube-system   kube-proxy-t8b25                   1/1     Running   0          44m   192.168.49.3   minikube-m02   <none>           <none>
kube-system   kube-scheduler-minikube            1/1     Running   0          44m   192.168.49.2   minikube       <none>           <none>
kube-system   metrics-server-58d4b776f5-v6l7c    1/1     Running   0          29m   10.244.2.2     minikube-m03   <none>           <none>
kube-system   storage-provisioner                1/1     Running   0          44m   192.168.49.2   minikube       <none>           <none>
----

[source,console]
----
$  kubectl -n kube-system get pod coredns-6d4b75cb6d-bmghz
NAME                       READY   STATUS    RESTARTS   AGE
coredns-6d4b75cb6d-bmghz   1/1     Running   0          160m
----

[source,console]
----
$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   16h
----

Por defecto, hay algunos tipos de recursos dentro de los namespaces que no se
muestran en la salida de `kubectl get all`.  Se puede utilizar lo siguiente
para verlos todos:

[source,console]
----
$ kubectl api-resources --verbs=list --namespaced -o name | xargs -n1 kubectl get --show-kind --ignore-not-found --all-namespaces
NAMESPACE         NAME                                           DATA   AGE
default           configmap/kube-root-ca.crt                     1      162m
kube-node-lease   configmap/kube-root-ca.crt                     1      162m
kube-public       configmap/cluster-info                         4      162m
kube-public       configmap/kube-root-ca.crt                     1      162m
kube-system       configmap/coredns                              1      162m
kube-system       configmap/extension-apiserver-authentication   6      162m
kube-system       configmap/kube-proxy                           2      162m
kube-system       configmap/kube-root-ca.crt                     1      162m
kube-system       configmap/kubeadm-config                       1      162m
kube-system       configmap/kubelet-config                       1      162m
NAMESPACE     NAME                                 ENDPOINTS                                     AGE
default       endpoints/kubernetes                 192.168.49.2:8443                             162m
kube-system   endpoints/k8s.io-minikube-hostpath   <none>                                        161m
kube-system   endpoints/kube-dns                   10.244.0.2:53,10.244.0.2:53,10.244.0.2:9153   162m
kube-system   endpoints/metrics-server             10.244.2.2:4443                               147m
NAMESPACE     LAST SEEN   TYPE      REASON                    OBJECT                                 MESSAGE
default       161m        Normal    NodeHasSufficientMemory   node/minikube-m02                      Node minikube-m02 status is now: NodeHasSufficientMemory
default       161m        Normal    NodeHasNoDiskPressure     node/minikube-m02                      Node minikube-m02 status is now: NodeHasNoDiskPressure
default       161m        Normal    RegisteredNode            node/minikube-m02                      Node minikube-m02 event: Registered Node minikube-m02 in Controller
default       161m        Normal    Starting                  node/minikube-m02
default       161m        Normal    NodeHasSufficientMemory   node/minikube-m03                      Node minikube-m03 status is now: NodeHasSufficientMemory
default       161m        Normal    NodeHasNoDiskPressure     node/minikube-m03                      Node minikube-m03 status is now: NodeHasNoDiskPressure
default       161m        Normal    RegisteredNode            node/minikube-m03                      Node minikube-m03 event: Registered Node minikube-m03 in Controller
default       161m        Normal    Starting                  node/minikube-m03
...
----

[[kubectl_get_events,`kubectl_get events`]]
===== kubectl get events

Muestra los eventos que se han producido en el cluster:

[source,console]
----
$ kubectl get events
LAST SEEN   TYPE     REASON                    OBJECT              MESSAGE
163m        Normal   NodeHasSufficientMemory   node/minikube-m02   Node minikube-m02 status is now: NodeHasSufficientMemory
163m        Normal   NodeHasNoDiskPressure     node/minikube-m02   Node minikube-m02 status is now: NodeHasNoDiskPressure
163m        Normal   RegisteredNode            node/minikube-m02   Node minikube-m02 event: Registered Node minikube-m02 in Controller
163m        Normal   Starting                  node/minikube-m02
163m        Normal   NodeHasSufficientMemory   node/minikube-m03   Node minikube-m03 status is now: NodeHasSufficientMemory
163m        Normal   NodeHasNoDiskPressure     node/minikube-m03   Node minikube-m03 status is now: NodeHasNoDiskPressure
163m        Normal   RegisteredNode            node/minikube-m03   Node minikube-m03 event: Registered Node minikube-m03 in Controller
163m        Normal   Starting                  node/minikube-m03
164m        Normal   NodeHasSufficientMemory   node/minikube       Node minikube status is now: NodeHasSufficientMemory
164m        Normal   NodeHasNoDiskPressure     node/minikube       Node minikube status is now: NodeHasNoDiskPressure
164m        Normal   NodeHasSufficientPID      node/minikube       Node minikube status is now: NodeHasSufficientPID
164m        Normal   Starting                  node/minikube       Starting kubelet.
164m        Normal   NodeHasSufficientMemory   node/minikube       Node minikube status is now: NodeHasSufficientMemory
164m        Normal   NodeHasNoDiskPressure     node/minikube       Node minikube status is now: NodeHasNoDiskPressure
164m        Normal   NodeHasSufficientPID      node/minikube       Node minikube status is now: NodeHasSufficientPID
164m        Normal   NodeAllocatableEnforced   node/minikube       Updated Node Allocatable limit across pods
164m        Normal   RegisteredNode            node/minikube       Node minikube event: Registered Node minikube in Controller
164m        Normal   Starting                  node/minikube
163m        Normal   NodeReady                 node/minikube       Node minikube status is now: NodeReady
----

Podemos filtrar los eventos utilizando la opci√≥n `--field-selector`:

[source,console]
----
$ kubectl get event --field-selector involvedObject.name=pod-2containers-lp --watch
LAST SEEN   TYPE      REASON      OBJECT                    MESSAGE
15m         Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
10m         Warning   BackOff     pod/pod-2containers-lp   Back-off restarting failed container
7m26s       Normal    Scheduled   pod/pod-2containers-lp   Successfully assigned blas/pod-2containers-lp to minikube-m03
7m25s       Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
7m23s       Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.386338018s
7m23s       Normal    Created     pod/pod-2containers-lp   Created container nginx
7m23s       Normal    Started     pod/pod-2containers-lp   Started container nginx
70s         Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
7m22s       Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.398490586s
69s         Normal    Created     pod/pod-2containers-lp   Created container loop
69s         Normal    Started     pod/pod-2containers-lp   Started container loop
101s        Warning   Unhealthy   pod/pod-2containers-lp   Liveness probe failed:
101s        Normal    Killing     pod/pod-2containers-lp   Container loop failed liveness probe, will be restarted
3m29s       Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.403924337s
2m39s       Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.35347142s
69s         Normal    Pulled      pod/pod-2containers-lp   Successfully pulled image "nginx" in 1.342802789s
0s          Normal    Pulling     pod/pod-2containers-lp   Pulling image "nginx"
0s          Warning   BackOff     pod/pod-2containers-lp   Back-off restarting failed container
0s          Warning   BackOff     pod/pod-2containers-lp   Back-off restarting failed container
0s          Warning   BackOff     pod/pod-2containers-lp   Back-off restarting failed container
----

Para ver los campos disponibles, podemos ver la salida de la orden en formato
YAML o JSON:

[source,console]
----
$ kubectl get events --output yaml
----

[source,yaml]
----
apiVersion: v1
items:
- apiVersion: v1
  count: 8
  eventTime: null
  firstTimestamp: "2022-07-18T11:28:37Z"
  involvedObject:
    kind: Node
    name: minikube-m02
    uid: minikube-m02
  kind: Event
  lastTimestamp: "2022-07-18T11:28:50Z"
  message: 'Node minikube-m02 status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2022-07-18T11:28:50Z"
    name: minikube-m02.1702e8ecfd61ab67
    namespace: default
    resourceVersion: "395"
    uid: fbe3fd76-837c-4d62-9c2b-5482c356ccbf
  reason: NodeHasSufficientMemory
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: minikube-m02
  type: Normal
...
----

=== Manipulaci√≥n del cluster

[[kubectl_apply,`kubectl apply`]]
==== kubectl apply

Aplica al cluster la configuraci√≥n indicada en el archivo YAML o JSON
especificado con `-f` (o desde la entrada est√°ndar, con `-f{nbsp}-`), haciendo
los cambios necesarios sobre la configuraci√≥n actual.

Tambi√©n se puede utilizar la opci√≥n `-k` para especificar un archivo
`kustomization.yaml`, que permite hacer referencia a varios archivos donde
especificar los recursos, y asignarles valores comunes, como el namespace o
etiquetas.  Los siguientes ejemplos son de la
https://kubectl.docs.kubernetes.io/references/kubectl/apply/[documentaci√≥n de
`kubectl`]:

[source,yaml]
----
# kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# list of Resource Config to be Applied
resources:
- deployment.yaml

# namespace to deploy all Resources to
namespace: default

# labels added to all Resources
commonLabels:
  app: example
  env: test
----

[source,yaml]
----
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: the-deployment
spec:
  replicas: 5
  template:
    containers:
      - name: the-container
        image: registry/container:latest
----

Se puede usar la orden `edit-last-applied` para editar la √∫ltima configuraci√≥n
aplicada, y `view-last-applied` para mostrarla.

Con la opci√≥n `--prune`, se eliminan los objetos del cluster que no est√©n en la
configuraci√≥n aplicada.


[[kubectl_create,`kubectl create`]]
==== kubectl create

Crea los recursos especificados en el archivo YAML o JSON pasado con la opci√≥n
`-f` (o desde la entrada est√°ndar, con `-f{nbsp}-`).

[[kubectl_delete,`kubectl delete`]]
==== kubectl delete

Elimina los recursos especificados en el archivo YAML o JSON pasado con la
opci√≥n `-f` (o desde la entrada est√°ndar, con `-f{nbsp}-`), o los indicados por
nombre o por etiqueta.

[[kubectl_edit,`kubectl edit`]]
==== kubectl edit

Edita el objeto especificado en el archivo YAML o JSON pasado con la opci√≥n
`-f` (o desde la entrada est√°ndar, con `-f{nbsp}-`), o los indicados por nombre
o por etiqueta.  Utiliza el editor especificado en las variables de entorno
`EDITOR` o `KUBE_EDITOR`, o con `vi` si no est√°n definidas.  Puede editar
varios objetos, pero de uno en uno.

=== Namespaces

Los _namespaces_ son una forma de hacer compartimentos dentro de Kubernetes, de
manera que se puede limitar la visibilidad de los recursos.

Los nombres de los recursos deben de ser √∫nicos dentro de un namespace, pero se
pueden repetir entre namespaces.

El prefijo `kube-` est√° reservado para uso interno de K8s.

Por defecto, hay cuatro namespaces:

* *default*, para los objetos que no est√°n en ning√∫n otro namespace.

* *kube-system*, para los objetos creados y gestionados por K8s.

* *kube-public*, para objetos p√∫blicos que puede ver cualquier usuario, incluso
   sin estar autenticado, y para los recursos que deban ser vistos por el
   cluster completo.

* *kube-node-lease*, para guardar informaci√≥n sobre los heartbeats de los
   nodos, de manera que el plano de control pueda detectar su ca√≠da.

El nombre que los servicios tienen en el DNS de K8s incluye el namespace
(`<servicio>.<namespace>.svc.cluster.local`), por lo que los contenedores solo
ver√°n los servicios que tengan en su propio namespace, a menos que especifiquen
el dominio DNS completo.  Como el nombre de los namespaces se usa en el DNS,
solo deben de tener caracteres v√°lidos para DNS (63 caracteres m√°ximo, solo
letras min√∫sculas o guiones, y empezar y terminar con un car√°cter
alfanum√©rico).

No todos los tipos de recursos pueden estar dentro de un namespace, como los
nodos o los propios namespaces (no se pueden anidar).  Se puede ver la lista
completa as√≠:

[source,console]
----
$ kubectl api-resources --namespaced=false
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
componentstatuses                 cs           v1                                     false        ComponentStatus
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumes                 pv           v1                                     false        PersistentVolume
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration
nodes                                          metrics.k8s.io/v1beta1                 false        NodeMetrics
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
podsecuritypolicies               psp          policy/v1beta1                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
----

==== Namespace por defecto

Se utiliza la opci√≥n global `--namespace` para indicar a `kubectl` el namespace
sobre el que queremos actuar.  Podemos especificar el namespace por defecto
sobre el que queremos actuar en el contexto actual haciendo `kubectl config
set-context --current --namespace=<namespace>`.

==== kubectl create namespace

Permite crear un namespace desde la l√≠nea de comandos, sin necesidad de
utilizar un archivo YAML o JSON:

[source,console]
----
$ kubectl create namespace blas
namespace/blas created

$ kubectl config set-context --current --namespace=blas
Context "minikube" modified.

$ kubectl get pods
No resources found in blas namespace.
----

=== Pods

[[kubectl_attach,`kubectl attach`]]
==== kubectl attach (POD | TYPE/NAME) -c CONTAINER [options]

Conecta los `stdout` y `stderr` del terminal actual con uno de los contenedores
de un pod en ejecuci√≥n.  Se puede especificar el contenedor con la opci√≥n
`--container` (si no se especifica ninguno, se elige el que tenga el nombre
contenido en la anotaci√≥n `kubectl.kubernetes.io/default-container` del pod, o
el primer contenedor del pod si esa anotaci√≥n no est√° definida).

La forma de interrumpir la conexi√≥n depender√° del _runtime_ de contenedores que
estemos usando, pero normalmente se hace pulsando `Ctrl-P`+`Ctrl-Q`, aunque
suele ser configurable.

Por defecto *no* conecta la entrada est√°ndar, pero puede hacerse con la opci√≥n
`--stdin`, con la que podemos usar adem√°s `--tty` para indicar que queremos que
funcione en modo interactivo, como una terminal, para poder pasar las se√±ales
de control generadas con el teclado.

[[kubectl_exec,`kubectl exec`]]
==== kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...]

Ejecuta una orden en uno de los contenedores de un pod.  Como ocurre con
<<kubectl_attach>>, solo conecta las corrientes `stdout` y `stderr` del
terminal a la orden, a menos que se ejecute con la opci√≥n `--stdin` y,
opcionalmente, con `--tty` si queremos conectar la terminal actual.

La elecci√≥n del contenedor donde se ejecuta la orden se hace igual que con
<<kubectl_attach>>.

[[kubectl_logs,`kubectl logs`]]
==== kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER] [options]

Muestra los registros de uno de los contenedores de un pod o del recurso que se
especifique.

Podemos ver los registros de todos los contenedores de un pod con la opci√≥n
`--all-containers`.

Con `-f`, el proceso seguir√° mostrando los registros a medida que se vayan
generando.

Con `-p`, podemos ver los registros de la instancia previa del contenedor, si
es que hubo una.

Con `-l`, podemos elegir los contenedores utilizando seleccionadores de
igualdad (ver <<seleccionadores>>).

Con `--since`, podemos especificar que queremos ver los registros desde el
tiempo relativo que especifiquemos (p. ej, `--since=10m` para ver los de los
√∫tlimos 10 minutos).

[[kubectl_top,`kubectl top`]]
==== kubectl top (node | pod) [NAME | -l label]

Muestra el uso de los recursos de un nodo o de un pod.  Solo funciona si
tenemos corriendo en el cluster la API de m√©tricas proporcionada por
<<metrics_server>>.

[source,console]
----
$ kubectl top node
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
minikube       196m         4%     1456Mi          9%
minikube-m02   51m          1%     405Mi           2%
minikube-m03   44m          1%     357Mi           2%
----

=== Contextos

La informaci√≥n de la configuraci√≥n de `kubectl` se agrupa en _contextos_ con
nombre.  `kubectl` permite consultar el contexto actual y cambiar de contexto.

==== kubectl config current-context

Muestra el contexto que usa `kubectl`:

[source,console]
----
$ kubectl config current-context
minikube
----


==== kubectl config get-contexts <contexto>

Muestra los contextos disponibles en la configuraci√≥n, o la informaci√≥n de uno
concreto:

[source,console]
----
$ kubectl config get-contexts
CURRENT   NAME       CLUSTER    AUTHINFO   NAMESPACE
*         minikube   minikube   minikube   default

$ kubectl config get-contexts minikube
CURRENT   NAME       CLUSTER    AUTHINFO   NAMESPACE
*         minikube   minikube   minikube   default
----

==== kubectl config use-context <contexto>

Alias: `kubectl config use`.

Cambia el contexto actual.

==== kubectl config set-context <contexto>

Modifica un contexto:


[source,console]
----
$ kubectl config set-context minikube --namespace=blas
Context "minikube" modified.
----

==== kubectl config view

Muestra el archivo _kubeconfig_ actual:

[source,console]
----
$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/jcouto/.minikube/ca.crt
    extensions:
    - extension:
        last-update: Thu, 16 Jun 2022 16:57:10 CEST
        provider: minikube.sigs.k8s.io
        version: v1.25.2
      name: cluster_info
    server: https://192.168.49.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    extensions:
    - extension:
        last-update: Thu, 16 Jun 2022 16:57:10 CEST
        provider: minikube.sigs.k8s.io
        version: v1.25.2
      name: context_info
    namespace: default
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/jcouto/.minikube/profiles/minikube/client.crt
    client-key: /home/jcouto/.minikube/profiles/minikube/client.key

$ diff ~/.kube/config (kubectl config view | psub)
$
----

== Troubleshooting

Las √≥rdenes m√°s √∫tiles para ver lo que ocurre dentro del cluster de K8s son
<<kubectl_get>>,  <<kubectl_get_events>> y <<kubectl_describe>>.

== Ejemplos

=== WordPress

WordPress es un sistema de gesti√≥n de contenidos web que necesita una base de
datos MySQL para funcionar.  Vamos a ver varias formas de desplegar este
servicio en K8s, evolucionando la configuraci√≥n inicial hasta tener un dise√±o
robusto.

==== Versi√≥n inicial

La siguiente configuraci√≥n b√°sica muestra c√≥mo lanzar WordPress en un pod con
los dos contenedores necesarios, monitorizando que WordPress est√© levantado
mediante el c√≥digo HTTP devuelto por su p√°gina de login:

.dep-wordpress-01.yaml
[source,yaml]
----
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dep-wordpress-01
  labels:
    app: wp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wp
  template:
    metadata:
      labels:
        app: wp
    spec:
      containers:
      - name: wp
        image: wordpress:6.0-apache
        ports:
        - containerPort: 80
        env:
        - name: WORDPRESS_DB_HOST
          value: 127.0.0.1
        - name: WORDPRESS_DB_USER
          value: blas
        - name: WORDPRESS_DB_PASSWORD
          value: estonticiassupernoiasexuperaciones
        - name: WORDPRESS_DB_NAME
          value: wp
        readinessProbe:
          httpGet:
            port: 80
            path: /wp-login.php
        livenessProbe:
          httpGet:
            port: 80
            path: /wp-login.php
      - name: db
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: computacionalmenteinversapolarizacionesmutan
        - name: MYSQL_USER
          value: blas
        - name: MYSQL_PASSWORD
          value: estonticiassupernoiasexuperaciones
        - name: MYSQL_DATABASE
          value: wp
----

Este ejemplo tiene varios problemas:

. No esperamos a que la base de datos est√© levantada para lanzar WordPress,
  aunque es necesario para que funcione.  WordPress reintentar√° la conexi√≥n con
  la base de datos varias veces hasta desistir.

. El servicio de base de datos y WordPress est√°n corriendo en el mismo pod, lo
  que ser√≠a equivalente a que los dos servicios corrieran en el mismo servidor.
  Esto no es recomendable porque hacemos que los dos servicios est√©n
  fuertemente acoplados, lo que nos impide hacer cosas como escalarlos de forma
  independiente.

. No tenemos persistencia del almacenamiento para ninguno de los dos servicios.

==== Base de datos separada del frontal

El siguiente ejemplo mejora algunos de los problemas indicados antes, separando
la base de datos MySQL y el frontal de WordPress en dos pods distintos, y
utilizando un servicio para permitir que WordPress localice la base de datos
por nombre:

[source,yaml]
----
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dep-wordpress-02
  labels:
    app: wp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wp
      service: wordpress
  template:
    metadata:
      labels:
        app: wp
        service: wordpress
    spec:
      containers:
      - name: wp
        image: wordpress:6.0-apache
        ports:
        - containerPort: 80
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-service
        - name: WORDPRESS_DB_USER
          value: blas
        - name: WORDPRESS_DB_PASSWORD
          value: estonticiassupernoiasexuperaciones
        - name: WORDPRESS_DB_NAME
          value: wp
        readinessProbe:
          httpGet:
            port: 80
            path: /wp-login.php
        livenessProbe:
          httpGet:
            port: 80
            path: /wp-login.php

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dep-db-02
  labels:
    app: wp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wp
      service: mysql
  template:
    metadata:
      labels:
        app: wp
        service: mysql
    spec:
      containers:
      - name: db
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: computacionalmenteinversapolarizacionesmutan
        - name: MYSQL_USER
          value: blas
        - name: MYSQL_PASSWORD
          value: estonticiassupernoiasexuperaciones
        - name: MYSQL_DATABASE
          value: wp

---
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
spec:
  selector:
    app: wp
    service: mysql
  ports:
  - protocol: TCP
    port: 3306
----


== Otras herramientas

=== kustomize

https://github.com/kubernetes-sigs/kustomize[kustomize] sirve para aplicar
cambios a plantillas YAML de K8s sin tener que modificar los archivos
originales, combinando funcionalidades de herramientas como `make` y `sed`.

=== helm

WARNING: TODO

== Para saber m√°s

* https://kubernetes.io/docs/home/[Documentaci√≥n oficial de Kubernetes].

== Glosario

kubeconfig:: Archivo de configuraci√≥n de `kubectl`, generalmente ubicado en
`~/.kube/config`.

[[label,_label_]]
label:: Las etiquetas son parejas de clave/valor que se asignan a los objetos
de K8s, y se pueden utilizar en los seleccionadores para hacer referencia a los
objetos que tengan determinadas etiquetas.

[[pod,pod]]
pod:: Unidad m√≠nima de proceso de Kubernetes, consistente en un entorno para
ejecutar contenedores donde comparten vol√∫menes, _namespaces_ y _cgroups_.  El
contenido de un pod se lanza en un √∫nico nodo, y se gestiona como un todo.
Todos los contenedores de un pod comparten la direcci√≥n IP 127.0.0.1 y la
pueden usar para comunicarse entre ellos.

[[selector,_selector_]]
selector:: Filtro que utiliza etiquetas para elegir objetos.  Por ejemplo, se
puede utilizar `nodeSelector` en la definici√≥n de un pod para indicar que solo
debe ejecutarse en los nodos que tengan las etiquetas indicadas.
